Topic,Count,Name,Representation,Representative_Docs
-1,182,-1_data_digital_science_energy,"['data', 'digital', 'science', 'energy', 'technology', 'big', 'big data', 'system', 'model', 'systems']","['Exploring and mining the explosive burst of ""big data"" has already generated a lot of innovative applications, especially the recent advances of AI applications, and thus produced big values to the human society and civilization. However, due to the centralized patterns of data governance activities, including creation, sharing, exchange, management, analytics, tracing, and accounting, the potential values of big data distributed on the Internet are far away from being adequately explored. The recent announcement of data protection policies/laws such as GDPR makes the problem even more challenging. We are now at a moment of truth where the data governance infrastructure should be reconsidered and redesigned. In this paper, we propose a software-defined infrastructure design in a decentralized fashion: data owners are able to implement and deploy their own rules to the application systems where the data are produced for further governance activities. Such a fashion is quite similar to the popular software-defined networking where users are allowed to deploy rules of switches and customize the use. Our principled infrastructure design can radically reform the current data governance activities into a decentralized topology. On the one hand, data can be separated from the application that generates the data, and data owners can have the full rights to decide where their data should be stored and how the data can be shared. On the other hand, data users can search, discover, integrate, and analyze the data from various data sources according to their application requirements and scenarios. As a result, we argue that our infrastructure can establish a new generation of responsive decentralized data governance that can promote the innovation of linking data to better adapt the open environment and diverse user requirements. With this perspective, we briefly discuss some key insights and enumerate several related new technologies and open challenges.', 'Recent technological advancements have made many industries more reliant on technology, which has led to an exponential increase in the amount of data generated every day. However, this surge in data has been driven by concerns including data breaches and lack of data management. Data security, integrity, efficient management, and advanced data analysis are the issues that are yet to be solved effectively. The extensive use of Artificial Intelligence (AI) and Centralized data storage have made it more accessible to sensitive information on computer networks and data breaches. This paper aims to address these challenges by ensuring secure data storage, sharing and computation within the systems, services and web browsers by using Big Data tools and frameworks along with AI and merge of Blockchain. The adoption of Blockchain Technology is of paramount importance in providing tamper-proof, ownership-guaranteed data, ensuring secure and dependable data sharing. Additionally, onchain Oracles have been used to improve connectivity from real-world data sources. The use of Big Data frameworks and tools, creates a strong infrastructure for data administration, monitoring, predictions, and analysis. The approach provides increased security, efficient management, and better connection in a decentralized manner.', 'Data Science is an emerging field of science, which requires a multi-disciplinary approach and is based on the Big Data and data intensive technologies that both provide a basis for effective use of the data driven research and economy models. Modern data driven research and industry require new types of specialists that are capable to support all stages of the data lifecycle from data production and input to data processing and actionable results delivery, visualisation and reporting, which can be jointly defined as the Data Science professions family. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects all multi-disciplinary knowledge and competences that are required from the Data Science practitioners in modern, data driven research and the digital economy. The educational model and approach should also solve different aspects of the future professionals that includes both theoretical knowledge and practical skills that must be supported by corresponding education infrastructure and educational labs environment. In modern conditions with the fast technology change and strong skills demand, the Data Science education and training should be customizable and delivered in multiple form, also providing sufficient data labs facilities for practical training. This paper discussed both aspects: building customizable Data Science curriculum for different types of learners and proposing a hybrid model for virtual labs that can combine local university facility and use cloud based Big Data and Data analytics facilities and services on demand. The proposed approach is based on using the EDISON Data Science Framework (EDSF) developed in the EU funded Project EDISON and CYCLONE cloud automation systems being developed in another EU funded project CYCLONE.']"
0,41,0_prediction_degradation_rul_health,"['prediction', 'degradation', 'rul', 'health', 'model', 'bearings', 'data', 'equipment', 'rul prediction', 'failure']","['Accurately predicting the remaining useful life (RUL) of bearings is quite significant for ensuring the operation reliability of wind turbines. Due to the limitation of real-world life cycle data, the accuracy of existing wind turbine RUL prediction methods needs to be improved. This article suggests a multisource domain meta transfer learning (MD-MTL)-based cross-scenario transferable RUL prediction method. The MD-MTL uses test rig data on multiple operating conditions to build up the prediction model and migrate the prediction knowledge to real wind turbines. In addition, a novel RUL prediction network based on a convolutional encoder and ProbSparse multihead attention decoder (CE-PSAD) is proposed for the improvement of prediction accuracy. It can extract key degradation features and long-term dependence relationships from raw vibration signals, and complete the precise mapping to RUL. The efficiency of the proposed method is proven based on two test rig datasets and two wind turbine bearing datasets.', 'Remaining useful life (RUL) prediction of rolling bearings is of paramount importance to various industrial applications. Recently, intelligent data-driven RUL prediction methods have achieved fruitful results. However, the existing methods heavily rely on the quality and quantity of the available data. For some critical bearings in industrial scenarios, the real run-to-failure data are insufficient, which impair the applicability of data-based methods for industrial practices. To address these issues, this article proposes a novel dynamic model-assisted RUL prediction approach for rolling bearing, in which sufficient simulation data are applied as the training data to solve the problem caused by insufficient real data. More specifically, a dynamic rolling bearing model is introduced for simulating the degradation process of physical structures. Then, a multilayer cross-domain transformer network is developed to implement RUL prediction and adapt the learned prediction knowledge from simulation to the actual measurements. Furthermore, a mutual information loss is utilized to preserve the generalized prediction knowledge of the measured data. The proposed approach can achieve a high RUL prediction accuracy with only limited measured data, which tackles the drawbacks of the existing data-driven methods. The experimental results of the rolling bearing degradation datasets demonstrate the effectiveness and superiority of the proposed RUL prediction approach.', 'Rolling bearing is an important component of rotating machinery equipment. Predictive maintenance can reduce unplanned maintenance expenditure and effectively improve the reliability of the equipment. The bearings produced in one batch may present completely different degradation trends, which increases the difficulty of tracking the degradation trend and predicting the remaining useful life (RUL) of bearings. Therefore, a RUL prediction method based on adaptive ensemble model is provided in this paper. First, an adaptive features integration technology is designed to construct a comprehensive health indicator with feature set in the time-frequency domain, which enhances the performance of health indicators to characterize the health status of bearings. Second, a dynamic ensemble model is further developed to predict RUL, which enhances the ability of the model to track different degradation trends and adaptively adjust the model parameters according to specific trend. A bearing life cycle dataset is applied to demonstrate the superiority the proposed approach.']"
1,347,1_data_big_big data_management,"['data', 'big', 'big data', 'management', 'processing', 'quality', 'which', 'system', 'analytics', 'data management']","['Satisfiable data quality is the basic guarantee for data-based research, decision-making, and service. Today, new trends in the creation, collection, and utilization of data are constantly emerging. With the usage of massive data, the problem of data quality is highlighted. Several studies on the measurement, evaluation, and management of big data quality have been proposed, and the data quality problem in the big data environment has received attention. The big data characteristics Vs model describes the dimensions and attributes information of data sources in detail, which can be implemented in big data quality measurement. In this paper, a novel rigorous big data quality measurement architecture is proposed for automatically and parallelly quantifying the value of six big data Vs, which are Volume, Variety, Velocity, Veracity, Validity, and Vincularity according to the developed algorithms in every big data process step and time phase of the big data pipeline. Thresholds for the six big data Vs are provided correspondingly for analyzing the result values. The hierarchical measurement model is constructed with multiple-based measures, derived measures, and indicators. The model is verified by comparative experiments and experiments results indicate that the designed architecture can improve the outcomes of data source implementation.', ""With the rapid advancements in technological applications have led to the flooding of data from various sources like web, social network data, business data, medical records, etc. over the preceding years. As compared to traditional data, big data reveals a unique characteristic from its three V's which means big data is unstructured. In this era, the emerging trend requires the involvement of advanced data analysis, acquisition and management techniques to mine and collect appropriate data in a structured way. In this paper, we describe the definitions and the challenges of big data systems. Next, a systematic framework decomposes the architecture of big data systems into four stages like data generation, data acquisition, data storage and data analytics. These stages form the basis of big data value chain. Finally, some solutions are discussed to tackle the challenges of big data and future attention is required for big data systems."", 'In the era of big data, effective data ingestion and pipelining are critical for organizations to harness the power of vast and diverse data sources, such as IoT devices, social media, and transactional systems. Data ingestion refers to the process of systematically collecting and importing data for processing, while data pipelining ensures the continuous flow of this data through various transformation stages, preparing it for analysis. As data volumes grow, platforms like Hadoop, with its HDFS (Hadoop Distributed File System) and MapReduce, have become essential in managing large-scale datasets by providing distributed storage and parallel processing capabilities. Additionally, tools such as Apache Kafka and Apache Flume facilitate both batch and real-time data ingestion, ensuring flexibility in handling different types of data streams. However, challenges remain, especially with the increasing complexity of datasets and the growing need for real-time processing. Emerging technologies such as cloud-native ingestion frameworks, edge computing, and AI-enhanced pipelines offer solutions to these challenges by improving scalability, performance, and cost efficiency. Machine learning is also playing an evolving role in optimizing pipeline workflows, enabling more intelligent and adaptive data processing. This paper reviews key technologies for data ingestion and pipelining, discusses their limitations, and explores cutting-edge trends aimed at enhancing the efficiency of big data platforms. These advancements promise to reshape how organizations handle big data, offering improved agility and operational effectiveness. In addition to established technologies, the rise of cloud-native architectures and AI-driven solutions is revolutionizing data ingestion and pipelining. Cloud platforms offer scalable, on-demand resources, while AI and machine learning enhance automation, anomaly detection, and predictive analytics within pipelines.']"
2,114,2_data_privacy_security_data security,"['data', 'privacy', 'security', 'data security', 'protection', 'personal', 'cloud', 'information', 'sensitive', 'system']","['The privacy and security of data stored in cloud environments remain a significant challenge for organizations and individual users. Since data is stored in different physical locations, data security and privacy are particularly critical in cloud environments. Today, data privacy and security are the main concerns of governments, organizations, academics, and IT industry experts. The future development of cloud computing depends on providing reliable data privacy and security solutions. The current study performs an in-depth assessment of data privacy and security issues affecting individual users and organizations. The analysis is followed up by a suggested solution of an adequate data security and privacy model based on a combination of file encryption and service fragmentation. Specifically, the development of the analysis and security model focuses on the Linux operating system. Finally, extensive simulation results prove that the proposed model gives more privacy accuracy and reliability in cloud environments.', ""Large capacity, fast-paced, diversified and high-value data are becoming a hotbed of data processing and research. Privacy security protection based on data life cycle is a method to protect privacy. It is used to protect the confidentiality, integrity and availability of personal data and prevent unauthorized access or use. The main advantage of using this method is that it can fully control all aspects related to the information system and its users. With the opening of the cloud, attackers use the cloud to recalculate and analyze big data that may infringe on others' privacy. Privacy protection based on data life cycle is a means of privacy protection based on the whole process of data production, collection, storage and use. This approach involves all stages from the creation of personal information by individuals (e.g. by filling out forms online or at work) to destruction after use for the intended purpose (e.g. deleting records). Privacy security based on the data life cycle ensures that any personal information collected is used only for the purpose of initial collection and destroyed as soon as possible."", 'The current age is witnessing an unprecedented dependence on data originating from humans through the devices that comprise the Internet of Things. The data collected by these devices are used for many purposes, including predictive maintenance, smart analytics, preventive healthcare, disaster protection, and increased operational efficiency and performance. However, most applications and systems that rely on user data to achieve their business objectives fail to comply with privacy regulations and expose users to numerous privacy threats. Such privacy breaches raise concerns about the legitimacy of the data being processed. Hence, this paper reviews some notable techniques for transparently, securely, and privately separating and sharing personally identifiable and non-personally identifiable information in various domains. One of the key findings of this study is that, despite various advantages, none of the existing techniques or data sharing applications preserve data/user privacy throughout the data life cycle. Another significant issue is the lack of transparency for data subjects during the collection, storage, and processing of private data. In addition, as privacy is unique to every user, there cannot be a single autonomous solution to identify and secure personally identifiable information for users of a particular application, system, or people living in different states/countries. Therefore, this research suggests a way forward to prevent the leakage of personally identifiable information at various stages of the data life cycle in compliance with some of the common privacy regulations around the world. The proposed approach aims to empower data owners to select, share, monitor, and control access to their data. In addition, the data owner is a stakeholder and a party to all data sharing contracts related to his personal data. The proposed solution has broad security and privacy controls that can be tailored to the privacy needs of specific applications.']"
3,172,3_industry_manufacturing_chain_digital,"['industry', 'manufacturing', 'chain', 'digital', '40', 'value', 'industrial', 'data', 'business', 'industry 40']","['Intelligent industry and manufacturing requires obtaining relevant sensor data and process information in real-time from all components in the manufacturing value-chain. It is envisioned that smart industry is achieved by embedding connectivity into industrial products, using Cloud and Internet-of-things (IoT) to leverage intelligence and actionable knowledge for machines, autonomous collaboration among machines, and integration of products and additional value-added services. For complex industrial systems, it is important to ensure a smooth transformation towards the smart industry vision despite of the associated challenges with respect to e.g., transition from the traditional multi-layered architecture to an open structured service-oriented automation system architecture, changes of business models and strategies, legacy system migration to cloud environment, etc. The focus of this study is therefore to examine the status of the existing research on cloud computing and IoT solutions that enable this transformation towards a smart industry. We applied the systematic mapping study method to obtain an overview of the existing related research literatures that focus on smart industry, industrial automation and manufacturing perspective. We also discuss the future research areas that need to be enhanced.', 'With the advent of the Big Data era, the industry sector has seen an increasingly intensive use of data processing, cyber-physical systems, Internet of Things and cloud technology. This digital transformation is fundamentally characterized by automation and the integration of new technologies into the enterprise value chain. From there, there are several challenges for being transformed into a connected and intelligent factory model such as the interconnection of machines, the dematerialization of communication and distribution channels, and the restructuring of the company for flexible and personalized production [P. Hébert and M. Moudallal, 2016]. Moreover, the relational model with the end customer, which is at the center of supply chain management, is thus changing, giving way to new distribution channels. The changes that are occurring for the industry sector are then called the Fourth Industrial Revolution. This article then attempts to analyze the supply chain system in the context of the new standards imposed by Industry 4.0, through a quantitative analysis conducted on the performance of industrial companies Biomérieux having upgraded its supply chain system. The indicators analyzed in this study thus include some of the most important elements of the supply chain impacted by this industrial revolution, namely; inventory management and purchasing management. The goal is to try to measure the real impact of a 4.0 upgrade of the so-called “classic” supply chain system on the performance of industrial companies.', 'With the proposal of intelligent manufacturing 2025, many manufacturing industries are exploring in upgrading and transformation. The reason is the country’s overall economic level cannot be improved without the support of the manufacturing industry. Since manufacturing activities involve multi-link resource integration, use of a variety of high and new technologies, and require a lot of labor force. To achieve high earnings, it is necessary to analyze and optimize the whole process of business links. However, the actual production process involves a large number of equipments and workers. This has created some ongoing challenges for data collection, storage, analysis and optimization. This paper focuses on the value chain of manufacturing enterprises, and proposes a big data-driven framework for manufacturing based on Internet of Things cloud platform. The framework is data-driven and aims at the value-added of enterprise value chain. Multiple production links are considered to be linked to the cloud. The dynamic feedback information of each link is used to adjust business operations. This paper theoretically analyzes the data flow process of manufacturing enterprises in the corporate value-added chain. Taking the production process as an example, the paper explains how the value-added of production can be obtained from a data perspective. This will help to speed up the process of digital and intelligent transformation of manufacturing.']"
