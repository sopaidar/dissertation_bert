Title,Publication Year,Topic,Topic_Name,Topic_Probability,Abstract Note,DOI,Author
Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security,2020,1,Topic_1_data_big_big data,0.7163932708205908,"Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.",10.1109/ICECOCS50124.2020.9314391,A. Z. Faroukhi; I. El Alaoui; Y. Gahi; A. Amine
A Comparative Perspective on Technologies of Big Data Value Chain,2023,1,Topic_1_data_big_big data,1.0,"Data is one of the most valuable assets in the digital era because it may conceal hidden valuable insights. Diverse organizations in diverse domains overcome the challenges of the big data value chain by employing a wide range of technologies to meet their needs and achieve a variety of goals to support their decision-making. Due to the significance of data-oriented technologies, this paper presents a model of the big data value chain based on technologies used in the acquisition, storage, and analysis of data. The following are the paper’s contributions: First, a model of the big data value chain is developed to illustrate a comprehensive representation of the big data value chain that depicts the relationships between the characteristics of big data and the technologies associated with each category. Second, in contrast to previous research, this paper presents an overview of technologies for each category of the big data value chain. The third contribution of this paper is to assist researchers and developers of data-intensive systems in selecting the appropriate technology for their specific application development use cases by providing examples of applications and use cases from prominent papers in a variety of fields and by describing the capabilities and stages of the technologies being presented so that the right technology is used at the right time in the big data collection, processing, storage, and analytics tasks.",10.1109/ACCESS.2023.3323160,A. A. Aydin
An Hybrid Approach to Quality Evaluation across Big Data Value Chain,2016,1,Topic_1_data_big_big data,0.750972164654855,"While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.",10.1109/BigDataCongress.2016.65,M. A. Serhani; H. T. El Kassabi; I. Taleb; A. Nujum
A Comprehensive Value Assessment Method for Energy Big Data Based on Data Value Chain,2024,-1,Outliers,0.26849196640126305,"Through the deep integration and intelligent analysis of energy big data, comprehensive perception, accurate prediction and optimization control of the energy system can be realized, but the problem of data value assessment has become one of the obstacles restricting the development of energy big data. This paper proposes a comprehensive value assessment method for energy big data based on the data value chain. It aims to construct a multi-dimensional assessment index system by comprehensively considering the value creation process of energy big data in data collection, processing, analysis and application. The core idea is to use the data value chain as a framework to decompose the value assessment of energy big data into multiple stages, and to design corresponding indicators for each stage, covering economic value, social value and environmental value, in order to comprehensively assess the integrated value of energy big data. The core idea is to use the data value chain as a framework to decompose the value assessment of energy big data into multiple stages and design corresponding indicators for each stage. It covers the aspects of economic value, social value and environmental value in order to comprehensively assess the comprehensive value of energy big data. And a case study is taken to prove the effectiveness of the proposed model, which provides a scientific basis for the effective use and value realization of energy big data.",10.1109/SGES63808.2024.10824114,S. Qi; D. Song; D. Zhang; L. Ju; Z. Zhang
Research on improved algorithm of logistics optimization based on big data,2023,3,Topic_3_industry_manufacturing_chain,0.6354923645012058,"As an important part of the modern service industry, the healthy development of the logistics industry has a very positive impact on improving the quality of the national economy and promoting the supply reform. With the arrival of the new economic era, the impact of the development of science and technology on logistics is deepening, the penetration speed is accelerating, and the competition between logistics industries is becoming increasingly fierce. For the logistics industry, in the face of endless new technologies, if you want to gain competitive advantage, you must improve the management level, especially the cost management level. In the information age, the goal of enterprise cost management should not be limited to the enterprise itself, but should be the cost management of the entire value chain. Therefore, it is of great significance to study the value chain management of logistics enterprises in the context of big data.",10.1109/BDICN58493.2023.00008,M. Xin; H. Zhu
A Multidimensional System Architecture Oriented to the Data Space of Manufacturing Enterprises,2021,3,Topic_3_industry_manufacturing_chain,0.5629070967882653,"The concept and characteristics of the data space model of manufacturing enterprises in various countries are expounded, and a multi-dimensional data system architecture oriented to the data space of manufacturing enterprises is proposed. The effective analysis and processing of big data in manufacturing enterprises can provide them with more effective model building, integrated retrieval and intelligent management strategies, so as to reduce costs and increase efficiency. A systematic overview of the data space of the entire system and the entire value chain of the manufacturing enterprises is carried out. First, the three dimensions of the business domain, the processing domain and the modal domain are clarified; secondly, the methods of applying data processing at each stage in each domain are explained; finally, the advantages and importance of the data model are summarized.",10.1109/ICCSS53909.2021.9722017,K. Lu; Z. Cheng; H. Ren; R. Lu
Towards Specification of a Software Architecture for Cross-Sectoral Big Data Applications,2019,1,Topic_1_data_big_big data,0.818911156711306,"The proliferation of Big Data applications puts pressure on improving and optimizing the handling of diverse datasets across different domains. Among several challenges, major difficulties arise in data-sensitive domains like banking, telecommunications, etc., where strict regulations make very difficult to upload and experiment with real data on external cloud resources. In addition, most Big Data research and development efforts aim to address the needs of IT experts, while Big Data analytics tools remain unavailable to non-expert users to a large extent. In this paper, we report on the work-in-progress carried out in the context of the H2020 project I-BiDaaS (Industrial-Driven Big Data as a Self-service Solution) which aims to address the above challenges. The project will design and develop a novel architecture stack that can be easily configured and adjusted to address cross-sectoral needs, helping to resolve data privacy barriers in sensitive domains, and at the same time being usable by non-experts. This paper discusses and motivates the need for Big Data as a self-service, reviews the relevant literature, and identifies gaps with respect to the challenges described above. We then present the I-BiDaaS paradigm for Big Data as a self-service, position it in the context of existing references, and report on initial work towards the conceptual specification of the I-BiDaaS software architecture.",10.1109/SERVICES.2019.00120,I. Arapakis; Y. Becerra; O. Boehm; G. Bravos; V. Chatzigiannakis; C. Cugnasco; G. Demetriou; I. Eleftheriou; J. Etienne Mascolo; L. Fodor; S. Ioannidis; D. Jakovetic; L. Kallipolitis; E. Kavakli; D. Kopanaki; N. Kourtellis; M. Maawad Marcos; R. Martin de Pozuelo; N. Milosevic; G. Morandi; E. Pages Montanera; G. Ristow; R. Sakellariou; R. Sirvent; S. Skrbic; I. Spais; G. Vasiliadis; M. Vinov
Multimodal Data Value Chain (M-DVC): A Conceptual Tool to Support the Development of Multimodal Learning Analytics Solutions,2020,-1,Outliers,0.16226848128533936,"Multimodal Learning Analytics (MMLA) systems, understood as those that exploit multimodal evidence of learning to better model a learning situation, have not yet spread widely in educational practice. Their inherent technical complexity, and the lack of educational stakeholder involvement in their design, are among the hypothesized reasons for the slow uptake of this emergent field. To aid in the process of stakeholder communication and systematization leading to the specification of MMLA systems, this paper proposes a Multimodal Data Value Chain (M-DVC). This conceptual tool, derived from both the field of Big Data and the needs of MMLA scenarios, has been evaluated in terms of its usefulness for stakeholders, in three authentic case studies of MMLA systems currently under development. The results of our mixed-methods evaluation highlight the usefulness of the M-DVC to elicit unspoken assumptions or unclear data processing steps in the initial stages of development. The evaluation also revealed limitations of the M-DVC in terms of the technical terminology employed, and the need for more detailed contextual information to be included. These limitations also prompt potential improvements for the M-DVC, on the path towards clearer specification and communication within the multi-disciplinary teams needed to build educationally-meaningful MMLA solutions.",10.1109/RITA.2020.2987887,S. K. Shankar; M. J. Rodríguez-Triana; A. Ruiz-Calleja; L. P. Prieto; P. Chejara; A. Martínez-Monés
Enterprise Definition for Industry 4.0,2018,3,Topic_3_industry_manufacturing_chain,1.0,"In the modern world of mining and manufacturing, it is pivotal for information to flow from the enterprise level, i.e., Business Intelligence (BI) and Enterprise Resource Planning (ERP), down to the shop floor. Information generated from the shop floor should conversely be shared with the enterprise level in real or semi real-time. Business partners should be integrated to form a value chain that self-adjusts, generates and shares information. The absence of real-time flow of information makes it difficult to inform business decisions. The introduction of Industry 4.0 yields the convergence of fragmented business systems and partners. This results in the seamless integration and communication in delivering premium operations. This paper proposes an architecture of a fully integrated enterprise. The architecture comprises systems from such levels as decision making, operations management, and shop floor. Big Data, Data Analytics, and Cyber Physical Systems (CPS) are harnessed to accomplish a 4th industrial revolution.",10.1109/IEEM.2018.8607642,A. Telukdarie; M. N. Sishi
To Find out High-Profit Zone From Industrial Value Chain Perspective in Big Data Era : Real Estate Sector as Case Study,2022,3,Topic_3_industry_manufacturing_chain,0.6974826814860474,"In the 21th century, economic competition takes place neither at corporate nor commercial mode level. Martin Christopher points out that the competition in the 21th century is not among enterprises, but between the supply chains behind each enterprise. An enterprise establishes an industrial value chain and competes against its counterpart who has its own industrial value chain. Also big data can offer new processing mode to generate decision making power, insight and discovery. For a good company, its assets and scale of operation should not be inevitable indicators because assets are bygone accumulation and profits serve as interim management performance. Future treasure can only be reflected by the value chain. Only under the guidance of value chain top-level design,together with mathematical modeling can a company finds out its high profit zone. And this shows that the company conducts strategic management in time-based competition stage.",10.1109/ICDSBA57203.2022.00103,X. Bao
Research on Big Data Reference Architecture Model,2020,1,Topic_1_data_big_big data,1.0,"For ISO / / IEC TS 25011:2017 some deficiencies of big data reference system. In this paper, we redesigned the big data reference system structure model, and proposed a goal (big data information service quality), three chains (information value chain, information technology value chain and information assurance value chain) and five roles (big data application provider, big data framework provider and big data information assurance provider) 1-3-5 model of data provider and data consumer, and all the attributes of the model are described. In the part of big data information service quality, the deficiencies in ISO / / IEC TS 25011:2017 are corrected. Then, the basic system of big data is proposed. Under the big data reference architecture model proposed in this paper, compared with some other typical models, the conclusion is that other typical models can be replaced completely.",10.1109/ICAIBD49809.2020.9137451,L. Xiaofeng; L. Jing
Data Value Networks: Enabling a New Data Ecosystem,2016,1,Topic_1_data_big_big data,0.48246565483501397,"With the increasing permeation of data into all dimensions of our information society, data is progressively becoming the basis for many products and services. It is hence becoming more and more vital to identify the means and methods how to exploit the value of this data. In this paper we provide our definition of the Data Value Network, where we specifically cater for non-tangible data products. We also propose a Demand and Supply Distribution Model with the aim of providing insight on how an entity can participate in the global data market by producing a data product, as well as a concrete implementation through the Demand and Supply as a Service. Through our contributions we project our vision of generating a new Economic Data Ecosystem that has the Web of Data as its core.",10.1109/WI.2016.0073,J. Attard; F. Orlandi; S. Auer
Automotive Big Data Pipeline: Disaggregated Hyper-Converged Infrastructure vs Hyper-Converged Infrastructure,2020,1,Topic_1_data_big_big data,1.0,"Big data disrupts everything it touches, but automotive is probably one of the top industries that enjoy and leverage the benefits. The Automotive Big Data Pipeline (ABDP) is a Big Data pipeline base on the automotive use case and is required to scale up agile and high performance in real-time or in batch. Nonetheless, there're many alternative infrastructure designs but lack of knowledge, which fits the best for the automotive domain. It leads this paper into a question: What kinds of infrastructure design could provide better performance for the ABDP?In this paper, we introduce two well-known infrastructure designs called Hyper-Converged infrastructure (HCI) and Disaggregated Hyper-Converged infrastructure (DHCI). HCI combines standard data center hardware using locally attached storage resources to create fast, common building blocks. However, does single standard hardware fit all the requirements? DHCI scale independently from compute and storage provides an option. It provides a more cost-efficient and flexible solution; however, there is no comparison from the performance point of view. Therefore, to address it, our objective is to conduct an empirical performance comparison to see which one performs better.The experiment result shows that DHCI performs almost the same as HCI on CPU utilization, memory, and network consumption. However, regarding storage and running time metrics, DHCI performs slightly higher storage throughput, IOPs, and less running time than HCI.",10.1109/BigData50022.2020.9378045,C. J. Wang; B. Kim
Big Data Pipeline with ML-Based and Crowd Sourced Dynamically Created and Maintained Columnar Data Warehouse for Structured and Unstructured Big Data,2020,1,Topic_1_data_big_big data,0.7316533583586019,"The existing big data platforms take data through distributed processing platforms and store them in a data lake. The architectures such as Lambda and Kappa address the real-time and batch processing of data. Such systems provide real time analytics on the raw data and delayed analytics on the curated data. The data denormalization, creation and maintenance of a columnar dimensional data warehouse is usually time consuming with no or limited support for unstructured data. The system introduced in this paper automatically creates and dynamically maintains its data warehouse as a part of its big data pipeline in addition to its data lake. It creates its data warehouse on structured, semi-structured and unstructured data. It uses Machine Learning to identify and create dimensions. It also establishes relations among data from different data sources and creates the corresponding dimensions. It dynamically optimizes the dimensions based on the crowd sourced data provided by end users and also based on query analysis.",10.1109/ICICT50521.2020.00018,K. Ghane
Maritime data technology landscape and value chain exploiting oceans of data for maritime applications,2017,1,Topic_1_data_big_big data,0.6458731996768873,"Maritime areas covers a large percentage of our world, being most of this area unexplored. Despite this, the sea has one of the most valuable and mostly exploited “economic platforms” of mankind, with applications in different sectors (as fishing industry, transportation cargo, etc.). Although this situation and the great evolution in technology can contribute to better know of the sea, this has not been happening. Given that a systematic collection of maritime data has already been carried out, yet is still dispersed and not used in its entirety. This is one of the objectives of the H2020 BigDataOcean project (http://www.bigdataocean.eu/site/), collecting the various data sources and thus being able to treat them together in order to obtain better results. This paper presents the analysis of the current landscape of big data, starting from the identification of existing ones, used tools and methodologies to be integrated in the project services, and platform with the aim of retrieving and analyzing the maritime data is presented. Then, the requirement engineering methodology is presented, being the methodology used during the project to identify the stakeholders, data sources, data value chain and the technologic gaps, resulting the in the identification of the first iteration of the requirements.",10.1109/ICE.2017.8280006,J. Ferreira; C. Agostinho; R. Lopes; K. Chatzikokolakis; D. Zissis; M. -E. Vidal; S. Mouzakitis
Design of big data-driven framework based on manufacturing value chain,2021,3,Topic_3_industry_manufacturing_chain,1.0,"With the proposal of intelligent manufacturing 2025, many manufacturing industries are exploring in upgrading and transformation. The reason is the country’s overall economic level cannot be improved without the support of the manufacturing industry. Since manufacturing activities involve multi-link resource integration, use of a variety of high and new technologies, and require a lot of labor force. To achieve high earnings, it is necessary to analyze and optimize the whole process of business links. However, the actual production process involves a large number of equipments and workers. This has created some ongoing challenges for data collection, storage, analysis and optimization. This paper focuses on the value chain of manufacturing enterprises, and proposes a big data-driven framework for manufacturing based on Internet of Things cloud platform. The framework is data-driven and aims at the value-added of enterprise value chain. Multiple production links are considered to be linked to the cloud. The dynamic feedback information of each link is used to adjust business operations. This paper theoretically analyzes the data flow process of manufacturing enterprises in the corporate value-added chain. Taking the production process as an example, the paper explains how the value-added of production can be obtained from a data perspective. This will help to speed up the process of digital and intelligent transformation of manufacturing.",10.1109/ICAMechS54019.2021.9661489,J. Song; A. Wang; P. Liu; D. Li; X. Han; Y. Yan
Engineering Big Data to Small Businesses: Lessons Learned from a Case Study,2018,-1,Outliers,0.2222666017844368,"Big Data has become an important technical force to advance many industries. Many big data techniques have been invented and open-sourced for public use. The small businesses, which form the major part of the whole business world, still face great challenges in applying big data solutions to their own businesses. However, there are few cases that have been published to report the procedure of engineering big data in small businesses. This fact leads to insufficient references for small businesses to take to figure out their own scenarios of applying big data. In this paper, we report a pilot case study on a small business applying big data to an electric signal process project. It describes in detail the procedure of analyzing the business logics, identifying big data requirements, and selecting appropriate big data techniques for engineering solutions. We also share lessons learned from this case study, which can be a general reference for those small businesses on trying their own big data application cases.",10.1109/BigDIA.2018.8632796,C. Jia; D. Jing; Y. Yang; P. Fan; W. Sun; Y. Feng
PoPI Act - opt-in and opt-out compliance from a data value chain perspective: A South African insurance industry experiment,2016,2,Topic_2_data_privacy_security,0.8614496233210088,"Personal information is collected and processed by various companies when individuals buy products and services, share their information on social media or enter their details in competitions and so on. This personal information, which could potentially also be shared with third party companies, is analyzed to tailor services and products to consumer's preferences and online behavior, with the objective of creating a data value chain. When the Protection of Personal Information (PoPI) Act (2013) comes into effect in South Africa, companies will have to comply with the conditions of PoPI and protect individuals' personal information accordingly. Companies will only be allowed to use personal information for the agreed purpose it was collected for and must obtain individuals' consent to share or further process their information. This research sets out to monitor the flow of personal information through an experiment to establish if data value chains are shaped within the South African insurance industry, and to establish whether the consumer's personal information, which is part of the data value chain, is processed in line with certain conditions of PoPI. The experiment highlighted that some of the insurance companies in the selected sample did not comply with the opt-in or opt-out preferences of the researcher. In addition some did not meet with the condition to obtain consent before sharing personal information with third parties for marketing purposes. No formal data value chains could be identified during the time frame of this experiment as it was found that the researcher was contacted randomly about generic marketing and communication offerings.",10.1109/ISSA.2016.7802923,P. Swartz; A. Da Veiga
"A Study of Big Data Analytics: Tools, Applications, and Information Value Chain",2024,1,Topic_1_data_big_big data,1.0,"In the internet world, data is generated quickly, every second, from different sources. Therefore, various applications, including social networking sites, e-commerce websites, review websites, mobile apps, and various blogs, generate terabytes of random data in their state. So, it is a big problem to store and manage data. Extracting relevant information to make decisions on these big data sets requires a lot of effort at various levels. As a result, big data analysis research is necessary in the automobile industry. In this paper, we focus on the meaningful impact of a big data analytic approach to achieve faster and more accurate results in various areas such as education, healthcare, insurance, banking and finance, manufacturing, government, and marketing, and also play a big role in achieving sustainable development goals. In this paper, we propose a new information value chain framework for big data analytics processes. We also examine some important analysis tools that can handle huge amounts of data.",10.1109/ICAIT61638.2024.10690840,M. Pandey; A. S. Bist
A Decoupled Data Pipeline and its Reliability Assessment: Case Study in Extreme Climatic Humans Studies,2021,1,Topic_1_data_big_big data,1.0,"Data platforms with an ability to capture, process and analyze high frequency streaming data from vast complex systems reliably with high scalability is the central problem in this paper. Particularly when developed data platforms are to be deployed within extreme climatic conditions. We have developed a decoupled data pipeline by fusing multiple services within edge and cloud computing paradigms following design principles in software reliability to provide optimal services during vast human studies in extreme climatic simulations. The data pipeline is demonstrated through a pre-deployment acclimation case study in climatic chambers. Performance evaluation of each service within the developed pipeline shows high levels of reliability and availability across multiple testing and study simulations. As a result, real-time data from multiple complex data sources are made available for on time personalized analytics for analysis of physiological responses to austere environmental conditions.",10.1109/BigData52589.2021.9671597,C. Inibhunu; J. Yeung; A. Gates; B. Chicoine; C. McGregor
Research of Information Management System by Big Data Algorithm and Computer visualization,2021,1,Topic_1_data_big_big data,1.0,"The article not only analyzes the impact of the big data value chain on the four processes of forecasting, decision-making, control and evaluation from the perspective of financial activities, but also analyzes the impact of big data technology on stakeholders from the perspective of financial the talents analyze the challenges in the application of big data. At the end of the article, an enterprise financial management information system is established based on computer big data algorithms. Combining SWT technology here, we elaborate on the specific design of the enterprise financial management system, and use actual operations in the financial system implementation to demonstrate the financial system process through the system interface. The graphical user interface developed based on SWT realizes corporate finance. The main function of management. Practice has shown that the system is not only simple to operate, but also has a friendly interface. It is a flexible, reliable, fully functional and practical information management system.",10.1109/ICESIT53460.2021.9696680,N. Zhang; H. Duan
My (fair) big data,2017,1,Topic_1_data_big_big data,0.8601401603407659,"Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.",10.1109/BigData.2017.8258267,T. Catarci; M. Scannapieco; M. Console; C. Demetrescu
Big Data Pre-Processing: Closing the Data Quality Enforcement Loop,2017,1,Topic_1_data_big_big data,0.7316650565781841,"In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.",10.1109/BigDataCongress.2017.73,I. Taleb; M. A. Serhani
An Outline on Big Data and Big Data Analytics,2018,1,Topic_1_data_big_big data,1.0,"With the rapid advancements in technological applications have led to the flooding of data from various sources like web, social network data, business data, medical records, etc. over the preceding years. As compared to traditional data, big data reveals a unique characteristic from its three V's which means big data is unstructured. In this era, the emerging trend requires the involvement of advanced data analysis, acquisition and management techniques to mine and collect appropriate data in a structured way. In this paper, we describe the definitions and the challenges of big data systems. Next, a systematic framework decomposes the architecture of big data systems into four stages like data generation, data acquisition, data storage and data analytics. These stages form the basis of big data value chain. Finally, some solutions are discussed to tackle the challenges of big data and future attention is required for big data systems.",10.1109/ICACCCN.2018.8748683,D. Gaurav; J. K. P. Singh Yadav; R. K. Kaliyar; A. Goyal
SIM-PIPE DryRunner: An approach for testing container-based big data pipelines and generating simulation data,2022,1,Topic_1_data_big_big data,1.0,"Big data pipelines are becoming increasingly vital in a wide range of data intensive application domains such as digital healthcare, telecommunication, and manufacturing for efficiently processing data. Data pipelines in such domains are complex and dynamic and involve a number of data processing steps that are deployed on heterogeneous computing resources under the realm of the Edge-Cloud paradigm. The processes of testing and simulating big data pipelines on heterogeneous resources need to be able to accurately represent this complexity. However, since big data processing is heavily resource-intensive, it makes testing and simulation based on historical execution data impractical. In this paper, we introduce the SIM - PIPE Dry Runner approach - a dry run approach that deploys a big data pipeline step by step in an isolated environment and executes it with sample data; this approach could be used for testing big data pipelines and realising practical simulations using existing simulators.",10.1109/COMPSAC54236.2022.00182,A. Thomas; N. Nikolov; A. Pultier; D. Roman; B. Elvesæter; A. Soylu
Big data gathering and mining pipelines for CRM using open-source,2015,1,Topic_1_data_big_big data,0.7273611073067439,"Customer Relationship Management (CRM) is currently the fastest growing sector of enterprise software, estimated to increase to $36.5B worldwide by 2017. CRM technologies increasingly use data mining primitives across multiple applications. At the same time, the growth of big data has led to the evolution of an open source big data software stack (primarily powered by Apache software) that rivals traditional enterprise database (RDBMS) stacks. New technologies such as Kafka, Storm, HBase have significantly enriched this open source stack, alongside more established technologies such as Hadoop MapReduce and Mahout. Today, enterprises have a choice to make regarding which stack they will choose to power their big data applications. However, there are no published studies in literature on enterprise big data pipelines built using open source components supporting CRM. Specific questions that enterprises have include: how is the data processed and analyzed in such pipelines? What are the building blocks of such pipelines? How long does each step of this processing take? In this work, we answer these questions for a large scale (serving over a 100M customers) industrial CRM pipeline that incorporates data mining, and serves several applications. Our pipeline has, broadly, two parts. The first is a data gathering part that uses Kafka, Storm, and HBase. The second is a data mining part that uses Mahout and Hadoop MapReduce. We also provide timings for common tasks in the second part such as data preprocessing for machine learning, clustering, reservoir sampling, and frequent itemset extraction.",10.1109/BigData.2015.7364128,K. Li; V. Deolalikar; N. Pradhan
Research on the Influencing Factors of Energy Big Data Value Based on DEMATEL-ISM Model,2024,-1,Outliers,0.2869293813704062,"Clarifying the influencing factors of energy big data value is an important part of releasing the value of energy big data assets and promoting the in-depth development of the energy industry, but the current research on the influence factors of energy big data value is relatively lacking. Therefore, based on the energy big data value chain, this paper analyzes the influencing factors affecting the whole link from energy big data value generation to value realization. Then, it establishes the DEMATEL-ISM model. Finally, it optimizes and screens the factor set based on the DEMATEL-ISM model, clarifies the intrinsic structural relationship among the factors, and reveals the key influencing factors and the degree of influence.",10.1109/SGES63808.2024.10824122,Y. Feng; S. Wang; Q. Li; L. Ju; Z. Zhang
A privacy weaving pipeline for open big data,2016,2,Topic_2_data_privacy_security,1.0,"The power of big data gives us an unprecedented chance to understand, analyze, and recreate the world, while open data ensures that power be shared and widely exploited. Open and big data has become the emerging topics for researchers and governments. Thus, the related privacy issues also become an emerging urgent problem. In this work, we propose a conceptual framework of privacy weaving pipeline dedicated for producing open and big data while preserving privacy. Within the processing pipeline, each step of the process flow considers the privacy assurance to manipulate datasets. However, the complexity of process flow is the same as normal data pipeline. The experimental prototype confirms the feasibility of framework design. We hope this work will facilitate the development of open and big data industry.",10.1109/ASONAM.2016.7752362,Y. -C. Yu; D. -R. Tsai
"A Framework for the Design, Development, Testing and Deployment of Reliable Big Data Platforms",2022,1,Topic_1_data_big_big data,1.0,"We consider the problem of reliability in big data science projects that are comprised of multiple computing platforms and complex architectures that harness data. Specifically on their ability to capture, process and analyze streaming high frequency data from vast complex systems reliably with effective scalability for deployment in vast domains such as clinical care, smart cities or within extreme climatic work environments. This paper introduces a framework to enable reliable data science projects by integrating multiple computing principles of autonomy, local responsibility, fault tolerance, symmetry, decentralization, well-understood building blocks, and simplicity. The designed framework is applied in the development of a decoupled data pipeline demonstrated through a case study on pre-deployment acclimation strategies that is continuously monitored to ensure reliability and availability is effectively quantified.",10.1109/BigData55660.2022.10020382,C. McGregor; C. Inibhunu
DEF-PIPE: Domain Specific Language Visualization for Big Data Pipelines,2023,1,Topic_1_data_big_big data,0.7634980012233145,"The complexity of Big Data analysis requires a combination of different software components into a pipeline performing different analysis steps. Supporting such pipelines requires different expertise provided by different actors: domain experts and technical/computing experts. The main objective of this work is to support domain experts with a tool for pipeline description, which does not require deep technical knowledge about the deployment and execution of Big Data pipelines. We present a solution to visualize Big Data pipeline description using the DEF-PIPE tool. The solution shows that the process of pipeline description is simple and intuitive for users who are not experts in computing. At the same time, DEF-PIPE automatically generates a textual description of the designed data pipelines, which contains the necessary information for simulation, adaptation, deployment, and resource management of the pipeline. In this case, a separation of concerns between the design and run-time phases of the Big Data pipeline lifecycle is supported. This solution allows us to bridge the gap between domain and technical experts. Providing libraries of steps and pipelines also allows the reusing of previously developed solutions.",10.1109/CSCI62032.2023.00253,A. L. Kheirabadi; V. Mitrovic; K. Dinh; Y. Chang; M. Matskin
Big Data Pipelines on the Computing Continuum: Ecosystem and Use Cases Overview,2021,1,Topic_1_data_big_big data,0.7961586335930637,"Organisations possess and continuously generate huge amounts of static and stream data, especially with the proliferation of Internet of Things technologies. Collected but unused data, i.e., Dark Data, mean loss in value creation potential. In this respect, the concept of Computing Continuum extends the traditional more centralised Cloud Computing paradigm with Fog and Edge Computing in order to ensure low latency pre-processing and filtering close to the data sources. However, there are still major challenges to be addressed, in particular related to management of various phases of Big Data processing on the Computing Continuum. In this paper, we set forth an ecosystem for Big Data pipelines in the Computing Continuum and introduce five relevant real-life example use cases in the context of the proposed ecosystem.",10.1109/ISCC53001.2021.9631410,D. Roman; N. Nikolov; A. Soylu; B. Elvesæter; H. Song; R. Prodan; D. Kimovski; A. Marrella; F. Leotta; M. Matskin; G. Ledakis; K. Theodosiou; A. Simonet-Boulogne; F. Perales; E. Kharlamov; A. Ulisses; A. Solberg; R. Ceccarelli
An Approach for Data Pipeline with Distributed Query Engine for Industrial Applications,2020,3,Topic_3_industry_manufacturing_chain,0.5822242321868291,"The data driven services in industrial automation systems are transforming the world of automation industry by optimizing industrial processes and providing Value Added Services (VASs) with the grace of Industry 4.0, Big Data and Artificial Intelligence (AI). A demand driven data pipeline is essential to connect different industrial data sources in a shop floor with different data storage systems for service provisioning. This paper analyzes an experimental approach and corresponding challenges to optimize computing resource allocation in industrial applications to construct such demand driven data pipeline to provide data driven services through an open source, flexible and extensible distributed query engine known as Presto, which can perform interactive analytical queries for different purposes such as condition monitoring, asset management or many others.",10.1109/ETFA46521.2020.9212050,A. Ghosh Chowdhury; M. Illian; L. Wisniewski; J. Jasperneite
Big data in power systems leveraging grid optimization and wave energy integration,2017,-1,Outliers,0.18696132818045952,"Power systems have been through different challenges and technological innovations in the last years and are rapidly evolving into digital systems through the deployment of the smart grids concept. Producing large amounts of data, power systems can benefit from the application of big data analytics which can help leveraging the optimization processes going on in power grids nowadays. The whole value of chain of electric power can benefit from the application of big data techniques. This paper presents a short overview of possible applications and challenges that still need to be considered for this synergy to grow. Under the framework of an H2020 funded project named BigDataOcean, a case study will be described, showing how a data-driven approach can foster the development of offshore renewable sources using the example of wave energy.",10.1109/ICE.2017.8279997,N. Amaro; J. M. Pina
Dynamic Optimization of Specialty Structure of Higher Education Based on Big Data Technology,2021,-1,Outliers,0.3027459664825039,"At present, big data technology with big data collection, big data analysis and mining, machine learning and other technologies as the core has been widely used in many fields. Through deep mining and analysis of massive data, big data technology can accurately predict and judge the supply and demand of disciplinary professionals, thus providing strong support for the dynamic optimization of the specialty structure of higher education. Through building life cycle management plan of specialty structure of universities and colleges, this article determines the source of big data acquisition, and by using Naive Bayes Classifier classifies and describes the talent status data of the industry, establishes the data model, and statistics the current distribution of professional and technical talents, and uses the time series prediction method to predict the future demand trend of all kinds of professional and technical personnel of enterprise, to provide strong support for specialty structure optimization, thereby to achieve higher education specialty structure dynamic optimization.",10.1109/ICET52293.2021.9563177,H. Cui
A comparative study to classify big data using fuzzy techniques,2016,1,Topic_1_data_big_big data,0.8072737579876197,"It is very difficult to implement an efficient analysis by using the customary techniques currently available; this is due to the fact that the data size has had a huge increase. Many complications were faced because of the numerous characteristics of big data; some of them include complexity, value, variability, variety, velocity, and volume. The objective of this paper is to implement classification techniques using the map reduce framework using fuzzy and crisp methods, also to arrange for a study that can compare and contrast the outcomes of the suggested systems against the methods appraised in the documented works. For this research the applied method for the fuzzy technique is the fuzzy k-nearest neighbor, and for the non-fuzzy techniques both the support vector machine and the k-nearest neighbor are used. The use of the map reduce paradigm is applied to be able to process big data. We also implemented an integrated system using the Support Vector Machine with the fuzzy soft label and Gaussian fuzzy membership. Results show that fuzzy k-nearest neighbor classifier gives higher accuracy but it takes a lot of time in classification compared to the other techniques. But the outcomes when projected onto other data sets demonstrate that the suggested method that used fuzzy logic in the Reducer function gives higher accuracy and lower time than the new suggested methods and the methods revised in the paper.",10.1109/ICEDSA.2016.7818508,S. S. Labib
Research on the Evolution Path of Big Data Service Ecosystem based on Data Mining,2022,1,Topic_1_data_big_big data,1.0,"The big data service ecosystem, as an emerging form of data service innovation spawned under the big data environment, explores the evolution path of the big data service ecosystem, and provides theoretical support and reference decision-making for the construction and development of my country's big data service ecosystem. First, combine the theoretical ideas of the service ecosystem, analyze the connotation and characteristics of the big data service ecosystem, and build the general structure of the big data service ecosystem; secondly, use data mining theory to build big data mining, and analyze the big data service ecosystem on this basis System evolution path; finally, conduct empirical research through the big data service ecosystem. Research shows that the evolution of the big data service ecosystem has accelerated by 6.5%.",10.1109/ICIRCA54612.2022.9985026,P. Hou
Cost-Optimized Cloud Scheduling for ETL and Big Data Using AI,2025,1,Topic_1_data_big_big data,1.0,"Cloud-based data pipelines are critical for large-scale ETL and big data analytics, yet in-efficient scheduling leads to high costs and resource underutilization. Traditional approaches, such as static provisioning and rule-based auto-scaling, fail to adapt to dynamic workloads and ever-changing cloud pricing. This paper introduces an AI-driven cost-aware scheduling framework that integrates LSTM-based workload forecasting, Isolation Forest anomaly detection, and real-time cost analytics. Unlike purely reactive or threshold-based methods, this approach proactively adjusts resources to minimize costs and prevent performance bottlenecks. Experiments using the Google Cluster Workload Traces dataset and Azure VM Pricing data demonstrate up to a 30-40% reduction in operational expenses, along with improved processing efficiency and auto-scaling responsiveness. However, the proposed solution may require further adaptation for extreme-scale scenarios or specialized compliance environments. These findings highlight the novelty of combining predictive analytics, anomaly detection, and cost optimization, offering a robust strategy for enhancing performance and reducing expenses in cloud-based ETL and big data workflows.",10.1109/ISDFS65363.2025.11012004,C. Krishnama; R. Puchhakayala; S. Kotha; F. Gouri
Healthcare Industry: Embracing Potential of Big Data across Value Chain,2022,3,Topic_3_industry_manufacturing_chain,0.41240883340269285,"In the fast-moving era of the Industrial Revolution (Industry 4.0), digitally fueled devices and technologies are paramount for driving innovation and creating values across a myriad of industries. A case in point is - Healthcare Industry. Healthcare insurance companies, hospitals, and other providers around the world are belligerently leveraging digital tools and technologies such as Big Data analytics, Lake, Machine Learning, Artificial Intelligence, Internet of Things (IoT), Natural Language Processing, smart sensors, and the Internet of Things (IoT), for improving the overall quality of care and overall process efficiency and effectiveness. The Healthcare industry has been a center of discussion for embracing Big Data practice across the value chain for the past couple of decades due to the prodigious potential that is concealed in it. With so much abundant information, there have been numerous provocations related to the apiece stage of maneuvering big data that can only be amplified by leveraging high-end computer science results for big data analytics, as mentioned above. Well-organized healthcare ecosystem, analysis, and magnification of big data can influence the course of the game by opening new paths in terms of offering unique yet innovative products and services for the modern age technology-propelled healthcare value chain. This paper emphasizes the impetus of Big Data across the healthcare value chain, which involves the amalgamation of technology, data, and business, yielding better decisions and improving the experience across all touch points.",10.1109/ASSIC55218.2022.10088406,R. S. Jha; P. R. Sahoo; S. Mohapatra
Use of Multimodal Data Value Chain as a Contribution to the Management of the Teaching-Learning Process in Higher Education Institutions,2021,-1,Outliers,0.15986200799358555,"In education, data collection from students and teachers has occurred in physical spaces and, recently, more frequently in digital spaces. For this reason, the interaction of students and technologies offers an opportunity for multimodal data collection. We present the initial conceptual model of the Multimodal Data Value Chain (M-DVC). It clearly extracts and systematically specifies the raw evidence of learning required for a multimodal learning analytics solution (MMLA) that processes the data and converts it into meaningful information. We followed an educational action research methodology that integrated the researcher-educators into a collective process of producing and reproducing the knowledge necessary to transform the digital post-pandemic educational environment. The qualitative analysis of the MDVC conceptual model's processes made it possible to recognize the institution's characteristics. The analyses occurred in the macro (institution), meso (training program), and micro (subjects) contexts. The results defined the characteristics expected to be crucial for pedagogical decision-making based on results and reliable sources.",10.1109/IEEECONF53024.2021.9733752,J. A. Ruiz Ramírez; L. D. Glasserman-Morales
Big Data Pipeline Scheduling and Adaptation on the Computing Continuum,2022,1,Topic_1_data_big_big data,0.8998529747376447,"The Computing Continuum, covering Cloud, Fog, and Edge systems, promises to provide on-demand resource-as-a-service for Internet applications with diverse requirements, ranging from extremely low latency to high-performance processing. However, eminent challenges in automating the resources man-agement of Big Data pipelines across the Computing Continuum remain. The resource management and adaptation for Big Data pipelines across the Computing Continuum require significant research effort, as the current data processing pipelines are dynamic. In contrast, traditional resource management strategies are static, leading to inefficient pipeline scheduling and overly complex process deployment. To address these needs, we propose in this work a scheduling and adaptation approach implemented as a software tool to lower the technological barriers to the management of Big Data pipelines over the Computing Continuum. The approach separates the static scheduling from the run-time execution, em-powering domain experts with little infrastructure and software knowledge to take an active part in the Big Data pipeline adaptation. We conduct a feasibility study using a digital healthcare use case to validate our approach. We illustrate concrete scenarios supported by demonstrating how the scheduling and adaptation tool and its implementation automate the management of the lifecycle of a remote patient monitoring, treatment, and care pipeline.",10.1109/COMPSAC54236.2022.00181,D. Kimovski; C. Bauer; N. Mehran; R. Prodan
Smart Data Placement for Big Data Pipelines: An Approach based on the Storage-as-a-Service Model,2022,1,Topic_1_data_big_big data,1.0,"The development of big data pipelines is a challenging task, especially when data storage is considered as part of the data pipelines. Local storage is expensive, hard to maintain, comes with several challenges (e.g., data availability, data security, and backup). The use of cloud storage, i.e., Storageas-a-Service (StaaS), instead of local storage has the potential of providing more flexibility in terms of such as scalability, fault tolerance, and availability. In this paper, we propose a generic approach to integrate StaaS with data pipelines, i.e., computation on an on-premise server or on a specific cloud, but integration with StaaS, and develop a ranking method for available storage options based on five key parameters: cost, proximity, network performance, the impact of server-side encryption, and user weights. The evaluation carried out demonstrates the effectiveness of the proposed approach in terms of data transfer performance and the feasibility of dynamic selection of a storage option based on four primary user scenarios.",10.1109/UCC56403.2022.00056,A. Q. Khan; N. Nikolov; M. Matskin; R. Prodan; H. Song; D. Roman; A. Soylu
Research on the Construction Path of Energy Internet Ecosystem Based on Value Chain Analysis,2021,3,Topic_3_industry_manufacturing_chain,0.5691404695064723,"With the wide application of Internet technologies, cloud computing and big data technologies are gradually integrating with energy technologies promoting the connection between energy enterprises and upstream and downstream such as information service companies, equipment manufacturers, universities and research institutions, and gradually building an energy Internet ecosystem. Firstly, this paper introduces the typical construction mode of multi-dimensional industrial ecosystem, analyzes the typical operation mode, and then studies the main driving force of value creation in combination with the main body of the ecosystem. Secondly, it introduces the participants of the energy Internet ecosystem, and uses the value chain analysis method to study the value transmission of business flow, information flow and capital flow among the participants of the energy Internet ecosystem. Finally, the financial platform and data platform of the energy Internet ecosystem are introduced, and the industrial ecology theory is applied to construct the energy Internet ecosystem that improves overall economic performance. The article provides a construction path for energy Internet ecosystem based on value chain analysis.",10.1109/iSPEC53008.2021.9735864,Q. Yan; J. Zhu; S. Ma; L. Guo; C. Wu; G. Xue
Balancing the Complexity of Data Pipeline Engineering: A Technological Landscape Where Human Expertise Meets Large Language Models,2024,1,Topic_1_data_big_big data,0.7821325828931432,"Data pipeline engineering plays a critical role in managing cost optimization, scalability, reliability, and data governance challenges by ensuring that big data can be ingested, processed, and stored in a scalable and reliable way, to enable analysis and insights. Addressing these obstacles necessitates a combination of human expertise, suitable technologies, and custom hardware resources. In this paper we present a survey of existing research on data pipeline engineering. Subsequently, we introduce a complex data pipeline designed to infer aircraft flights from crowdsourced networks. We implement the data pipeline using suitable big data frameworks Apache Spark and Apache Sedona. We also provide a comprehensive review of the capabilities of prominent Large Language Models (LLMs) in assisting data pipeline engineering.",10.1109/ISNCC62547.2024.10758939,R. Moussa; T. Bejaoui
A Novel Rigorous Measurement Model for Big Data Quality Characteristics,2022,1,Topic_1_data_big_big data,0.7655469559717102,"Satisfiable data quality is the basic guarantee for data-based research, decision-making, and service. Today, new trends in the creation, collection, and utilization of data are constantly emerging. With the usage of massive data, the problem of data quality is highlighted. Several studies on the measurement, evaluation, and management of big data quality have been proposed, and the data quality problem in the big data environment has received attention. The big data characteristics Vs model describes the dimensions and attributes information of data sources in detail, which can be implemented in big data quality measurement. In this paper, a novel rigorous big data quality measurement architecture is proposed for automatically and parallelly quantifying the value of six big data Vs, which are Volume, Variety, Velocity, Veracity, Validity, and Vincularity according to the developed algorithms in every big data process step and time phase of the big data pipeline. Thresholds for the six big data Vs are provided correspondingly for analyzing the result values. The hierarchical measurement model is constructed with multiple-based measures, derived measures, and indicators. The model is verified by comparative experiments and experiments results indicate that the designed architecture can improve the outcomes of data source implementation.",10.1109/BigData55660.2022.10020564,H. Zou; K. Xiang
Research on regional collaborative innovation model of manufacturing resources based on value chain,2021,3,Topic_3_industry_manufacturing_chain,0.8361296111695516,"Manufacturing is the pillar and core competitiveness of the country. With the implementation of the ""Internet plus"" plan, the era of intelligent manufacturing has arrived. However, the home appliance manufacturing industry is facing many new challenges, such as the transition of smart home appliances, mass customization mode, regional industrial ecology construction and so on. In this paper, aiming at the problems of variability of the value chain and unclear value-added path in the process of regional smart home appliance manufacturing, a regional value chain architecture based on Porter diamond theory is proposed to realize the aggregation and sharing of regional home appliance manufacturing resources. And then from the perspective of embedded position and value-added path of value chain, the collaborative innovation model of manufacturing resources is constructed with mass customization mode as the main line, leading enterprises pulling and regional value chain integration, so as to realize global collaboration and whole process interaction of manufacturing resources in regional industrial ecosystem. Finally, taking Haier smart home as an example, the collaborative innovation model of regional value chain is applied to optimize the application of enterprise, upgrading the value-added path from the perspective of producer and purchaser, so as to realize the innovative development of collaborative mode of regional home appliance manufacturing industry.",10.1109/ITNEC52019.2021.9587148,S. Di; N. Yang; Y. Ding; H. Liu; J. Leng
Leveraging Cloud-Native Data Engineering for Big Data Analytics,2025,1,Topic_1_data_big_big data,1.0,"Recent evolution of traditional ETL systems into a cloud-native data engineering design for big data analytics capable of elastic and low-cost scaling. The proposed pipeline combines Apache Spark on Kubernetes, AWS Glue, and Delta Lake by leveraging microservices, containerization, serverless computing, and distributed orchestration. Based on the Taxi Trip dataset, the framework allows up to several times reduction in query execution time, processing throughput, and cost efficiency as compared to typical ETL approaches. Thus, the experimental results show that the query response is 75.9% faster and the cost is reduced by 64.5%. This research highlights the advantages of cloud-native architectures in maximizing real-time analytics and details in future work auto-scaling driven by AI and hybrid edge-cloud models.",10.1109/InCACCT65424.2025.11011292,S. Gupta; M. Sundararamaiah; G. Geeta
Towards Platform-Agnostic and Autonomous Orchestration of Big Data Services,2021,1,Topic_1_data_big_big data,0.8032642787986592,"Big data analytics and business insights are of high importance and demand among today’s services and applications. Traditionally, the entire big data pipeline goes through numerous processing steps. However, the complexity of supporting big data analytic applications is more than its recent reputation would suggest. On top of hybrid big data and high-performance computing resources, this paper presents a comprehensive microservices architecture to ease the management and enactment of end-to-end big data workflow management processes. It is developed along with intuitive graphical user interfaces to abstract and hide to the end user the specificities of the underlying network, storage and compute infrastructure. Entitled as Big Data Apps Composition Environment, it facilitates the design, composition, configuration, orchestration, enactment, and validation of end-to-end big data analytic services actuated into deployment workflows. Our approach differentiates to the current engines, as it adopts a big data-driven methodology which is scalable to multiple executors and has embedded notebooks for on-demand and real-time scripting analytics. Therefore, big data services and analytic applications deployment are being accelerated, while semi-automatic scaling through the definition of multiple executors for improved time performance of demanding tasks is supported.",10.1109/BigDataService52369.2021.00009,S. Iatropoulou; P. Petrou; S. Karagiorgou; D. Alexandrou
Big data analytics in public safety and personal security: Challenges and potential,2017,1,Topic_1_data_big_big data,1.0,"Public safety and personal security organizations need to achieve and maintain high levels of service, while managing the expectations of citizens, the mandates of legislation, and the impacts of new forms of technology, including mobile and IoT communications. In this context, Big and Linked Data analysis is expected to play a key role in the domain of Public Safety and Personal security industries. The paper presents the challenges in achieving efficient and secure big data analysis in this domain, in view of the current state-of-the-art in big and linked data technologies, and proposes a framework that brings together the data, the network & the technologies to create a curated, semantically enhanced, interlinked & multilingual platform for public & personal safety-related big data.",10.1109/ICE.2017.8280043,E. Biliri; P. Kokkinakos; A. Michailitsi; D. Papaspyros; J. Tsapelas; S. Mouzakitis; S. Koussouris; Y. Glickman; F. Kirstein
Data Pipeline Design for Dangerous Driving Behavior Detection System,2023,-1,Outliers,0.37884856073482304,"More than half of all traffic accidents are caused by non-compliance with safe driving duties. If the driver is awarded for carefully following safety regulations, the number of non-compliance with safety practices will drop significantly. Additionally, research is being conducted to reduce the number of cases by identifying driving behavior using data generated from a front camera of a vehicle. However, as data sources increase, there are difficulties in data analysis for artificial intelligence models and data science research. In this paper, we propose a data pipeline design for a dangerous driving behavior detection system. The proposed system is an data pipeline design that can assist refinement, processing, and analysis after collection of vehicle front camera data for learning dangerous driving behavior detection system.",10.1109/ICAIIC57133.2023.10067059,H. Jo; S. Yeom; H. Chen; K. Kim
Toward a Novel Measurement Framework for Big Data (MEGA),2021,1,Topic_1_data_big_big data,0.7389224091672201,"Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.",10.1109/COMPSAC51774.2021.00235,D. Bhardwaj; O. Ormandjieva
Finding Your Way Through the Jungle of Big Data Architectures,2021,1,Topic_1_data_big_big data,0.4701031410166174,"This paper presents a systematic review of common analytical data architectures based on DAMA-DMBOK and ArchiMate. The paper is work in progress and provides a first view on Gartner’s Logical Data Warehouse paradigm, Data Fabric and Dehghani’s Data Mesh proposal as well as their interdependencies. It furthermore sketches the way forward how this work can be extended by covering more architecture paradigms (incl. classic Data Warehouse, Data Vault, Data Lake, Lambda and Kappa architectures) and introducing a template with among others ""context"", ""problem"" and ""solution"" descriptions, leading ultimately to a pattern system providing guidance for choosing the right architecture paradigm for the right situation",10.1109/BigData52589.2021.9671862,T. Priebe; S. Neumaier; S. Markus
Stocks Analysis and Prediction Using Big Data Analytics,2019,1,Topic_1_data_big_big data,0.7212365984456771,"Big data analytics are used primarily in various sectors for accurate prediction and analysis of the large data sets. They allow the discovery of significant information from large data sets, otherwise, it is hidden. In this paper, an approach of robust Cloudera-Hadoop based data pipeline is proposed to perform analyses for any scale and type of data, in which selected US stocks are analysed to predict daily gains based on real time data from Yahoo Finance. The Apache Hadoop big-data framework is provided to handle large data sets through distributed storage and processing, stocks from the US stock market are picked and their daily gain data are divided into training and test data set to predict the stocks with high daily gains using Machine Learning module of Spark.",10.1109/ICITBS.2019.00081,Z. Peng
Research on DataOps Capability - Practice and Development,2023,1,Topic_1_data_big_big data,0.6876768032239199,"The rapid advancement of modern techniques, including big data, the Internet, cloud computing, communication technology, and artificial intelligence, has resulted in an increasing number of businesses relying on data to manage and monitor their operations. As a result, there is a substantial need for data. Given this immense demand, the secure, efficient, and high-quality delivery of data has become a critical concern for enterprises. This paper proposes a capability framework for the integration of data development and operations (DataOps) to help enterprises build complete DataOps capabilities.",10.1109/TrustCom60117.2023.00303,Z. Yin; S. Zhou; J. Zhou; M. Tian; M. Lin; S. Liu
Construction of Farmer's Financing Mode Under Internet Finance Embedded,2020,-1,Outliers,0.3034680559993695,"Under the guidance of rural revitalization strategy, as the main body of agricultural economy, the development of farmers has become an important factor. The natural attributes of agricultural products and other factors cause farmers to face financing difficulties, financing expensive dilemma. The financial model of Internet embedded in agriculture to a great extent alleviates the financing pressure of farmers. Through the questionnaire survey of farmers and the analysis of AHP, it is found that the value chain risk is taken as the standard layer of the choice of Internet financial model, and farmers are more concerned about the risk of enterprise financial management and credit risk.",10.1109/BDEIM52318.2020.00014,W. Zhengjun; M. Junling; Z. Yuehong
Business driving force models for big data environment,2016,3,Topic_3_industry_manufacturing_chain,0.569451834593328,"Companies and organizations are acutely admitting the discernment that business itself can be the driving force in developing new innovations and competitive strategies so that the critical and sustainable factor is whether they have it or not and that is one of the substantial factors in of the entity existence. In this research, concerning the enormously appearing and evaporating industries can capture revival opportunities with big data, we classify the driving force issues along with several viewpoints, and investigate these driving force issues models with mounting the viewpoints ought to be signified by the classifications. We first present the successful traditional driving force models and then applied to the big data environment. Second, the driving force models in accordance to the conventional theory, and profits origin, trailed by the elucidation about digitalization of conventional driving force models and investigation of several conceptualization associated with the essential applications.",10.1109/BIGCOMP.2016.7425928,Jumi Kim; W. Lee; Kwan-Hee Yoo
Analysis of the Development of Chinese Children's Animation Products on Mobile Terminals in the Context of Big Data,2021,1,Topic_1_data_big_big data,1.0,"From the perspective of practical application, there is a relationship of mutual promotion and coordinated development between the animation industry and the public cultural service system. The public cultural service system provides a platform for the animation industry and defines the content and objectives of the animation industry. Animation industry provides technical support for the public cultural service system. Big data has great economic and social value. The huge economic and social changes caused by big data also make more and more enterprises realize the importance and necessity of developing big data. Combined with the application requirements of big data center system, this paper systematically studies the implementation method of data retrieval from two aspects of spatial data and non spatial data, aiming to achieve the rapid and accurate acquisition of target data, and provide reference and basis for users' decision analysis.",10.1109/ICICCS51141.2021.9432165,J. Deng
Industrial track: Architecting railway KPIs data processing with Big Data technologies,2019,1,Topic_1_data_big_big data,1.0,"In our conducted research we have built the data processing pipeline for storing railway KPIs data based on Big Data open-source technologies - Apache Hadoop, Kafka, Kafka HDFS Connector, Spark, Airflow and PostgreSQL. Created methodology for data load testing allowed to iteratively perform data load tests with increased data size and evaluate needed cluster software and hardware resources and, finally, detected bottlenecks of solution. As a result of the research we proposed architecture for data processing and storage, gave recommendations on data pipeline optimization. In addition, we calculated approximate cluster machines sizing for current dataset volume for data processing and storage services.",10.1109/BigData47090.2019.9006196,A. Suleykin; P. Panfilov; N. Bakhtadze
Managing Big Data Stream Pipelines Using Graphical Service Mesh Tools,2021,1,Topic_1_data_big_big data,1.0,"Current big data frameworks like Apache Flink and Spark enable efficient processing of large-scale streaming data in a distributed setup. For the management of such data pipelines and the computing resources, we propose a combination of a graphical tool for pipeline management, Apache StreamPipes, and container management tools like Kubernetes. For evaluation, we implemented a use case with data preprocessing, vehicle power consumption, and driving behavior services in StreamPipes. We discuss the capabilities of StreamPipes in managing and executing complex stream processing pipelines and also evaluate the possible integration of container and service mesh tools (i.e., Istio) with StreamPipes. Furthermore, we implemented and evaluated a service management layer in our system design to provide extended features. In particular, we evaluated the delay when such a complex pipeline is restarted, e.g. for updates or reconfiguration.",10.1109/IEEECloudSummit52029.2021.00014,M. Faizan; C. Prehofer
Big Data Quality: A Survey,2018,1,Topic_1_data_big_big data,0.7582549464073972,"With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.",10.1109/BigDataCongress.2018.00029,I. Taleb; M. A. Serhani; R. Dssouli
A Study of Business Insight Tool using Big Data Analytics,2023,1,Topic_1_data_big_big data,0.5688668240851917,"In the generation of modern and advanced technologies, an immense proportion of information is accessible. Big Data is a huddle of huge quantities of data which keeps on growing exponentially with time. Because of the expeditious widening of day to day information and data, resolutions have to be investigated and given with the aim of managing and bringing out important valuable insights from the information databases. Moreover, it is mandatory for business strategists to obtain knowledge from huge swiftly changing data. Big data analytics are acquiring huge dominance in all the domains of business development and management. Further research illustrates that logistic networks and business activities are one of the most gigantic sources of information in the company. Hence, their business strategy building methodology would be beneficial from accumulated utilization of business data analytics technologies. However, there is still a deficiency of recognizing what influences the capability of a company to build business data analytics technologies to gain competitive insights from it. In this study, we focus on drawing helpful insights from the data stored in the company's database. In today’s scenario where we have a large database with n number of tables having millions of rows, it becomes impossible for a human to explore the information and bring out perceptions from it. Tools like MS Excel fail in analyzing such a large amount of data. As a solution to it, we as a BI Developer made a simplified tool in Power BI which acts as a coupling between the data warehouse and employees. Using that tool, the employees can pull out inner sights from the large database within seconds.",10.1109/CICTN57981.2023.10140726,R. Rana; N. Paliwal; A. Singhal
An Open-framework Big Data Analytic Platform Applied to Timeseries Sensor Data,2024,1,Topic_1_data_big_big data,1.0,"Emerging use-cases that involve vast amounts of high-resolution sensor data are prompting utilities to reconsider their conventional approaches to handling such data. In this paper, an open-platform solution is proposed for a big data analytics platform that leverages vendor-distributed software. The platform outlines a comprehensive solution to effectively handle large volumes of high-resolution sensor data, specifically targeting substation digital fault recorders. This open-platform approach is intended to be extensible to various other data streams and sensor types. To establish an end-to-end data pipeline from the edge to centralized databases, employed are a combination of tools and processes. This includes deploying an edge agent, MiNiFi, data ingestions using NiFi, and an extract-transform-load (ETL) process powered by Spark and Hive. Scalable device management is achieved through Cloudera Edge Management. Additionally, a database schema is introduced tailored to PoW data to ensure data integrity across multiple DFRs. Throughout the implementation of this approach, challenges included addressing high CPU usage stemming from the MiNiFi agent, configuring the Hadoop system, converting Spark code, and optimizing the ETL processing of continuous PoW files. By adopting an open-platform approach, adaptability of this platform provides insights into addressing key implementation challenges.",10.1109/TD47997.2024.10556121,M. Balestrieri; J. Capuzzi; K. Chang; H. Valizadehhaghi
Research on economic risk model of global value chain based on Artificial Intelligence,2022,3,Topic_3_industry_manufacturing_chain,0.4879088705149124,"With the deepening of economic globalization, the economic ties between countries in the world are becoming increasingly close. Many countries have completed vertical division of labor and cooperation. A global value chain is a state in which each country undertakes the production of part of the product, and uses foreign intermediate products to complete the final product and export it, thereby promoting the formation of a global value chain. Based on the analysis of the shape of the global value chain, this research uses an algorithm to establish an economic risk prediction model to achieve economic risk trend prediction and provide theoretical support for global value chain economic risk avoidance.",10.1109/CEI57409.2022.9950188,S. Liu; F. Liu
An Explainable AI Data Pipeline for Multi-Level Survival Prediction of Breast Cancer Patients Using Electronic Medical Records and Social Determinants of Health Data,2024,-1,Outliers,0.36899899214106524,"This study introduces an innovative explainable AI (XAI) pipeline designed to predict breast cancer survival by integrating clinical, socioeconomic, and geographic data. Using data from 10,172 patients treated at hospitals in the Memphis, Tennessee metropolitan area, the pipeline identifies key survival determinants and reveals significant survival disparities affecting Black women. Advanced machine learning models combined with SHapley Additive exPlanations (SHAP) provide actionable and interpretable insights into the role of tumor stage, socioeconomic conditions, and access to preventive care. This framework facilitates personalized survival predictions and targeted equity-focused interventions, demonstrating the potential of multi-source data integration to address health inequities and improve patient outcomes.",10.1109/BigData62323.2024.10825044,S. Hashtarkhani; S. White-Means; S. Li; R. Rashid; F. Kumsa; C. Lemon; L. Chipman; J. Dapremont; B. White; A. Shaban-Nejad
TemPredict: A Big Data Analytical Platform for Scalable Exploration and Monitoring of Personalized Multimodal Data for COVID-19,2021,-1,Outliers,0.47715488856737287,"A key takeaway from the COVID-19 crisis is the need for scalable methods and systems for ingestion of big data related to the disease, such as models of the virus, health surveys, and social data, and the ability to integrate and analyze the ingested data rapidly. One specific example is the use of the Internet of Things and wearables (i.e., the Oura ring) to collect large-scale individualized data (e.g., temperature and heart rate) continuously and to create personalized baselines for detection of disease symptoms. Individualized data, when collected, has great potential to be linked with other datasets making it possible to combine individual and societal scale models for further understanding the disease. However, the volume and variability of such data require novel big data approaches to be developed as infrastructure for scalable use. This paper presents the data pipeline and big data infrastructure for the TemPredict project, which, to the best of our knowledge, is the largest public effort to gather continuous physiological data for time-series analysis. This effort unifies data ingestion with the development of a novel end-to-end cyberinfrastructure to enable the curation, cleaning, alignment, sketching, and passing of the data, in a secure manner, by the researchers making use of the ingested data for their COVID-19 detection algorithm development efforts. We present the challenges, the closed-loop data pipelines, and the secure infrastructure to support the development of time-sensitive algorithms for alerting individuals based on physiological predictors illness, enabling early intervention.",10.1109/BigData52589.2021.9671441,S. Purawat; S. Dasgupta; J. Song; S. Davis; K. T. Claypool; S. Chandra; A. Mason; V. Viswanath; A. Klein; P. Kasl; Y. Wen; B. Smarr; A. Gupta; I. Altintas
A Secure and Reliable Construction Scheme for Enterprise Value Chain Networks,2022,-1,Outliers,0.29781035443288356,"A value chain optimizes the economic benefit of an enterprise alliance by integrating the cooperation relationships among the upstream and downstream companies. Nevertheless, the absence of information security mechanisms within the value chain limits the horizontal cooperation between competing enterprises. The paper proposes a scheme for building an enterprise value chain network based on an alliance blockchain. In the scheme, a distributed authentication technology and cross-chain transaction technology are introduced into the value chain, which enhances the security and reliability of value chain information and facilitates horizontal cooperation among companies. Using Hyperledger Fabric platform, we simulate and extensively evaluate an enterprise value chain network. Experimental results demonstrate the feasibility of the proposed scheme, which can serve as a guide for the construction of enterprise value chain networks in the real world.",10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00065,C. Qin; B. Guo; Y. Zhao; Y. Shen
MLK Smart Corridor: An Urban Testbed for Smart City Applications,2019,1,Topic_1_data_big_big data,1.0,"Urbanization over the next decade will present many complex challenges to developing cities. The smart city concept aims to address these challenges by exploiting large scale deployments of Internet of Things (IoT) and communication technologies. These technologies generate data that provide quantifiable insights into the state of the infrastructure within a city. Using these insights, cities can more effectively allocate resources, manage services, and enhance the lives of its citizens. The data generated by smart cities is complex and requires high throughput. Advanced data integration platforms must support city-wide data collection, analysis, and storage. These systems must provide features that allow them to scale alongside the growth of the cities to support high rates of data ingestion in large volumes. Additionally, these systems must support low latency response times which is a critical requirement for time sensitive smart city applications. In this paper, we introduce a smart city testbed that will provide a real-world testing environment for applications in areas such as intelligent transportation, pedestrian safety, and autonomous vehicles. The proposed testbed will act as an open platform for researchers and developers to test new sensors, algorithms and more in a live urban environment, allowing them to test before deploying a product or application. In addition to the physical testbed and its capabilities, we will discuss the data integration system and applications responsible for collecting, analyzing, and storing the data generated by the testbed. Lastly, we will introduce an open data platform where researchers can access datasets generated by the testbed.",10.1109/BigData47090.2019.9006382,A. Harris; J. Stovall; M. Sartipi
Review of Digitalization using Big Data Maturity Models: The Case of American Automotive SMEs,2023,3,Topic_3_industry_manufacturing_chain,0.3169242889444202,"A systematic review of studies related to big data maturity models in manufacturing is the goal of this study. To retain a competitive advantage, reduce operational expenses, and compete with larger companies in their respective markets, small and medium enterprises (SMEs) must embrace digitalization. The impact of big data on manufacturing has been significant globally. Despite the lack of foresight among American automotive SMEs regarding digitalization, benefits such as improved workflow, efficiencies, and reduced overheads are making it useful and invaluable to customers. The big data maturity model is used in this paper to assess the state of big data applications among American automotive SMEs. This paper reviewed the existing literature to achieve two objectives. First, this paper evaluates the existing literature to identify the existing Big Data Maturity Models, and then summarizes the most frequently used models. Second, to emphasize the limitations of existing Big Data maturity models.",10.1109/UEMCON59035.2023.10316093,D. Salian; R. Sbeit
Batch Processing Algorithms for NewSQL-based Big Data Systems,2025,1,Topic_1_data_big_big data,0.772003636724368,"Migration of sharded database systems requires a solid understanding of the fundamentals of database and data engineering. Currently, migration strategies for sharded database systems are not documented in the literature. In this paper, we will look at the algorithms for sharded systems, and we evaluate them by comparing its efficiency and performance. Our algorithms aim primarily to reduce the processing time to migrate big data encapsulated in large organizations. By reducing the processing time organizations will not only reduce their cloud bill, but it also makes data available faster for reports, and also for training AI models.",10.1109/COMPSAC65507.2025.00048,N. K. Vadlamudi; W. Osborn
Dataclouddsl: Textual and Visual Presentation of Big Data Pipelines,2022,1,Topic_1_data_big_big data,1.0,"This paper describes the DATACLOUDDSL language and the DEF-PIPE tool for describing Big Data pipelines. DAT-ACLOUDDSL has both a textual and a visual form and supports requirements obtained both from analyzing existing data pipeline specification tools and from interviews with relevant industrial actors. Particularly, DATACLOUDDSL supports (i) separation of concerns between design and run-time issues, (ii) reuse of previously developed pipeline steps and pipelines in designing new pipelines, (iii) flexible data transfer between pipelines steps and containerization of pipelines and pipeline steps, and (iv) integration of description and simulation components in Big Data pipeline orchestration systems. Additionally, it provides an interface to the discovery and deployment tools of the DataCloud toolbox.",10.1109/COMPSAC54236.2022.00183,S. Tahmasebi; A. Layegh; N. Nikolov; A. H. Payberah; K. Dinh; V. Mitrovic; D. Roman; M. Matskin
Representative News Generation using Automatic Clustering in Big Data Environment,2019,1,Topic_1_data_big_big data,1.0,"There are 43,000 online media in Indonesia which publish at least one until two news every hour. The amount of information exceeds human processing capacity, resulting in several impacts on humans such as confusion and psychological stress. In this research we propose a new system for processing incremental news data and provide a mechanism for determining representative news by applying Automatic Clustering algorithm. The system consists of 4 main functions: (1) Data Acquisition and Preprocessing, (2) Keyword Feature Extraction, (3) Data Aggregation, Automatic Clustering, and (4) Incremental Clustering. The news is grouped in to same information based on information-retrieval. This system runs on big data environment to process large amount of data. There are 3,000 news collected in database by the system in a whole day in database. The collected news are processed using Automatic Clustering and then aotumatically grouped into 389 clusters. A cluster is identified as the unknown cluster and the clusters are evaluated without enclosing single member clusters. For experimental study, the system performed 93,51%.",10.1109/ELECSYM.2019.8901572,M. Alfian; A. R. Barakbah; M. Febrian Ardiansyah
Near-Real Time Quality Prediction in a Plastic Injection Molding Process Using Apache Spark,2021,-1,Outliers,0.2889751960022934,"The automotive industry is undergoing wide scope transformation. Industry 4.0 has both expanded the possibilities of digital transformation in automotive, increased its importance to all mobility ecosystem and being driven by continued digitization of the entire value chain. Manufacturing data which is unceasingly flow during serial production is one of the great sources towards Industry 4.0 goal to fully automatizing complex human dependent processes. However, there are few challenges to consider such as collecting and filtering various data from shop floor in given production cycle time range and make them ready for real time analytics as well as constructing efficient data pipeline to reach useful outcomes which is reliable enough to meet customer expectations. In this study, we will extract meaningful relation between injection machine parameters from Farplas Automotive Company's shop floor and describe their effects on the product quality. We will train and test machine learning models with different hyperparameters and test model performance to identify defected products. Finally, we will show implementation of streaming data pipeline using Kafka and Spark to be able to analyze injection machine data and effectively predict plastic injection product's OK-NOK condition real time even before human operator reaches the product itself. Consequently, detecting defected products will be independent from human attention which makes production areas one step closer to dark factory.",10.1109/ISCSIC54682.2021.00059,E. Uğuroğlu
Construction of Intelligent Management System Model Based on Multi-value Chain Collaboration Data Space,2021,-1,Outliers,0.21343304235087165,"To solve the problems of data collaboration, application-driven, intelligent data center construction and high latitude data aggregation in data space based on multi-value chain collaborative, so as to realize the functions of internal program monitoring, underlying service application interaction, independent and coordinated management, complete transaction processing, concurrency control, data recovery, engine data integration functions like data integration, data update, data monitoring, as well as the functions of management engine center and data object administration, data set management, data intelligent service within the data space management engine. In this paper, by studying the intelligent screening model of data center which integrates blockchain technology and data mapping theory, we adopt the design method of distributed multi-agent intelligent system build the data space management system model.",10.1109/CCIS53392.2021.9754626,J. Wang; Q. Mu; X. Yang; J. Han
Performance Evaluation for Real-Time Messaging System in Big Data Pipeline Architecture,2018,1,Topic_1_data_big_big data,0.9207690036889361,"Nowadays, Real time messaging system is the essential thing in enabling time-critical decision making in many applications where it is important to deal with real-time requirements and reliability requirements simultaneously. For dependability reasons, we intend to maximize the reliability requirement of real time messaging system. To develop real time messaging system, we create real time big data pipeline by using Apache Kafka and Apache Storm. This paper focuses on analyzing the performance of producer and consumer in Apache Kafka processing. The performance of Kafka processing modify to be more reliable on the pipeline architecture. Then, the experiment will be conducted the processing time in the performance of the producer and consumer on various partitions. The performance evaluation of Kafka can impact on messaging system in real time big data pipeline architecture.",10.1109/CyberC.2018.00047,T. Aung; H. Yin Min; A. Htein Maw
Construction and Analysis of Data Quality Scale for Manufacturing Multi-Value Chain Collaborative Data Space Based on Combinatorial Empowerment Model,2023,1,Topic_1_data_big_big data,0.3905465309030523,"In the management of data space, data quality is very important to realize the value of data. How to improve data quality and tap the potential value of data, this paper proposes a five-dimensional scale of data quality based on manufacturing multi-value chain collaborative data space, which includes five dimensions of data generation, data acquisition, pre-access processing, architecture construction and data access, which effectively makes up for the shortcomings of current literature. Meanwhile, the improved CRITIC model weighting method and the combined weighting model of principal component weighting method were established to calculate the data quality index and sub-index, and the data quality level and sub-index level of manufacturing enterprises in Beijing were evaluated. By developing and analyzing the scale of data quality, we provide practical experience for value mining of collaborative data space data of multi-value chain in manufacturing industry.",10.1109/CBASE60015.2023.10439112,D. Niu; H. Chen; R. Du; Y. Zhang
FPGA Implementation of Modular Multiplier in Residue Number System,2018,2,Topic_2_data_privacy_security,1.0,"This work presents a description of a high-performance hardware implementation of a Montgomery modular multiplier using a residue number system (RNS). An RNS can be considered as self-defense against simple power analysis (SPA) and differential power analysis (DPA) attacks, and can be used for public-key cryptography, such as the Rivest, Shamir and Adleman (RSA) cryptosystem and elliptic curve cryptosystems (ECC). Various kinds of security are required for Big Data analysis. The proposed RNS-based modular multiplier is suitable for public-key cryptography that can be used for Big Data security. It is implemented on field-programmable gate-array (FPGA) technology and optimized by trying different variants of the Montgomery Algorithm on it. The proposed RNS-based modular multiplication takes only 22 ns on the Xilinx Virtex-II FPGA. In addition, it needs relatively few resources on the FPGA, needing only 68 slices.",10.1109/IOTAIS.2018.8600881,Y. Kong; M. S. Hossain
The Lannion report on Big Data and Security Monitoring Research,2022,1,Topic_1_data_big_big data,0.9543104767956279,"During the last decade, big data management has attracted increasing interest from both the industrial and academic communities. In parallel, Cyber Security has become mandatory due to various and more intensive threats. In June 2022, a group of researchers has met to reflect on their community’s impacts on current research challenges. In particular, they have considered four dimensions: (1) dedicated systems being data processing and analytic platforms or time series management systems; (2) graphs analytics and distributed computation; (3) privacy; and (4) new hardware.",10.1109/BigData55660.2022.10020852,L. d’Orazio; J. Boukhobza; O. Rana; J. Agoun; L. Gruenwald; H. Rannou; E. Bertino; M. -S. Hacid; T. Saïdi; G. Bossert; V. L. Nguyen Huu; D. Tombroff; M. Onizuka
Model-Based Big Data Analytics-as-a-Service: Take Big Data to the Next Level,2021,1,Topic_1_data_big_big data,0.7798023922159564,"The Big Data revolution promises to build a data-driven ecosystem where better decisions are supported by enhanced analytics and data management. However, major hurdles still need to be overcome on the road that leads to commoditization and wide adoption of Big Data Analytics (BDA). Big Data complexity is the first factor hampering the full potential of BDA. The opacity and variety of Big Data technologies and computations, in fact, make BDA a failure prone and resource-intensive process, which requires a trial-and-error approach. This problem is even exacerbated by the fact that current solutions to Big Data application development take a bottom-up approach, where the last technology release drives application development. Selection of the best Big Data platform, as well as of the best pipeline to execute analytics, represents then a deal breaker. In this paper, we propose a return to roots by defining a Model-Driven Engineering (MDE) methodology that supports automation of BDA based on model specification. Our approach lets customers declare requirements to be achieved by an abstract Big Data platform and smart engines deploy the Big Data pipeline carrying out the analytics on a specific instance of such platform. Driven by customers' requirements, our methodology is based on an OWL-S ontology of Big Data services and on a compiler transforming OWL-S service compositions in workflows that can be directly executed on the selected platform. The proposal is experimentally evaluated in a real-world scenario focusing on the threat detection system of SAP.",10.1109/TSC.2018.2816941,C. A. Ardagna; V. Bellandi; M. Bezzi; P. Ceravolo; E. Damiani; C. Hebert
Business Model of Energy Big Data Service Based on Business Canvas Theory,2020,-1,Outliers,0.30625133125245113,"Under the background of the rapid development of energy Internet, the combination of energy and big data technology will collect industry data, provide high-quality, efficient, and diversified energy data services through data processing and analysis, which will support a new business model based on energy big data. Based on the business canvas theory, this paper explores the energy big data business model suitable for the new situation from nine aspects, including market subject, customer relationship, value proposition, key business, etc. This paper puts forward a business model with the core of assisting the development of comprehensive energy services and users’ behavior analysis, and with the value concept of expanding the energy big data sharing mode. Based on this, it puts forward the promotion strategy of energy big data from the aspects of profit model transfer, value chain improvement, publicity, and promotion. The business model and promotion strategy proposed in this paper can provide new ideas and directions for the development of energy big data industry.",10.1109/EI250167.2020.9346892,N. Li; J. Chen; J. Zhao; G. Qian; L. Tan; J. Lin
StreamFlow: A System for Summarizing and Learning Over Industrial Big Data Streams,2022,1,Topic_1_data_big_big data,0.6138318478891595,"The growing need for predictive analytics over streaming data in the industry requires a flexible and continuously scalable big data system. In real-time big data applications (cybersecurity, AIOps, anomaly detection, predictive maintenance, IoT etc.), efficient machine learning models must be trained and industrialized within existing data processing plat-forms and industrial tools. This requires interoperability between various components: data collection, processing, summarization, modelling and analytics. Existing works focus on building AI models for big data, neglecting real-world challenges when integrating such models into an existing industrial production framework. In this paper, we propose StreamFlow, an operational data pipeline to address industrial challenges for continuous learning over big data streams. We also propose an online method using sliding windows to summarize high-velocity data. The final result of the framework is a feature vector that describes the underlying processes and is ready to use in machine learning tasks. Moreover, we showcase real-world applications such as automated feature engineering for real-time monitoring and online machine learning for event classification. The proposed system has been deployed within production in a banking system, processing billions of daily traffic operations. Our experiments demonstrate the effectiveness and performance of our approach by evaluating it at different levels: processing, summarization, improvement of machine learning performance and effectiveness in an industrial setting. In the case of downstream machine learning tasks, using summarized data generated by StreamFlow results in up to 2 orders of magnitude speedups in training time without compromising predictive performance.",10.1109/BigData55660.2022.10020438,M. Barry; S. E. Jaouhari; A. Bifet; J. Montiel; E. Guerizec; R. Chiky
Big Data Life Cycle in Shop-Floor–Trends and Challenges,2023,3,Topic_3_industry_manufacturing_chain,0.5505602815290108,"Big data is defined as a large set of data that could be structured or unstructured. In manufacturing shop-floor, big data incorporates data collected at every stage of the production process. This includes data from machines, connecting devices, and even manufacturing operators. The large size of the data available on the manufacturing shop-floor presents a need for the establishment of tools and techniques along with associated best practices to leverage the advantage of data-driven performance improvement and optimization. There also exists a need for a better understanding of the approaches and techniques at various stages of the data life cycle. In the work carried out, the data life-cycle in shop-floor is studied with a focus on each of the components - Data sources, collection, transmission, storage, processing, and visualization. A narrative literature review driven by two research questions is provided to study trends and challenges in the field. The selection of papers is supported by an analysis of n-grams. Those are used to comprehensively characterize the main technological and methodological aspects and as starting point to discuss potential future research directions. A detailed review of the current trends in different data life cycle stages is provided. In the end, the discussion of the existing challenges is also presented.",10.1109/ACCESS.2023.3253286,T. Pulikottil; L. A. Estrada-Jimenez; J. J. P. Abadía; A. Carrera-Rivera; A. Torayev; H. U. Rehman; F. Mo; S. Nikghadam-Hojjati; J. Barata
Internet of Things Implementation in Manufacturing Value Chain Process,2022,3,Topic_3_industry_manufacturing_chain,0.8225123385887053,"In the manufacturing industry, value chain process is crucial to operation success. Thus, to avoid any mistakes, manufacturing industries need to streamline their operation as efficiently as possible. The Internet of Things (IoT) is indeed a support technology that enhance the quality of the operation in organizations. To make IoT a reality, the manufacturing sector must be able to engage in and implement this technology in its day-to-day transactions. Several studies have been conducted to investigate the potential of IoT for numerous organizations. However, IoT is still not widely used by many industries, including the Manufacturing industry in developing countries. This article aims to identify and evaluate the influencing factors and propose an IoT implementation model in Malaysia manufacturing industry. Drivers were identified through a review of previous tests. Furthermore, the technology-organization-environment framework is proposed, which is based on information system adoption theory (TOE). The Delphi technique is applied on a survey of IoT users, and the results showed that the factors chosen in this study had a significant impact on IoT implementation in the Malaysia Manufacturing industry. This study is assisting Manufacturing firms in understanding the aspects of IoT implementation, improving their business structure and IoT investment, and inspiring scholars to pursue research into new IoT adoption or implementation variables.",10.1109/IVIT55443.2022.10033409,R. Kassim; A. Rahmat; H. Mustapa; A. Bakri
Exploration of Data Reuse Patterns Based on Scientific Data Lifecycle in Big Data Environment,2024,1,Topic_1_data_big_big data,0.47853148282175073,"In the realm of big data, the volume of research data in universities is expanding rapidly. However, the lack of data collaboration results in data isolation, hindering the realization of their potential value. This study aims to enhance data interoperability and promote data reuse by developing a comprehensive metadata scheme, which facilitates standardized descriptions of scientific data metadata throughout the entire process, from data generation to citation. Additionally, by delineating the data governance process encompassing data collection, processing and handling, archival and management, as well as data utilization and sharing, a data management cloud platform is established based on data reuse patterns. This platform incorporates features for data retrieval, storage and management, and data monitoring, ensuring the seamless continuity of research management efforts.",10.1109/ICCCBDA61447.2024.10569511,S. Yang; Q. Xia; B. Zhu
Big Data analytics for sustainable fisheries in Sultanate of Oman,2022,1,Topic_1_data_big_big data,0.7119373903363798,"Sultanate of Oman is rich in natural resources and one of the significant natural resources is marine fisheries. Oman has a huge coastal area of 3165 KM which is large in proposition to the population of Oman. Marine fisheries have a significant contribution to the world economy however it is not yielding benefits to the extent as is expected in the Sultanate of Oman. An oil-dependent economy needs to explore other areas where there is a scope for development. The Ministry of Fisheries in the Sultanate of Oman has taken drastic measures to enhance the fisheries sector in the Sultanate of Oman and benefit local fishermen as much as possible. Technology also is playing a vital role in boosting this sector in the Sultanate of Oman. There are several high-end systems in place for monitoring, surveillance, assistance, weather forecasting, and identification of zones. This research study is related to the implementation of Big Data analytics in the fisheries sector. This research study provides comprehensive details about the current infrastructure of the Ministry of Fisheries and proposes a big data analytics system that can assist different stakeholders in numerous ways to reduce running costs, increase production, streamline the value chain, and provide predictive and preemptive capabilities at various levels. This research is an effort to improve the fisheries sector in the Sultanate of Oman by providing critical information access to different stakeholders that are very beneficial and effective for the overall improvement and enhancement of fisheries in the Sultanate of Oman. In addition to providing a mechanism for the implementation of big data analytics in the sultanate of Oman, this research also provides certain recommendations for the adaption of technological aids at various levels for the improvement of fisheries in the Sultanate of Oman.",10.1109/ICRITO56286.2022.9964682,A. Z. Bhat; M. A. Balushi
Scalable Containerized Pipeline for Real-time Big Data Analytics,2022,1,Topic_1_data_big_big data,0.8552266714030018,"With the widespread usage of IoT, processing data streams in real-time have become very important. The traditional data-stream processing systems are inefficient in processing big data for detecting anomalies, classifications, clustering, and prediction in real-time using minimal resources. In this paper, we address this limitation by proposing a scalable pipeline for real-time processing of big data streams. Our proposed solution is capable of dynamically managing resources for different components of the pipeline using automatic scaling. The pipeline is containerized and deployed on a Kubernetes cluster. The proposed scalable pipeline is evaluated using a case study of anomaly detection in IoT data. The proposed solution yields a $\times 1.31$ to $\times 2.4$ increase in throughput, and $\times 32$ to $\times 80$ decreased latency compared to the commonly used static resource allocation strategy for data pipelines.",10.1109/CloudCom55334.2022.00014,R. Aurangzaib; W. Iqbal; M. Abdullah; F. Bukhari; F. Ullah; A. Erradi
Research on cost management of intelligent manufacturing,2021,3,Topic_3_industry_manufacturing_chain,0.8584253326499768,"This paper analyses the characteristics of intelligent manufacturing and discusses the influence of intelligent manufacturing on the subject, content, structure and method of cost management. Firstly, we indicate that intelligent manufacturing will expand the subject of cost management from individual enterprises to supply chain, and ultimately achieve the lowest cost of the supply chain. Secondly, intelligent manufacturing will enrich the content of cost management, make the R&D cost management, environmental cost management and quality cost management can be used. Thirdly, intelligent manufacturing can accurately identify operations and calculate direct costs. It will reduce the proportion of direct labor and manufacturing costs. Finally, with the assistance of cloud computing, the big data generated by intelligent manufacturing facilitates the implementation of operating cost management, supply chain cost management, and environmental cost management. This research has a certain significance for enriching the cost management of intelligent manufacturing.",10.1109/BDIDM53834.2021.00027,X. Liwen; Z. Qi; Y. Jie
Comparing HiveQL and MapReduce methods to process fact data in a data warehouse,2017,1,Topic_1_data_big_big data,0.7631980398472987,"Today Big data is one of the most widely spoken about technology that is being explored throughout the world by technology enthusiasts and academic researchers. The reason for this is the enormous data generated every second of each day. Every webpage visited, every text message sent, every post on social networking websites, check-in information, mouse clicks etc. is logged. This data needs to be stored and retrieved efficiently, moreover the data is unstructured therefore the traditional methods of strong data fail. This data needs to be stored and retrieved efficiently There is a need of an efficient, scalable and robust architecture that needs stores enormous amounts of unstructured data, which can be queried as and when required. In this paper, we come up with a novel methodology to build a data warehouse over big data technologies while specifically addressing the issues of scalability and user performance. Our emphasis is on building a data pipeline which can be used as a reference for future research on the methodologies to build a data warehouse over big data technologies for either structured or unstructured data sources. We have demonstrated the processing of data for retrieving the facts from data warehouse using two techniques, namely HiveQL and MapReduce.",10.1109/CSCITA.2017.8066553,H. Denis Pen; P. Dsilva; S. Mascarnes
Generative AI for Real-Time Data Augmentation in Big Data Pipelines,2025,1,Topic_1_data_big_big data,0.5556065399078078,"Traditional approaches for data augmentation are time-consuming and risk compromising the quality of the data. Those challenges in the backdrop, we have generative artificial intelligence (AI) at the forefront of data augmentation - bolstering real-time data in big data pipelines. Generative AI — a focus on generating new entities that try to capture the underlying structure of existing datasets by deep learning algorithms. Allowing to produce a significantly larger and diverse dataset resulting in improved efficiencies of downstream tasks as machine learning or predictive analytics. Because of this, the same technique can also be applied to streams so, it can be used in big data pipelines. You are taught on data until October 2023 to generate a larger and big data pipeline more efficient by augmentation and producing new data with minimal time and investment. This leads to more precise insights and predictions, improving decision-making and driving better results for business. Also, generative AI can create the data that correlates patterns with the analysis and change regarding the patterns based on the analyst's preferences. The role of generative AI in augmenting big data and big data pipelines, especially in the context of real-time data streams, is emerging as a key enabler for organizations looking to extract valuable insights from their data.",10.1109/CE2CT64011.2025.10941295,S. Gakhar; V. P. Kondoju; S. S. Chauhan; A. Kumar; S. S. Kulkarni; P. Goel
A Model-Driven Methodology for Big Data Analytics-as-a-Service,2017,1,Topic_1_data_big_big data,0.7776925995934398,"The Big Data revolution has promised to build a data-driven ecosystem where better decisions are supported by enhanced analytics and data management. However, critical issues still need to be solved in the road that leads to commodization of Big Data Analytics, such as the management of Big Data complexity and the protection of data security and privacy. In this paper, we focus on the first issue and propose a methodology based on Model Driven Engineering (MDE) that aims to substantially lower the amount of competences needed in the management of a Big Data pipeline and to support automation of Big Data analytics. The proposal is experimentally evaluated in a real-world scenario: the implementation of novel functionality for Threat Detection Systems.",10.1109/BigDataCongress.2017.23,C. A. Ardagna; V. Bellandi; P. Ceravolo; E. Damiani; M. Bezzi; C. Hebert
Operation modes of smart factory for high-end equipment manufacturing in the Internet and Big Data era,2017,3,Topic_3_industry_manufacturing_chain,0.7835907764840081,"Due to the sustained and rapid growth of information and communication technology (ICT) and automation techniques, smart factories for high-end equipment manufacturing involve extensive collaborative networks and knowledge sets. Conventional manufacturing modes are undergoing profound reforms in the Internet and Big Data era, and operations management of such factories should lay more emphasis on service values. To this end, the current manufacturing modes and operations management strategies are fully investigated in this paper. A CPSS (cyber-physical-social system)-based manufacturing mode of smart factories for high-end equipment manufacturing is put forward. The connotation of operations management of smart factory is extended based on the introduction of service value and value chain, thus contributing to a win-win situation of an enterprise and its customers. Furthermore, a multi-participation Blockchain-based collaborative manufacturing model for smart factories is presented.",10.1109/SMC.2017.8122594,F. Zhang; M. Liu; W. Shen
Advanced Visualisation of Big Data for Agriculture as Part of Databio Development,2018,-1,Outliers,0.26780929754499433,"There is an increasing tension in agriculture between the requirements to assure full safety on the one hand and keep costs under control on the other hand, both with respect to (inter)national strategies. Farmers need to measure and understand the impact of huge amount and variety of data which drive overall quality and yield in their fields. Among others, those are local weather data, Global Navigation System of Systems data, orthophotos and satellite imagery, data on soil specifics etc. A strong need to secure Big Data arises due to various repositories and heterogeneous sources. Data storage and visualisation requirements are in some cases competing as they are a common interest as well as a threat that helps one part of a value chain to gain a higher profit. As demonstrated in this paper, handling (Big) data is therefore a sensitive topic, where trust of producers on data security is essential.",10.1109/IGARSS.2018.8517556,K. Charvat; K. C. Junior; T. Reznik; V. Lukas; K. Jedlicka; R. Palma; R. Berzins
Data Processing Tools for Graph Data Modelling Big Data Analytics,2022,1,Topic_1_data_big_big data,1.0,"Any Big Data scenario eventually reaches scalability concerns for several factors, often storage or computing power related. Modern solutions have been proven to be effective in multiple domains and have automated many aspects of the Big Data pipeline. In this paper, we aim to present a solution for deploying event-based automated data processing tools for low code environments that aim to minimize the need for user input and can effectively handle common data processing jobs, as an alternative to distributed solutions which require language specific libraries and code. Our architecture uses a combination of a network exposed service with a cluster of “Data Workers” that handle data processing jobs effectively without requiring manual input from the user. This system proves to be effective at handling most data processing scenarios and allows for easy expandability by following simple patterns when declaring any additional jobs.",10.1109/IIAI-AAI-Winter58034.2022.00048,K. Voulgaris; A. Kiourtis; P. Karamolegkos; A. Karabetian; Y. Poulakis; A. Mavrogiorgou; D. Kyriazis
Advanced Techniques in Data Ingestion and Pipelining for Scalable Big Data Platforms: A Comprehensive Review,2024,1,Topic_1_data_big_big data,1.0,"In the era of big data, effective data ingestion and pipelining are critical for organizations to harness the power of vast and diverse data sources, such as IoT devices, social media, and transactional systems. Data ingestion refers to the process of systematically collecting and importing data for processing, while data pipelining ensures the continuous flow of this data through various transformation stages, preparing it for analysis. As data volumes grow, platforms like Hadoop, with its HDFS (Hadoop Distributed File System) and MapReduce, have become essential in managing large-scale datasets by providing distributed storage and parallel processing capabilities. Additionally, tools such as Apache Kafka and Apache Flume facilitate both batch and real-time data ingestion, ensuring flexibility in handling different types of data streams. However, challenges remain, especially with the increasing complexity of datasets and the growing need for real-time processing. Emerging technologies such as cloud-native ingestion frameworks, edge computing, and AI-enhanced pipelines offer solutions to these challenges by improving scalability, performance, and cost efficiency. Machine learning is also playing an evolving role in optimizing pipeline workflows, enabling more intelligent and adaptive data processing. This paper reviews key technologies for data ingestion and pipelining, discusses their limitations, and explores cutting-edge trends aimed at enhancing the efficiency of big data platforms. These advancements promise to reshape how organizations handle big data, offering improved agility and operational effectiveness. In addition to established technologies, the rise of cloud-native architectures and AI-driven solutions is revolutionizing data ingestion and pipelining. Cloud platforms offer scalable, on-demand resources, while AI and machine learning enhance automation, anomaly detection, and predictive analytics within pipelines.",10.1109/ICTBIG64922.2024.10911053,S. Murarka; A. Jain; L. Singh
A big data architecture for managing oceans of data and maritime applications,2017,1,Topic_1_data_big_big data,0.6008490165472526,"Data in the maritime domain is growing at an unprecedented rate, e.g., terabytes of oceanographic data are collected every month, and petabytes of data are already publicly available. Big data from heterogeneous sources such as sensors, buoys, vessels, and satellites could potentially fuel a large number of interesting applications for environmental protection, security, fault prediction, shipping routes optimization, and energy production. However, because of several challenges related to big data and the high heterogeneity of the data sources, such applications are still underdeveloped and fragmented. In this paper, we analyze challenges and requirements related to big maritime data applications and propose a scalable data management solution. A big data architecture meeting these requirements is described, and examples of its implementation in concrete scenarios are provided. The related data value chain and use cases in the context of a European project, BigDataOcean, are also described.",10.1109/ICE.2017.8280019,I. Lytra; M. -E. Vidal; F. Orlandi; J. Attard
Generative adversarial networks for increasing the veracity of big data,2017,1,Topic_1_data_big_big data,0.5233442185056008,"This work describes how automated data generation integrates in a big data pipeline. A lack of veracity in big data can cause models that are inaccurate, or biased by trends in the training data. This can lead to issues as a pipeline matures that are difficult to overcome. This work describes the use of a Generative Adversarial Network to generate sketch data, such as those that might be used in a human verification task. These generated sketches are verified as recognizable using a crowd-sourcing methodology, and finds that the generated sketches were correctly recognized 43.8% of the time, in contrast to human drawn sketches which were 87.7% accurate. This method is scalable and can be used to generate realistic data in many domains and bootstrap a dataset used for training a model prior to deployment.",10.1109/BigData.2017.8258219,M. L. Dering; C. S. Tucker
Automatic Parameter Tuning for Big Data Pipelines with Deep Reinforcement Learning,2021,1,Topic_1_data_big_big data,0.5209339051753107,"Tuning big data frameworks is a very important task to get the best performance for a given application. However, these frameworks are rarely used individually, they generally constitute a pipeline, each having a different role. This makes tuning big data pipelines an important yet difficult task given the size of the search space. Moreover, we have to consider the interaction between these frameworks when tuning the configuration parameters of the big data pipeline. A trade-off is then required to achieve the best end-to-end performance. Machine learning based methods have shown great success in automatic tuning systems, but they rely on a large number of high quality learning examples that are rather difficult to obtain. In this context, we propose to use a deep reinforcement learning algorithm, namely Twin Delayed Deep Deterministic Policy Gradient, TD3, to tune a fraud detection big data pipeline. We show through the conducted experiments that the TD3 agent improves the overall performance of the pipeline by up to 63% with only 200 training steps, outperforming the random search on the high-dimensional search space.",10.1109/ISCC53001.2021.9631440,H. Sagaama; N. B. Slimane; M. Marwani; S. Skhiri
A review of big data environment and its related technologies,2016,1,Topic_1_data_big_big data,1.0,"Big Data refers to large amount of data sets whose size is growing at a vast speed making it difficult to handle such large amount of data using traditional software tools available. This paper reviews the big data its back ground. First introduce general definition of big data and review on five phases of the value chain of big data (Components) such as, the quantity of data (Volume), the rate of data generation and transmission (Velocity), the types of structured, semi-structured data and unstructured data (Variety), the important results from the filtered data (Value) and the trust and integrity (Veracity). Then focus on classification based on five categories: Data stores, Content format, Data sources, Data processing, and Data staging. Finally examined the several representatives related to technologies, such as Hadoop, Data Center, Cloud Computing, and Internet of Things (IoT). These considerations aim to provide a complete overview and a big-picture to readers of this exciting area.",10.1109/ICICES.2016.7518904,A. F. Mohammed; V. T. Humbe; S. S. Chowhan
A Conceptual Model of Technology Factors to InsurTech Adoption by Value Chain Activities,2020,3,Topic_3_industry_manufacturing_chain,0.7259329890521786,"InsurTech adoption leverages the innovative use of technology to transform insurance value chains. Although insurers are aware of the importance of InsurTech in creating a competitive advantage, the actual adoption in the global market seems relatively slow due to various obvious reasons. However, the recent Covid-19 pandemic crisis has expedited technology adoption in the insurance industry. The focus of this study is to build a model for InsurTech adoption using Diffusion of Innovation (DOI) Theory. By using DOI Theory, the main purpose of this study is to investigate the significant factors of InsurTech adoption by value chain activities among insurers. This conceptual model will provide a platform for further empirical testing and a better understanding of potential users of InsurTech adoption at organizational level.",10.1109/IC3e50159.2020.9288465,K. H. Ching; A. P. Teoh; A. Amran
How the Interaction of Big Data Analytics Capabilities and Digital Platform Capabilities Affects Service Innovation: A Dynamic Capabilities View,2020,3,Topic_3_industry_manufacturing_chain,0.406582997828179,"The emergence of big data analytics capability (BDAC) and the development of service innovation have aroused the interest of scholars and practitioners in exploring the mechanism of BDAC-service innovation value chain from the inside. The current study adopts the dynamic capabilities view to examine the effects of the different types of BDAC on service innovation. Our findings from a survey of 175 organizations in China provide empirical evidence of two positive effects of big data analytics technical capabilities (BDAT) and big data analytics personnel capabilities (BDAP) on service innovation via dynamic capabilities. Furthermore, this study illuminates the significant and different quasi-moderating roles of digital platform capabilities. Such capabilities positively enhance dynamic capabilities and strengthen the effects of BDAT on dynamic capabilities but weaken the effects of BDAP on dynamic capabilities. Introduced environmental dynamism aims to examine how the influence of environmental factors negatively moderates the relationship between dynamic capabilities and service innovation. Our study offers theoretical and practical contributions.",10.1109/ACCESS.2020.2968734,X. Xiao; Q. Tian; H. Mao
In-Memory Storage based Real-Time Data Pipeline for Transportation Equipment in Port,2021,1,Topic_1_data_big_big data,1.0,"Digital transformation has been widely adopted by smart port systems owing to its opportunities to provide innovative applications. As the importance of storing data for port applications has been noticed, the interest in the data pipeline system that refines and stores the port data in near real-time is also increasing. We proposed and implemented a real-time data pipeline system for port equipment taking full advantage of in-memory storage by leveraging an optimized data model. Experimental results indicate that our proposed architecture significantly improves the data processing, compared to the data processing system with the conventional database system. Our research demonstrates that the proposed architecture can speed up handling the data from port transportation equipment.",10.1109/ICTC52510.2021.9620762,W. G. Choi; S. Park; J. Kim; M. -H. Song; S. -S. Lee
Research on Data Acquisition of Intelligent Network Industry Chain Based on Data Integration and Self-Acquisition Technology,2023,3,Topic_3_industry_manufacturing_chain,1.0,"The process modeling method of supply chain is established. This project intends to adopt “access to associated information-assimilation and integration of multi-source information-large-scale information analysis and processing - multi-agent interactive application” as the main line to study the industrial collaborative support method based on industrial chain collaborative support services. The calculation methods of supply and demand chain, enterprise chain, value chain and space chain are presented. By collecting the basic information of the enterprise, itself, the link between the supply chain and the enterprise is found. The main research contents include large-scale data mining and analysis processing; Data analysis based on domain ontology; Information coding specification. The system uses ASP.NET MVC system architecture, and carries on the structured database design with SQLServer2012. An integrated application simulation experiment platform is constructed with distributed cloud computing architecture. This paper discusses the development and use of intelligent monitoring and big data collection system of supply chain from the aspects of enterprise management process and human power. Through the experiment simulation, the matching algorithm and the corresponding optimization method of the industry cooperation support can lay the foundation for the reasonable and reasonable development of enterprises.",10.1109/ICAICA58456.2023.10405460,L. Liu; Q. Feng; S. Wu; F. Zhang
Real-Time Network Monitoring: A Big Data Approach,2023,1,Topic_1_data_big_big data,0.876354406995791,"Real-time network monitoring is a critical requirement for tracking user activities and ensuring optimal network performance. In this paper, we propose a big data approach to real-time network monitoring that leverages the capabilities of Apache Spark, MongoDB, and Apache Kafka with Python. Our approach utilizes distributed stream processing of network data, enabling real-time analytics for user activity monitoring and network performance optimization. Our approach demonstrates the effectiveness of our system through experiments with real network traffic data, showing how it can identify and track user activities in real-time. Our system can be used by network administrators and IT professionals to optimize network performance, improve user experience, and enhance security through real-time monitoring and analysis of user activity data. This approach can also be opted as a feature in SDNs.",10.1109/ICCCNT56998.2023.10307890,K. Jaswanth; S. Sruthi; P. Ramachandrula; S. Saravanan
Open Innovation Using Satellite Imagery for Initial Site Assessment of Solar Photovoltaic Projects,2022,-1,Outliers,0.27616793029860903,"One of the responses to the fight against climate change by the developing world has been the large-scale adoption of solar energy. The adoption of solar energy in countries like India is propagating mainly through the development of energy producing photovoltaic farms. The realization of solar energy producing sites involves complex decisions and processes in the selection of sites whose knowhow may not rest with all the stakeholders supporting (e.g., banks financing the project) the industry value chain. In this article, we use the region of Bangalore in India as the case study to present how open innovation using satellite imagery can provide the necessary granularity to specifically aid in an independent initial assessment of the solar photovoltaic sites. We utilize the established analytical hierarchy process over the information extracted from open satellite data to calculate an overall site suitability index. The index takes into account the topographical, climatic, and environmental factors. Our results explain how the intervention of satellite imagery-based big data analytics can help in buying the confidence of investors in the solar industry value chain. Our study also demonstrates that open innovation using satellites can act as a platform for social product development.",10.1109/TEM.2019.2955315,N. P. Nagendra; G. Narayanamurthy; R. Moser; A. Singh
The Application of the Big Data Algorithm for Pipeline Lifetime Analysis,2019,-1,Outliers,0.23900306019125928,"Oil and gas pipeline integrity management has always been a field of huge data accumulation. In recent years, the rise of big data technology has provided new ideas for pipeline integrity evaluation technology. Firstly, this paper systematically studies the key technologies related to the lifetime analysis of oil and gas pipelines and big data. Meanwhile, we also studies the next generation oil and gas pipeline big data system architecture, the development direction of big data architecture tends to be batch-stream unified and integrated. We completes the model establishment process from theoretical derivation, algorithm design and gives the key algorithm steps to accurately predict the pipeline inspection period. From the results, in the single models the minimum risk decision based on Naive Bayes is the best, the accuracy rate is 91.86%, and in the ensemble model the accuracy of GBDT is slightly better than that of the random forest, which accuracy rate reached 99.7%. In contrast, the ensemble learning method has a much better dataset fitting performance, which provides an idea for the selection of the algorithm in engineering applications.",10.1109/CAC48633.2019.8996228,J. Gu; H. Zhang; L. Chen; S. Lian
Exploring privacy-enhancing technologies in the automotive value chain,2021,2,Topic_2_data_privacy_security,1.0,"Privacy-enhancing technologies (PETs) are becoming increasingly crucial for addressing customer needs, security, privacy (e. g., enhancing anonymity and confidentiality), and regulatory requirements. However, applying PETs in organizations requires a precise understanding of use cases, technologies, and limitations. This paper investigates several industrial use cases, their characteristics, and the potential applicability of PETs to these. We conduct expert interviews to identify and classify uses cases, a gray literature review of relevant open-source PET tools, and discuss how the use case characteristics can be addressed using PETs’ capabilities. While we focus mainly on automotive use cases, the results also apply to other use case domains.",10.1109/BigData52589.2021.9671528,G. M. Garrido; K. Schmidt; C. Harth-Kitzerow; J. Klepsch; A. Luckow; F. Matthes
Architecture Design of Intelligent Management System for Multi-value Chain Collaborative Data Space,2022,3,Topic_3_industry_manufacturing_chain,0.846305941703899,"With the in-depth development of information technology, in order to solve the problems of data collaboration barriers, poor knowledge mining effect and management optimization performance in the process of multi-value chain collaboration in manufacturing industry. At the same time, it realizes the wisdom synchronization of data flow, two-way transmission of information flow, cognitive sharing of knowledge flow, optimized scheduling of business flow and value-added of value flow under the support of management engine. This paper adopts Block Chains, Big Data, Internet of Things, Artificial Intelligence, Cognitive Mapping and other technologies to build a multi-value chain collaborative data space intelligent management system architecture. It also designs the application mechanism for the integration of service engine and management engine, and the management optimization and coordination mechanism for the management engine and management system. To realize the intelligent management of multi-value chain collaborative data space.",10.1109/CSCWD54268.2022.9776162,J. Wang; Z. Liu; X. Yang; J. Han
Visual Analysis of Research Hotspots of Enterprise Transformation and Upgrading Based on Internet Big Data,2020,1,Topic_1_data_big_big data,0.7892600386612438,"This paper refers to the Chinese Social Science Citation Index (CSSCI) database. In this paper, the relevant research literature on enterprise transformation and upgrading from 2009 to 2019 is searched based on research hot spots of Internet Big Data visualization, to refine the research hot spot of enterprise transformation and upgrading for providing some reference for the future research of enterprise transformation and upgrading.",10.1109/IAI50351.2020.9262202,H. Xie; A. Wang; X. Gao
Comprehensive Energy Data Business Model Based on Business Canvas,2024,-1,Outliers,0.2985925337995099,"Energy is the lifeblood of the national economy. The imbalance of energy consumption structure leads to the imminent depletion of energy. In order to solve this problem, energy big data, which is the combination of energy production, consumption and big data, has been put on the agenda. Based on the background of energy big data, this paper conducts in-depth research on the business model of energy big data from multiple perspectives such as customer segmentation, value proposition, channel, customer relationship, revenue stream, key resources, key activities, partners, cost structure, etc., so as to give full play to its business value.",10.1109/CSIS-IAC63491.2024.10919342,C. Xu; J. Feng; C. Xiao; R. Huang; H. Cai; Q. Wei
Research on the evaluation system for the implementation effect of the Chinese water efficiency mandatory national standard for water closets (GB 25502-2017) based on big data platform and FAHP method,2021,-1,Outliers,0.35579651498840914,"To enhance the methodology to evaluate the implementation effects of ""Minimum allowable values of water efficiency and water efficiency grades for water closets"" (GB 25502-2017), we analyzed various influence factors and established a system using AHP method and conducted empirical research based on the Fuzzy comprehensive evaluation method. An AHP evaluation model for the implementation effect of the water efficiency standard was established for the first time, which consisted of 31 indicators. Most evaluation results of these indicators could be found though big data platform. The empirical research results mostly from big data investigation show that the implementation effect of the standard is nearly good, especially the technical requirements are consistent with other standards, regulations and market development. They are advanced with international standards. This paper establishes a method system for the evaluation of the implementation effect of water efficiency standards, conducts an empirical analysis of the implementation effect of GB 25502-2017, and explores the application of big data analysis in the evaluation of the implementation effect of the standards..",10.1109/ICBASE53849.2021.00140,Y. -B. Zhang; L. Lin; H. -Y. Hu; X. Bai
A study of digital twin-based digital derivation mechanisms for manufacturing companies,2023,3,Topic_3_industry_manufacturing_chain,1.0,"This paper starts from the endogenous causes of value creation of the digital twin of manufacturing enterprises and breaks down the composition and functions of the digital twin. Through the process linkage and collaboration of the digital twin, the 4.0 value chain architecture of manufacturing enterprises is dissected. On this basis, the endogenous driving mechanism of the supply chain and industrial chain derived from the 4.0 value chain of manufacturing enterprises is analysed using manufacturing big data, and the framework of the enterprise digital endogenous integration tower 3D process of manufacturing enterprises is proposed, and the nature and derivation mechanism of the supply chain of manufacturing enterprises based on the 4.0 value chain drive is analysed to systematise the formation mechanism and framing mode of the digital twin. The manufacturing enterprise industrial chain is the main body of the digital economy development and the fundamental research paradigm of the new theory of the newly created digital era, whose breakthrough and transformation is the basis for the future matching and optimisation of industrial core elements with industrial digital organisation and technological innovation resources. This is a newly innovative exploration of the digital knowledge system of manufacturing enterprises, which will have a profound impact on the construction of a digital enterprise industry chain with full perception, full connectivity, full scenario and full intelligence, boosting the development of the digital economy and building a new paradigm system of macro and micro interdisciplinary theories.",10.1109/CISCE58541.2023.10142669,F. Wang; C. Zhe; M. Sun
SAC: A System for Big Data Lineage Tracking,2019,1,Topic_1_data_big_big data,1.0,"In the era of big data, a data processing flow contains various types of tasks. It is nontrivial to discover the data flow/movement from its source to destination, such that monitoring different transformations and hops on its way in an enterprise environment. Therefore, data lineage or provenance is useful to learn how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. However, existing systems offer limited support for such use cases in a distributed computing setup. To address this issue, we build Spark-Atlas-Connector (short as SAC), a new system to track data lineage in a distributed computation platform, e.g., Spark. SAC tracks different processes involved in the data flow and their dependencies, supporting different data storage (e.g., HBase, HDFS, and Hive) and data processing paradigms (e.g., SQL, ETL, machine learning, and streaming). SAC provides a visual representation of data lineage to track data from its origin to downstreams, and is deployed in a distributed production environment for demonstrating its efficiency and scalability.",10.1109/ICDE.2019.00215,M. Tang; S. Shao; W. Yang; Y. Liang; Y. Yu; B. Saha; D. Hyun
The State of Big Data Reference Architectures: A Systematic Literature Review,2022,1,Topic_1_data_big_big data,1.0,"Big Data (BD) is a nascent term emerged to describe large amount of data that comes in different forms from various channels. In modern world, users are the ceaseless generators of structured, semi-structured, and unstructured data that if gleaned and crunched precisely, will reveal game-changing patterns. While the opportunities exist with BD, the unprecedented amount of data has brought traditional approaches to a bottleneck, and the growth of data is outpacing technological and scientific advances in data analytics. It is estimated that approximately 75% of the BD projects have failed within the last decade according to multiple sources. Among the challenges, system development and data architecture are prominent. This paper aims to facilitate BD system development and architecture by conducting a systematic literature review on BD reference architectures (RA). The primary goal is to highlight the state of BD RAs and how they can be helpful for BD system development. The secondary goal is to find all BD RAs, describe the challenges of creating these RA, discuss the common architectural components of these RA and the limitations of these RA. As a result of this work, firstly major concepts about RA are discussed and their applicability to BD system development is depicted. Secondly, 22 BD reference architecture is assessed from academia and practice and their commonalities, challenges, and limitations are identified. The findings gained emerges the understanding that RAs can be an effective artefact to tackle complex BD system development.",10.1109/ACCESS.2022.3217557,P. Ataei; A. Litchfield
BBC: A DSL for designing cloud-based heterogeneous bigdata pipelines,2017,1,Topic_1_data_big_big data,1.0,"Several frameworks are available for processing and analyzing the bigdata. But there is limited support available in integrating such heterogeneous bigdata programs to create a bigdata pipeline. In this paper, we introduce BBC, a Domain Specific Language (DSL) designed to create a bigdata pipeline from the heterogeneous programs. The paper focuses on Spark jobs but using a similar approach the DSL can be extended for other frameworks. We have successfully used BBC DSL to design the The Home Depot (THD) search evaluation pipeline that is instrumental in deploying every search feature to the THD website.",10.1109/BigData.2017.8258099,F. Jacob; I. Karunanithi; P. Salian; R. Sambhu
Orchestrating Apache NiFi/MiNiFi within a Spatial Data Pipeline,2023,1,Topic_1_data_big_big data,1.0,"In many smart city projects, a common choice to capture spatial information is the inclusion of LiDAR data, but this decision will often invoke severe growing pains within the existing infrastructure. In this paper, we introduce a data pipeline that orchestrates Apache NiFi (NiFi), Apache MiNiFi (MiNiFi), and several other tools as an automated solution in order to relay and archive LiDAR data captured by deployed edge devices. The LiDAR sensors utilized within this workflow are Velodyne Ultra Pucks sensors that capture at a rate of 10 frames per second and produces 6-7 GB packet capture (PCAP) files per hour. By both compressing the file after capturing it and compressing the file in real-time, we discovered that gzip produced a file of 5 GB and saved about 5 minutes in transmission time to NiFi, as well as saving considerable CPU time when compressing the file in real-time. Alternatively, we chose XZ as the compression algorithm for the ingestion of LiDAR data onto an institution compute cluster due to its high compression ratio. In order to evaluate the capabilities of our system design, the features of this data pipeline were compared against existing third-party services, namely Globus and RSync.",10.1109/SERA57763.2023.10197731,C. Carthen; A. Zaremehrjardi; V. Le; C. Cardillo; S. Strachan; A. Tavakkoli; F. C. Harris; S. M. Dascalu
Big data exploitation for maritime applications a multi-segment platform to enable maritime big data scenarios,2017,1,Topic_1_data_big_big data,0.5898939216340283,"Although a plethora of individual and disconnected applications can be found serving the “data exploitation for marine-related applications” profile, there is a lack of networked initiatives bringing together organisations and knowledge from different scientific and policy domains, as well as geographical areas. The present paper is a work under the European funded research project BigDataOcean1 and its main objective is to build on this identified need for maritime stakeholders and establish a completely new value chain of interrelated data streams coming from diverse sectors, leveraging existing modern technological breakthroughs in the areas of the big data driven economy. The main output of the proposed approach will be novel services and applications for maritime-related industries, organisations and stakeholders through a multi-segment platform that will combine data of different velocity, variety and volume and will serve as a constantly growing pool of cross-sectorial and multi-lingual linked data, bringing together organisations of different activity fields and needs.",10.1109/ICE.2017.8280008,P. Kokkinakos; A. Michalitsi-Psarrou; S. Mouzakitis; I. Alvertis; D. Askounis; S. Koussouris
DataOps for Societal Intelligence: a Data Pipeline for Labor Market Skills Extraction and Matching,2020,-1,Outliers,0.16960635866626148,"Big Data analytics supported by AI algorithms enable skills localization and retrieval, in the context of a labor market intelligence problem. We formulate and solve this problem through specific DataOps models, blending data sources from administrative and technical partners in several countries into cooperation, creating shared knowledge to support policy and decision-making. We then focus on the critical task of skills extraction from resumes and vacancies featuring state-of-the-art machine learning models. We showcase preliminary results with applied machine learning on real data from the employment agencies of the Netherlands and the Flemish region in Belgium. The final goal is to match these skills to standard ontologies of skills, jobs and occupations.",10.1109/IRI49571.2020.00063,D. A. Tamburri; W. -J. V. D. Heuvel; M. Garriga
Big Data and Supply Chains Strategy in the 21st Century: insights from the field,2020,3,Topic_3_industry_manufacturing_chain,0.5356638146958065,"This study focuses on big data application in supply chain management, which added value orientation, offer new a guideline and operational strategy for existing supply chain practitioner and related manager. A survey was conducted among employees of multinational companies across French, China and Switzerland, and in qualitative methods by focus groups and interview to explore the feasibility and sincerity of the application of big data, discuss the situation in digitalization of supply chain of strategy in practical business. The study also examines the ways in which research in digitalization of supply chain business and related fields differ effect when responding to and managing. The study expands grounds theory the applicability of information the digital supply chain strategy from academic research to commercial application. The results show that the adoption of big data technology can create considerable value-added and monetary gain for firms and will soon become a standard throughout the industry. due to the cost of learning and some non-negligible risks, it is currently more effective for middle and senior practitioners. It can be an inefficient business strategy for basic practitioners.",10.1109/DFHMC52214.2020.00026,Z. Changyi
Blockchain Application in Mass Customization: A Furniture Sector Example,2020,-1,Outliers,0.2960474599173537,"Blockchain technology attracts attention with its successful results in many areas. It has managed to become widespread in the global arena with the efficiency, security, and transparency feature it offers in applications. Detailed information requirements in every stage and process of a product in the manufacturing sector require traceability from the manufacturer to the customer. In addition, with the developing technology, manufacturers turn to customer-oriented production models. For this purpose, a traceable furniture chain prototype that develops production focused on customer demands is proposed as an exemplary scenario. In this study, the traceability problem was analyzed and a blockchain-based platform was proposed. In addition, the compatibility of the Internet of Things (IoT), Cyber-Physical Systems (CPS), and Big Data technologies, which have made great progress in the manufacturing sector, with the blockchain has been examined. In addition, the question of why blockchain should be used with these technologies was investigated.",10.1109/ICDABI51230.2020.9325630,N. Baygin; M. Baygin; M. Karaköse
Big Data Framework to Detect and Mitigate Distributed Denial of Service(DDoS) Attacks in Software-Defined Networks (SDN),2023,1,Topic_1_data_big_big data,0.6176437214596908,"The software-defined network is in recent years come into the sight of so many network designers as a successor to the traditional network. This type of network is vulnerable to DDoS attacks, targeting a different part of the SDN network architecture by continuously injecting fake flows. It imposes substantial processing on the controller, and the result ultimately leads to the inaccessibility of the controller and the lack of network service to legitimate users.This paper proposes a scalable and reliable real-time DDoS attack detection and mitigation framework using machine learning incorporated with a big data pipeline infrastructure for the SDN environment. The framework supports the detection of DDoS attacks with the ability of processing and analyzing the huge amount of network traffic in near real-time. The framework employs the power of big data solutions, such as Apache Kafka, and Apache Spark in order to realize a scalable pipeline for big data processing.The main objective of this research is to offer a vital supporter which can assist network security systems in addressing the DDoS threats in the SDN environment. For this purpose, machine learning algorithms are also utilized to provide a viable opportunity for classifying network data and detecting DDoS attacks in this regard.",10.1109/ICECCME57830.2023.10253230,A. F. Hamedani; M. Aziz; P. Wieder; R. Yahyapour
Text Big Data Analytics: Exploring API opportunity: Internet as global storage — how to get the situation awareness from dark data,2016,1,Topic_1_data_big_big data,1.0,"Text Big Data Analytics Study ""Third Wave"" is described. Morphological matrix of several Keywords Phrases was collected from Internet's open textual resources using API. The results are analyzed from the point of view that global Internet's audience forms ""people-to-IT"" system through that we can study the three levels of behaviour of Global society. Level 1 - countries' activity on the Internet. Level 2 - technological evolution in countries. Level 3 - countries' reaction to destructive stress factors. Cluster analysis was used for investigation of Level 1. Simon Wardley's Value Chain Map was used for investigation of Level 2. And diagrams with percentage ratio of different sets of Keywords Phrases were used for investigation of Level 3. Article is focused on regions of Eurasia and North Africa. The Study findings are following: among post-Soviet countries Russia shows the highest activity on Internet and has advantages of economic development, as well as Georgia; Azerbaijan among some countries from Eurasia and North Africa shows the lowest risk to become fragile country under Global warming.",10.1109/ICAICT.2016.7991649,O. Kolesnichenko; G. Smorodin; D. Yakovleva; L. Mazelis; O. Zhurenkov; Y. Kolesnichenko
An Interactive Approach to Support Event Log Generation for Data Pipeline Discovery,2022,1,Topic_1_data_big_big data,0.367205197830806,"Process Mining is a discipline that sits between data mining and business process management. The starting point of process mining is an event log, which is analyzed to extract useful insights and recurrent patterns about how processes are executed within organizations. However, often its concrete application is hampered by the considerable preparation effort that needs to be conducted by human experts to collect the required data for building a suitable event log. Instead, event logs need to be extracted from different and heterogeneous data sources, often using customized extraction scripts whose implementation requires both technical and domain expertise. While this is recognized as a relevant issue in the process mining community, literature solutions tend to be ad-hoc for particular application contexts, or not enough structured to be easily applied in practice. In this paper, we tackle this issue by proposing an interactive and general-purpose approach to support organizations in generating simulated event logs that can be employed to discover the structure of the data pipelines executed within a business process. A data pipeline is a composite workflow for processing data that is enacted as part of process execution. To assess the practical applicability of the approach, we show the results of a preliminary evaluation performed in a digital marketing scenario in the range of the recently funded H2020 DataCloud project.",10.1109/COMPSAC54236.2022.00184,D. Benvenuti; L. Falleroni; A. Marrella; F. Perales
Detailed Investigation of Influence of Internet of Things and Big Data on Digital Transformation in Marketing,2023,3,Topic_3_industry_manufacturing_chain,1.0,"In the industrial age of digitalization, businesses are spending more money on for the purposes of data gathering, analysis, and performance improvement, they need tools and technology that let their operations, machinery, workers, and even the actual items can all be a member of the same web. Big Data Analytics (BDA) and the Internet of Things (IoT) are two breakthroughs that have already had a substantial impact on enterprises' usage of digital communication. Examining the benefits of BDA and the factors that influence users' emotions regarding information systems in either a negative or positive way is essential to reducing attrition associated with the installation of new technology. Value chain model is used to assess the impact of Smart manufacturing on the organisation. This model is particularly useful when focusing on business sectors that play a significant role in generating consumer value. The strategy is applicable since Industry 4.0 has so far had the most disruptive effects on value-creating activities, where it is most felt. The aim of this research is to understand how Hungary-based businesses comprehend the concept of Industries, what Internet of Everything solutions they employ to assist their workflows, and what major challenges they encounter when adapting. The Network Big data is growing exponentially at the same time that the Internet of Things is experiencing an exponential growth in the number of internet of things. Important problems about the efficacy of data collection, processing, analysis, and security are raised by big data administration in a network that is always expanding. Researchers have looked at the difficulties involved with the effective implementation of IoT in order to solve these concerns. The aim of this research paper is to conduct a detailed investigation about the influence of big data and Internet of Things (IoT), on digital transformation in marketing. The researchers of this study set some research objectives which are described systematically with the help of an appropriate research methodology. Furthermore, this paper has significantly covered all relevant points and different aspect of new emerging technologies of future.",,K. Pandit; D. Buddhi; A. Averineni; M. S. Narayana; G. Jahnavi; K. Rohitha
Sakdas: A Python Package for Data Profiling and Data Quality Auditing,2020,1,Topic_1_data_big_big data,0.7325191441454063,"Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.",10.1109/IBDAP50342.2020.9245455,S. Loetpipatwanich; P. Vichitthamaros
Big data provenance and analytics in telecom contact centers,2017,1,Topic_1_data_big_big data,1.0,"Cloud computing and big data are the two important technology innovations which have the potential to restructure the value chain of telecom service providers. Apart from total cost saving, there are many operational and business objectives while adopting cloud computing technology into the telecom domain. Telecom service providers typically have terabytes of operational data available. When effectively analyzed, this information can help them to maintain consistent and appropriate service delivery across the seasonal peaks and valleys. Big data analytics offer telecom operators a real opportunity to gain a broad image of their day to day operations, customers, and more over organizations and innovative efforts to be taken. In this research paper we identify and list the key factors affecting the operational performance of a telecom contact center and the potential role of big data analytics and provenance in overcoming the performance bottlenecks. We present a case study evaluating how telecom companies can employ big data analytics in inbound and outbound contact centers for enhancement of operational performance. Results indicate that, despite of many challenges including lack of well-defined big data processing strategies, potential security threats and the need for workforce re-skilling, the use of opportunities of Big Data analytics in telecom domain is unprecedented.",10.1109/TENCON.2017.8228107,B. Jose; T. R. Ramanan; S. D. M. Kumar
An AI-based Alarm Prediction in Industrial Process Control Systems,2022,0,Topic_0_prediction_degradation_rul,0.47436523677893416,"In process-based industries, modern process control systems have become data-driven and collect a vast amount of data from sensors in the field and alarm information. The collected data provides an opportunity for the data scientist to learn from historical data and apply Machine Learning (ML) models to automate the process control systems. Thus, assisting the plant operators in making informed decisions. In this paper, we focus on the alarm prediction of control systems. Alarm prediction assists plant operators in observing the functioning of plants and taking corrective measures beforehand to avoid upcoming failure situations. A data pipeline is proposed in this paper comprising two approaches for alarm prediction. Both the approaches consider the alarm log simulated data from an industrial three-phase separator process typically found in oil fields. The first approach requires domain knowledge regarding the alarm thresholds values, and ML models are trained using the threshold values to perform alarm prediction. The second approach comprises ML models trained independently of the alarm threshold values, thus providing the alarm prediction time window. The alarm prediction time window provides the plant operator sufficient time to act on an impending failure. As the outcome from the two approaches is different, Long short-term memory (LSTM) is the best performing model for the first approach with an RMSE value of 0.03. For the second approach, a fully convolutional network (FCN) is the best performing model for time windows of 20, 40, 60, and 120 minutes, and LSTM is the best performing model with an accuracy 94% for the time window of 10 minutes.",10.1109/BigComp54360.2022.00053,M. Dix; A. Chouhan; M. Sinha; A. Singh; S. Bhattarai; S. Narkhede; A. Prabhune
SQL-SA for big data discovery polymorphic and parallelizable SQL user-defined scalar and aggregate infrastructure in Teradata Aster 6.20,2016,1,Topic_1_data_big_big data,0.9726626412493842,"There is increasing demand to integrate big data analytic systems using SQL. Given the vast ecosystem of SQL applications, enabling SQL capabilities allows big data platforms to expose their analytic potential to a wide variety of end users, accelerating discovery processes and providing significant business value. Most existing big data frameworks are based on one particular programming model such as MapReduce or Graph. However, data scientists are often forced to manually create adhoc data pipelines to connect various big data tools and platforms to serve their analytic needs. When the analytic tasks change, these data pipelines may be costly to modify and maintain. In this paper we present SQL-SA, a polymorphic and parallelizable SQL scalar and aggregate infrastructure in Aster 6.20. This infrastructure extends Aster 6's MapReduce and Graph capabilities to support polymorphic user-defined scalar and aggregate functions using flexible SQL syntax. The implementation enhances main Aster components including query syntax, API, planning and execution extensively. Integrating these new user-defined scalar and aggregate functions with Aster MapReduce and Graph functions, Aster 6.20 enables data scientists to integrate diverse programming models in a single SQL statement. The statement is automatically converted to an optimal data pipeline and executed in parallel. Using a real world business problem and data, Aster 6.20 demonstrates a significant performance advantage (25%+) over Hadoop Pig and Hive.",10.1109/ICDE.2016.7498323,X. Tang; R. Wehrmeister; J. Shau; A. Chakraborty; D. Alex; A. Al Omari; F. Atnafu; J. Davis; L. Deng; D. Jaiswal; C. Keswani; Y. Lu; C. Ren; T. Reyes; K. Siddiqui; D. Simmen; D. Vidhani; L. Wang; S. Yang; D. Yu
Streamlined Data Pipeline for Real-Time Threat Detection and Model Inference,2025,1,Topic_1_data_big_big data,0.5880263178463188,"Real-time threat detection in streaming data is crucial yet challenging due to varying data volumes and speeds. This paper presents an architecture designed to manage large-scale, high-speed data streams using deep learning and machine learning models. The system utilizes Apache Kafka for high-throughput data transfer and a publish-subscribe model to facilitate continuous threat detection. Various machine learning techniques, including XGBoost, Random Forest, and LightGBM, are evaluated to identify the best model for classification. The ExtraTrees model achieves exceptional performance with accuracy, precision, recall, and F1 score all reaching 99% using the SensorNetGuard dataset within this architecture. The PyFlink framework, with its parallel processing capabilities, supports real-time training and adaptation of these models. The system calculates prediction metrics every 2,000 data points, ensuring efficient and accurate real-time threat detection.",10.1109/COMSNETS63942.2025.10885573,R. Singh; A. V; S. Mishra; S. K. Singh
Big Data Tools: Interoperability Study and Performance Testing,2023,1,Topic_1_data_big_big data,1.0,"The technological revolution, the huge sharing of data via social networks, web and mobile applications and IoT devices are generating a huge volume of data every day, commonly referred to as “Big Data”. To cope with Big Data and the challenges associated with their specific features, the last decade, a multitude of technologies and platforms have emerged to harness their potential. The community is still seeking a comprehensive and up-to-date comparative study of these tools. Such an experimentally-derived study is essential for enabling informed decision-making, fostering innovation, and ensuring that organizations can make the best choices when implementing Big Data solutions. In this paper, a multi-purpose experimental study was conducted. The primary objective is to provide an overview of today’s most popular Big Data tools, and to evaluate their interoperability. The second is to test performance by varying different technical constraints. The aim of these tests is twofold: i) To compare the resource consumption requirements of these tools, ii) To evaluate the impact of resource variation of one tool on the performance of another one in the same Big Data pipeline.",10.1109/BigData59044.2023.10386089,A. Dhaouadi; W. Paccoud; K. Bousselmi; S. Monnet; M. M. Gammoudi; S. Hammoudi
Trends of digitalization and adoption of big data & analytics among UK SMEs: Analysis and lessons drawn from a case study of 53 SMEs,2020,-1,Outliers,0.36596833164040377,"Small and Medium Enterprises (SMEs) now generate digital data at an unprecedented rate from online transactions, social media marketing and associated customer interactions, online product/service reviews and feedback, clinical diagnosis, Internet of Things (IoT) sensors, and production processes. All these forms of data can be transformed into monetary value if put into a proper data value chain. This requires both skills and IT investments for the longterm benefit of businesses. However, such spending is beyond the capacity of most SMEs due to their limited resources and restricted access to finance. This paper presents lessons learned from a case study of 53 UK SMEs, mostly from the West Midlands region of England, supported as part of a 3-year ERDF11European Regional Development Fund. project - Big Data Corridor22An ERDF funded 3-year project from 2016 to 2019: https://bigdatacorridor.com/. - in the areas of big data management, analytics and related IT issues. Based on our study's sample companies, several perspectives including digital technology trends, challenges facing the UK SMEs, and the state of their adoption in data analytics and big data, are presented.",10.1109/ICE/ITMC49519.2020.9198545,M. Mohamed; P. Weber
Technology Assessment Using Satellite Big Data Analytics for India's Agri-Insurance Sector,2023,-1,Outliers,0.1782828790870922,"Over half of India's employment is attached to the agriculture sector and their survival is dependent on the performance of farms. The uncertainty in the performance of farms due to weather fluctuations and other risks is tackled by providing insurance cover. However, policymakers’ choice of administrative measures for estimating crop loss has resulted in inaccurate data collection, opened vulnerability to the politicization of the process, and created bottlenecks to operate at scale. These problems have led to skewed timelines for data collation, lack of confidence in the data produced by the agri-insurance providers, and caused long-drawn delays in settling claims made by farmers. In this article, we present a case study on the assessment of using satellite big data as a technology deployed in Northern India to solve the aforementioned problems between the stakeholders in the agri-insurance claim settlement process. Satellite big data based analytics provides an independent data source and decision-making platform for the agri-insurers to conduct an assessment for calculating the indemnity payments. The results showcase how transparency brought in by the satellite big data analytics curbs the plausible exploitation of the claim settlement process and leads to increased efficiency and efficacy in settling farmer claims.",10.1109/TEM.2022.3159451,N. P. Nagendra; G. Narayanamurthy; R. Moser; E. Hartmann; T. Sengupta
A 5G-IoT enabled Big Data infrastructure for data-driven agronomy,2022,-1,Outliers,0.32663773972481835,"The increasing necessity of efficient and effective agriculture has pushed towards the development of computer aided techniques, where on-field measurements are used to take objective decisions to optimize the production, giving birth to data-driven agronomy. With the diffusion of 5G-based IoT devices it becomes possible to deploy a variety of sensors in large amounts, enabling continuous collection of monitoring data. Agronomists necessitate the adoption of Big Data techniques and technologies to handle such large amount of data. These solutions provide powerful tools to analyze and model the complexity of the field, i.e. applying statistics and Machine Learning based methods to the product processes. In this paper, we propose a Big Data infrastructure that integrates with 5G-enabled sensors, providing scalable data ingestion, pipelining and information querying capabilities. We also show a practical scenario where our infrastructure has been implemented and report preliminary results on its performance.",10.1109/GCWkshps56602.2022.10008727,F. Berto; C. Ardagna; M. Torrente; D. Manenti; E. Ferrari; A. Calcante; R. Oberti; C. Fra’; L. Ciani
A Blockchain Technology-Based Marketplace to Speed Up Life Cycle Assessment Calculations,2025,-1,Outliers,0.31586361326448825,"In recent decades, environmental challenges have become an increasingly global concern. The rapid advancement of technology and the growing demand for products and resources have fueled industrial growth but also exacerbated environmental issues. To identify key impact areas and enhance environmental performance, accurately quantifying the environmental footprint of various processes is essential. Life Cycle Assessment (LCA) is a widely recognized methodology for measuring and estimating these impacts. However, its application faces several challenges, including difficulties in collecting reliable supply chain data, inefficiencies in data distribution, privacy concerns that discourage companies from sharing information, and the absence of incentives for data sharing. This paper explores how a digital ecosystem, built around a multi-sided market, can facilitate the structured exchange of environmental life cycle data within a value chain. Specifically, it examines how blockchain technology can enhance transparency and efficiency in data tracking and sharing. Smart contracts can automate data validation and transaction execution, reducing the risk of fraud or manipulation while streamlining data transactions. Specifically, the study focuses on leveraging this technology to improve environmental impact data sharing through a purposebuilt marketplace designed to accelerate LCA calculations. The proposed framework is based on a decentralized platform that ensures secure data sharing and verification by utilizing a public infrastructure. The platform's architecture follows a modern three-tier design pattern, and the proposed infrastructure serves as a proof of concept.",10.1109/ICE/ITMC65658.2025.11106600,E. Marra; C. Capuzzimati; S. Menato; L. Canetta
Evaluating the Quality of Social Media Data in Big Data Architecture,2015,1,Topic_1_data_big_big data,1.0,"The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.",10.1109/ACCESS.2015.2490723,A. Immonen; P. Pääkkönen; E. Ovaska
A Spatial Data Pipeline for Streaming Smart City Data,2024,1,Topic_1_data_big_big data,1.0,"Point cloud data in the form of LiDAR is often utilized for its spatial qualities, especially in smart city projects for tasks involving vehicles and pedestrians. However, the process in which LiDAR data is acquired can be cumbersome to setup and automate. In this paper, we introduce a streaming and an on-demand pipeline for capturing LiDAR data from Velodyne Ultra Pucks placed along northern Nevada intersections known as the Living Lab as part of a smart city project for the city of Reno. The data coming from these intersections consist of the following formats: ROS 2 bag file, PCD, LAZ, Google Draco, and PCAP. A streaming point cloud service with PCD, LAZ, and Draco was implemented to stream any of these formats, as well as to allow the user to capture the current monitored point cloud. Additionally, two on-demand web services were implemented for both the PCAP and ROS 2 bag file to enable a user to start and stop the acquisition of LiDAR data in these formats. Through our analysis, it was discovered that Draco provided the best processing time and had a wider range of options that affected the quality of the point cloud. To evaluate this pipeline, the features of existing software were compared and a discussion was provided with an analysis of the point cloud formats.",10.1109/SERA61261.2024.10685604,C. Carthen; A. Zaremehrjardi; V. Le; C. Cardillo; S. Strachan; A. Tavakkoli; S. M. Dascalu; F. C. Harris
Valuable Insights Framework for Big Data and Analytics in the Malaysian Public Sector Organization,2023,1,Topic_1_data_big_big data,0.6329875801005663,"Implementation of Big Data and Analytics (BDA) is crucial to facilitate quality decision making among higher management in the government bodies. BDA consists of large datasets that are able to reveal relationships, trends, and noteworthy patterns, especially those connected to other interactions and human behaviors. Big data's potential worth is increasingly apparent to the public sector. Governments generate and gather enormous amounts of data through routine operations. It performs the role of data consumer in a way that enables data to be used for the facilitation of decision-making in public policymaking, service delivery, organizational management and innovation. Data analytics can make the appropriate decisions at the right time is crucial to achieving improved policy outcomes. In conjunction with the establishment and arising awareness of data driven decision making in the public sector, there is a significant need to understand the values of BDA from real organization and settings. Hence, this study has identified the values of BDA from four perspectives of financial, people, process and research development. These perspectives have been enriched via qualitative approaches of case studies from selected ministries and public agencies. The findings show and elaborate how far these valuables perspectives true from the official based on their experience. At the end, this framework will guide and give awareness about the impact and values of BDA in facilitating strategic thinking and planning in the decision making. By taking BDA as an effective method or tool which clearly emphasizes data, it drives for more transparent and accountable decision making in the bureaucracy of public sector.",10.1109/HORA58378.2023.10156701,Y. M. Zain; S. Yaacob; R. Ibrahim; S. S. Hussein
A Case Study in Creating Transparency in Using Cultural Big Data: The Legacy of Slavery Project,2018,-1,Outliers,0.16439625789605636,"The Maryland State Archives (MSA) and the Digital Curation Innovation Center (DCIC) of the University of Maryland's iSchool are collaborating on a digital project that utilizes digital strategies and technologies to create an in-depth understanding of the African-American experience in Maryland during the era of slavery. Utilizing crowdsourcing for transcription, data cleaning and transformation techniques, and data visualization strategies, the joint project team is creating new avenues for understanding the complex web of relationships that undergirded the institution of slavery. iSchool students, full participants on the project team, are learning digital curation and other technical skills while gaining insights into the multiple uses of how cultural Big Data can penetrate the past and illuminate the present.",10.1109/BigData.2018.8621932,R. Cox; S. Shah; W. Frederick; T. Nelson; W. Thomas; G. Jansen; N. Dibert; M. Kurtz; R. Marciano
Big Data Analytics in Industry 4.0: Automating Production Metrics With an Edge-Based Architecture,2025,3,Topic_3_industry_manufacturing_chain,0.510509439786443,"This paper presents the development and evaluation of a Big Data Analytics (BDA) platform designed for Industry 4.0 applications, focusing on the integration, processing, and visualization of data at the edge. The platform integrates tools such as Apache Spark, Airflow, MinIO, Airbyte, and Streamlit, to facilitate automated data collection and report generation. Deployed in a real manufacturing environment in São Paulo, Brazil, the solution aims to minimize data latency by avoiding cloud transmission and leveraging edge computing. Automated daily reports have been generated since May 2024, improving decision-making and reducing human error associated with manual data integration. This work provides a practical contribution to industrial data integration, highlighting the benefits of edge-based data processing, automation, and visualization to optimize production efficiency and decision-making processes.",10.1109/ICIT63637.2025.10965329,A. Kretzer; D. Costa; G. Pardini; J. Wolkers; R. Luz; F. Siqueira
"Semantic Data Ingestion for Intelligent, Value-Driven Big Data Analytics",2018,1,Topic_1_data_big_big data,0.7365944519349201,"In this position paper we describe a conceptual model for intelligent Big Data analytics based on both semantic and machine learning AI techniques (called AI ensembles). These processes are linked to business outcomes by explicitly modelling data value and using semantic technologies as the underlying mode for communication between the diverse processes and organisations creating AI ensembles. Furthermore, we show how data governance can direct and enhance these ensembles by providing recommendations and insights that to ensure the output generated produces the highest possible value for the organisation.",10.1109/Innovate-Data.2018.00008,J. Debattista; J. Attard; R. Brennan
Stream2Graph: Dynamic Knowledge Graph for Online Learning Applied in Large-scale Network,2022,1,Topic_1_data_big_big data,0.5850450193780667,"Knowledge Graphs (KG) are valuable information sources that store knowledge in a domain (healthcare, finance, e-commerce, cyber-security.). Most industrial KGs are dynamic by nature as they are updated regularly with streaming data (customer activity, network traffic, application logs, IT process). However, extracting insights from continuously updated data comes with major challenges, particularly in big data settings. In this paper, we address the following challenges: 1) ingesting heterogeneous data, 2) training and deployment of predictive models on continuously evolving data, and 3) implementation of data pipelines for updating and maintaining the KG in production. We cover multiple aspects of this process, from knowledge collection to its operationalization. We propose Stream2Graph, a stream-based system for building and updating the knowledge base dynamically in real time. Then we show how graph features can be used in downstream online machine learning models. The solution speeds up big data stream learning and knowledge extraction to enhance Graph-based AI applications. Experimental results show the effectiveness of our solution for knowledge base construction and improvement of big data learning capabilities. Using data from Stream2Graph resulted in speedups for training and inference time in the range from 547x to 2000x in downstream ML models. Finally, we provide the lessons learned from applying graph-based online learning on large-scale network processing high-velocity streaming data.",10.1109/BigData55660.2022.10020885,M. Barry; A. Bifet; R. Chiky; S. El Jaouhari; J. Montiel; A. El Ouafi; E. Guerizec
System Restore in a Multi-cloud Data Pipeline Platform,2019,1,Topic_1_data_big_big data,1.0,"Data pipeline platforms hosting big data analytics can span multiple clouds. Backup and restore service is typically applied to deal with data corruptions in such platforms. This paper proposes a novel approach to providing consistency to the restored state of a multi-cloud data pipeline platform from its backups, and also presents the performance of the approach demonstrated in a dry run test.",10.1109/DSN-Industry.2019.00012,L. Wang; H. Ramasamy; V. Salapura; R. Arnold; X. Wang; S. Bakthavachalam; P. Coulthard; L. Suprenant; J. Timm; D. Ricard; R. Harper; A. Gupta
Green Open Innovation and Circular Economy: Investigating the Role of Big Data Management and Sustainable Supply Chain,2024,3,Topic_3_industry_manufacturing_chain,0.46229477634003835,"The study delves into the concept of circular economy target (CET) performance in the context of green open innovation (GOI) to understand the role of Big Data management (BDM) and sustainable supply chain performances (SSCPs), including knowledge management (KM). The authors developed a self-administered survey for 294 participants from the cement, electronics, tyres, rubber, and energy sectors. These industries are relevant for their environmental impacts and sustainability challenges. The study's empirical findings emphasize the positive association between BDM with Big Data capability (BDC) and KM. Further, it positively associates with SSCP and CET, especially in the context of GOI. GOI was also observed as a moderating variable in the relationships between BDC-SSCP and KM-SSCP. The conceptual framework elucidates the interaction between BDC, knowledge capabilities, and SSCP. It underscores the collective synergy of these components in achieving CET. This is in the frame of GOI, which intertwines the value chain between territorial actors such as companies, universities, and research institutes that exploit and convert big data into knowledge to get new forms of sustainable innovations.",10.1109/TEM.2024.3387107,R. K. Singh; K. Mathiyazhagan; V. Scuotto; M. Pironti
Scalable Cloud Architectures for Efficient Processing of Multi-Structured Big Data,2025,1,Topic_1_data_big_big data,0.8941221231829193,Large Language Models (LLMs) now dominate the field of intelligent cloud-native applications by providing strong generative functionality with context-aware automated operations. The achievement of large-scale benefits from LLMs depends on implementing an integrated data engineering solution which combines multi-source data pipelines and live data processing while supporting operational governance measures and maximizing scalability and cost effectiveness. This paper develops an integrated data engineering system designed for cloud applications using LLMs which applies modular design patterns with distributed control systems and automated pipeline optimization algorithms. The framework combines data ingestion along with feature transformation and metadata management capabilities and LLM-centric model services that incorporate Mops and Limos practices. Here it is explained both the design structure of the system alongside deployment tactics and measurement standards which support higher operational output and processing capacity. The system provides multiple cloud platform capabilities including AWS Azure and GCP so businesses can deploy LLMs in multi-tenant environments with standardized data management and security features. Experimental assessments prove the framework supplies scalability while minimizing latency and achieving operational stability which prepares it as a base for future-generation enterprise AI applications.,10.1109/GINOTECH63460.2025.11077101,K. Alang; S. B. Peta; R. R. Pai; B. Patil
Continuous Performance Improvement of Infrastructure Guidance Service for Autonomous Cooperative Driving: Focusing on Data-centric AI,2024,1,Topic_1_data_big_big data,0.6948403317645663,"Infrastructure services in autonomous cooperative driving are providing various applications along with the development of edge computing technology. While existing infrastructure services simply integrated collected information and shared it with autonomous vehicles, research that actively intervenes in the driving judgment and control of real-time autonomous vehicles is increasing. The representative research is a project to develop autonomous driving support technology using infrastructure guidance. In this project, we are developing technology that predicts the driving intentions and paths of vehicles in real-time using an Edge RSU (Road Side Unit) equipped with sensors and ML models, and provides guidance information that enables vehicles to drive more safely and efficiently. This paper deals with the research content of a data pipeline that can continuously improve the performance of Machine Learning (ML) models installed in Edge RSU in infrastructure guidance services. The data pipeline concept presented in this study was designed based on Data-Centric AI from data collection to ML model deployment. The method that was researched for autonomous driving vehicles was redesigned to be suitable for infrastructure guidance services. This concept has the advantage of allowing multiple Edge RSUs to perform continuous data collection and services from fixed locations. For this reason, we expect higher stability and efficiency than the results applied to autonomous driving vehicles. The current research stage is proceeding with the development of an automated pipeline design and ML models to be installed in Edge RSU and plans to apply it to the infrastructure guidance service testbed from 2024.",10.1109/ICEIC61013.2024.10457156,J. Kim; J. Jeon; J. Park; S. Jung
A data management and analytic model for business intelligence applications,2017,1,Topic_1_data_big_big data,0.5938643165341663,"Most organisations use several Data Management and Business Intelligence solutions which are on-premise and/or cloud-based to manage and analyse their constantly growing business data. Challenges faced by organisations nowadays include but not limited to: growth limitations, big data, inadequate analytics, computing, and data storage capabilities. A data management and analytic model is proposed for organisations to rely on for decisive guidance when planning to procure and implement a unified business intelligence solution. The model which is considered hybrid relies on existing on-premise and cloud models with limitations mitigated. To assess the state of business intelligence utilisation, and validate and improve the proposed architecture, two case studies targeting users and experts were conducted following quantitative and qualitative approaches. The findings further recognised the proposed hybrid architecture as appropriate for managing complex organisations with big data challenges.",10.23919/ISTAFRICA.2017.8102350,M. Banda; E. K. Ngassam
Strategy-oriented Digital Transformation of Logistics Enterprises: The roles of artificial intelligence and blockchain,2020,3,Topic_3_industry_manufacturing_chain,0.7218240892356648,"Currently, affected by the COVID-19, the global economy has experienced varying degrees of impact. The logistics industry, which is the foundation of social and economic operations, has also been affected to varying degrees. Under this economics background, the logistics industry is driven by factors such as resource integration to attract support from technology companies, new demand for contactless delivery, intelligent operating platforms such as blockchain, and artificial intelligence to replace labor shortages. The challenges for the development of logistics enterprises have become more prominent. This paper starts from a new strategic perspective: the strategy-oriented digital transformation of the logistics enterprises analyzes from the three levels of the logistics value chain, delicate operation development, and digital logistics activities. We will present the applications of the most trending digital technology in logistics enterprises. The purpose is to help enterprises better understand how digital transformation through blockchain and artificial intelligence can enhance their competitiveness as a business strategy.",10.1109/CITISIA50690.2020.9371847,H. Liu; S. M. N. Islam; X. Liu; J. Wang
Construction of Data Quality Evaluation Index for Manufacturing Multi-value Chain Collaborative Data Space Based on the Whole Life Cycle of Data,2021,1,Topic_1_data_big_big data,0.4348348678255148,"As a new data management model, data space can effectively manage a large amount of multi-heterogeneous dynamic data, but the construction of data space often needs to be based on accurate and scientific original data and to obtain valuable information in data, which poses a challenge to the data quality control of the whole life cycle of data, so it is especially important to evaluate the data quality. By analyzing the synergistic effect of multi-value chain in manufacturing industry and combining the dynamic system of the whole life cycle of data, the data quality evaluation index system is proposed from three aspects of data provider, data space construction and data user, combining four levels of data itself, technology, data flow layer and data management. Through the construction of AHP-TOPSIS data quality evaluation model, AHP is used to determine the index weight, TOPSIS is used to calculate the ideal solution and relative closeness degree, and the evaluation results are obtained. Through the application analysis of examples, quantitative evaluation of data quality, the construction, access and mining of multi-value chain collaborative data space can provide practical experience.",10.1109/CCIS53392.2021.9754682,S. Peng; Z. Tian; Z. Siqin; X. Xu
Intelligent Data Management Practice Under Ocean Shipping Based on Data Lifecycle Theory,2023,2,Topic_2_data_privacy_security,0.6184672019634557,"The purpose of this article is to explore the use of wireless communication technology for network connectivity in ocean liner environments, which is different from the data security system of wired networks. The key work is based on data security practices in the ocean liner environment, including building a data security classification system and developing different security strategies in data collection, storage, transmission, processing, and other aspects. In addition, machine learning methods are introduced into security warning strategies to intelligently analyze data security risks and make decisions.",10.1109/IHMSC58761.2023.00026,H. Jing; C. Ming-jun
Towards Security Monitoring for Cloud Analytic Applications,2018,1,Topic_1_data_big_big data,0.8956709068671912,"Cloud computing is empowering new innovations for big data. At the heart, cloud analytic applications become the most-hyped revolution. Cloud analytic applications have remarkable benefits for big data processing, making it easy, fast, scalable and cost-effective; albeit, they pose many security risks. Security breaches due to malicious, vulnerable, or misconfigured analytic applications are considered the top security risks to big data. The risk is further expanded from the coupling of data analytics with the cloud. Effective security measures, delivered by cloud analytic providers, to detect such malicious and anomalous activities are still missing. This paper presents real-time security monitoring as a service (SMaaS). SMaaS is a novel framework that aims to detect security anomalies in cloud analytical applications running on Hadoop clusters. It aims to detect vulnerable, malicious, and misconfigured applications which violate data integrity and confidentiality. Towards achieving this goal, we are motivated by leveraging big data pipeline that mixes advanced software technologies (Apache NiFi, Hive, and Zeppelin) to automate the collection, management, analysis, and visualization of log data from multiple sources, making it cohesive and comprehensive for security inspection. SMaaS monitors a candidate application by collecting log data on real-time. Then, it leverages log data analysis to model the application's execution in terms of information flow. The information flow model is crucial for profiling processing activities conducted throughout the application's execution. Such model, in turn, enriches the detection of various types of security anomalies. We evaluate the detection effectiveness and performance efficiency of our framework. The experiments are conducted over benchmark applications. The evaluation results demonstrate that our system is a viable solution, yet very efficient. Our system does not make modification in the monitored cluster, nor does it impose overhead to the monitored cluster's performance.",10.1109/BDS/HPSC/IDS18.2018.00028,M. Elsayed; M. Zulkernine
The Data Provision Game: Researching Revenue Sharing in Collaborative Data Networks,2020,1,Topic_1_data_big_big data,0.4760205136975699,"Data sharing between companies enables formerly unexploited potential for data-driven business applications. While it is clear how the owner of the business application will profit from the shared data, it is still in question how other stakeholders are incentivized to share data across company borders. We developed the data provision game based on a synthesized view of data value chains as a mathematical framework to analyze the economic interactions and incentives in a collaborative data network. We thereby emphasize revenue sharing as an important managing activity as everyone who contributes to a business application should profit from it in order to set right incentives. Further, we differentiate between entity-borne and network-borne activities. Finally, we formulated Shapley Pricing and Leave-One-Out Pricing as two options for revenue sharing mechanisms, which set incentives for welfare-optimal participation and data quality.",10.1109/CBI49978.2020.00028,W. Badewitz; S. Kloker; C. Weinhardt
A comprehensive scenario agnostic Data LifeCycle model for an efficient data complexity management,2016,1,Topic_1_data_big_big data,0.6367855693294481,"There is a vast amount of data being generated every day in the world, coming from a variety of sources, with different formats, quality levels, etc. This new data, together with the archived historical data, constitute the seed for future knowledge discovery and value generation in several fields of eScience. Discovering value from data is a complex computing process where data is the key resource, not only during its processing, but also during its entire life cycle. However, there is still a huge concern about how to organize and manage this data in all fields, and at all scales, for efficient usage and exploitation during all data life cycles. Although several specific Data LifeCycle (DLC) models have been recently defined for particular scenarios, we argue that there is no global and comprehensive DLC framework to be widely used in different fields. For this reason, in this paper we present and describe a comprehensive scenario agnostic Data LifeCycle (COSA-DLC) model successfully addressing all challenges included in the 6Vs, namely Value, Volume, Variety, Velocity, Variability and Veracity, not tailored to any specific environment, but easy to be adapted to fit the requirements of any particular field. We conclude that a comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data organization and integration, easing the adaptation to any kind of scenario, guaranteeing good quality data levels, and helping save design time and efforts for the research and industrial communities.",10.1109/eScience.2016.7870909,A. Sinaeepourfard; J. Garcia; X. Masip-Bruin; E. Marín-Tordera
Towards a Comprehensive Data LifeCycle Model for Big Data Environments,2016,1,Topic_1_data_big_big data,0.6017924743654154,"A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.",,A. Sinaeepourfard; J. Garcia; X. Masip-Bruin; E. Marín-Tordera
Data Lifecycle: From Big Data to SmartData,2018,1,Topic_1_data_big_big data,0.5089845020575023,"Data management is becoming increasingly complex, especially with the emergence of the Big Data era. The best way to manage this data is to dispose a data lifecycle from creation to destruction. This paper proposes a new Data LifeCycle (DLC) called Smart DLC that helps to make from raw and worthless data to Smart Data in a Big Data context. In order to do this, we have followed a method which consists firstly in identifying and analyzing the lifecycles from a literature review, and then in defining the phases of our cycle and finally in modeling it. The cycle is modeled in the form of a process cartography resulting from the ISO 9001: 2015 standard and the CIGREF framework to facilitate its implementation within companies. Smart DLC is qualified as a set of management, realization and support processes that could be addressed by an Information System urbanization approach. The advantage of modeling the phases such as processes is to be concerned not only with the technical activities but also with management, which is a major player for the success of the technique.",10.1109/CIST.2018.8596547,M. El Arass; N. Souissi
Smart Data Collection in Mobile Edge Computing Environment,2020,1,Topic_1_data_big_big data,1.0,"With the digital transformation, businesses and public administrations must change the place of data in the value chain to serve all areas of the business and open up information systems. The value of the knowledge extracted from this data is directly linked to the quality of data collection. Mobile devices are particularly suitable for reporting data. They are very widespread, very suitable and can be used at any time. These characteristics mean that the use of mobile support for data collection corresponds to a paradigm shift more than a simple new additional technology compared to the panoply of existing tools. The explosion of information sharing and data, which stems from our daily by these devices is stored mostly in the cloud servers. Thus, to reduce the number of data transferred and generated by mobile devices to the cloud servers, the edge computing allows to process data at the network edge where they are generated directly reducing certain characteristics of Big Data. Big data involves the collection of complex data on the “V” dimensions which describe the quantity and type of data collected, as well as their importance and relevance to the challenges of the requester. However, the smart data goes a step further and consist to extract from the data collected only the most relevant information for the client in order to make predictions. Our results show that using an intelligent data collection process in mobile computing could generate savings in terms of data storage and analysis at the cloud level.",10.1109/ISCV49265.2020.9204277,I. Tikito; N. Souissi
Tutorial and Workshop Summaries,2016,3,Topic_3_industry_manufacturing_chain,0.6857416633826908,"These tutorials and workshops discuss the following: Benchmarking, measurement, data analytics - trends and practice; Big Data technologies; Productivity impact factors for projects; Software measurement in the context of Industry 4.0; Data manipulation management; Estimating packaged software; and Metrics in contracts.",10.1109/IWSM-Mensura.2016.012,
Transmission analysis of the traditional enterprise with the Internet thinking,2015,3,Topic_3_industry_manufacturing_chain,0.7267456881334519,"In the Internet age, application of the Internet has penetrated into people's daily life. It has a great influence on people's daily life and overturns people's traditional conception of thinking. Depending on the Internet's resources integration ability, Internet companies are rapidly rising and affect the traditional industry's core business. Many traditional enterprises have begun to transform itself through the Internet thinking. This paper embarks from the nature of the Internet and with the seven Internet thinking it derived to analysis the transformation of the enterprise model, the enterprise model and the enterprise value chain.",10.1109/LISS.2015.7369800,Jiawei Song; Dan Chang
Multi- Value Chain Auto Parts Demand Prediction Based on Dynamic Heterogeneous Graph Convolution*,2022,0,Topic_0_prediction_degradation_rul,0.5390129405220787,"Traditional platform-based procurement and sales system has the defects of single-point management. It lacks consideration of the global supply chain, the entire historical cycle, and the value chain flow. It is unable to make good joint procurement decisions and cross-chain procurement allocation. To deal with this problem, a Dynamic Heterogeneous Graph Convolution neural network based on Long Term Memory is proposed (DHGCN-LSTM). Firstly, several Spatial-temporal heterogeneous graphs are constructed based on the procurement and sales links in the multi-value chain. Secondly, the heterogeneous graphs of time series sales are input into GCN to extract the spatial features of multiple value chains. Next, the output results of GCN are aggregated and compressed into a 2-dimensional matrix. Finally, a 2-dimensional matrix is used as the input of LSTM to obtain the output prediction results. Experimental results show that the root RMSE of the prediction accuracy of the DHGCN-LSTM model is improved by about 10% compared with XXX, which can provide parts procurement and intelligent forecasting service for the Auto parts procurement department in practical application.",10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00331,X. WU; C. LI
Performance analysis of efficient pipeline architectures for underwater big data analytics,2015,1,Topic_1_data_big_big data,1.0,"Underwater sensor networks (UWSNs) have emerged as an essential technology for various undersea applications. However, the use of acoustic links in UWSNs poses critical challenges in terms of overall delay, energy consumption and low bandwidth, which make the transmission of raw data in large sizes impractical. In our previous work, we employed various nodes (processing nodes, gateway nodes, and sensing nodes) that are used to construct different candidate architectures (i.e. single, pipeline, and hybrid of parallel/pipeline). We exploit the idea of in-network data processing in order to reduce the volume of data and extract only valuable information. In this paper, we analytically calculate and compare the performance of the various types of proposed architectures. The tradeoff between the cost of processing and the expected speedup is also analyzed. The results confirm our previous research hypothesis that processing the collected data locally and applying the idea of pipeline/parallel processing leads to significant improvement in the performance of UWSNs.",10.1109/ISCC.2015.7405646,A. Alharbi; S. Ibrahim; R. Ammar; H. Alhumyani
QED: Groupon's ETL management and curated feature catalog system for machine learning,2016,1,Topic_1_data_big_big data,0.5566512245023812,"In today's technology industry where machine learning has become essential, the effectiveness of algorithms ultimately depends on a robust data pipeline, and fast model prototyping and tuning require easy feature discovery and consumption. Careful management of ETL processes and their produced datasets is key to both model development in the research stage and model execution in the production environment. In this paper we present QED, an ETL management and curated feature catalog system that provides robust, streamlined machine learning pipelines. First, QED promises dynamic, reliable, and timely data delivery to the production pipeline. Its enhanced ETL process persists data from upstream sources in local data stores and ensures their correctness. Second, in contrast to previous systems, QED is capable not only of producing a daily scoring dataset, but also a training dataset with minimized bias by preserving the historical observations of feature values. Third, QED's multiple data store design allows batch process of large datasets as well as fast random access to single records. Finally, its curated feature catalog system enables sharing and reuse of machine learning features. QED serves as the data backend for a variety of machine learning models that provide key insights into the global business, and optimize the daily operations of Groupon.",10.1109/BigData.2016.7840776,D. C. Spell; L. -Y. Wang; R. T. Shomer; B. Nooraei; J. Waggoner; X. -H. T. Zeng; J. Y. Chung; K. -C. Cheng; D. Kirsche
Continuous natural language processing pipeline strategy,2021,1,Topic_1_data_big_big data,0.6019350478357132,"Natural language processing (NLP) is a division of artificial intelligence. The constructed model's quality is entirely reliant on the training dataset's quality. A data streaming pipeline is an adhesive application, completing a managed connection from data sources to machine learning methods. The recommended NLP pipeline composition has well-defined procedures. The implemented message broker design is a usual apparatus for delivering events. It makes it achievable to construct a robust training dataset for machine learning use-case and serve the model's input. The reconstructed dataset is a valid input for the machine learning processes. Based on the data pipeline's product, the model recreation and redeployment can be scheduled automatically.",10.1109/SACI51354.2021.9465571,I. Pölöskei
Scalable Generic Platform For Big Data Visualization,2020,1,Topic_1_data_big_big data,0.4556357362392718,"The term data analytics is referred to as the study of raw data to gain knowledge. Visualizations are the best way to represent the analyzed data so that it could be consumed by anyone on the go. However, when the amount of data increases and use cases differ, creating these visualizations indulge difficulties and will become repetitive yet hectic tasks. The insights platform depicted here is a generic platform to ease the above said complex process. Here, a generalized platform is made using the internal data from the company and a full stack application is furnished. Since it's developed keeping the generic aspect in mind, the platform could be used for a wide range of use cases by making changes to the data engineering configurations and visualization queries. All others will remain the same and hence new and new applications could be developed in no time with much ease.",10.1109/PICC51425.2020.9362365,C. C; A. P; J. Joy; S. K. P
Development of Data Ingestion Pipelines for the Federated Use of Biomedical Data in Research: The Health Big Data Project,2024,-1,Outliers,0.43109979999880027,"The secondary use of health data represents a great opportunity to advance pathophysiological knowledge and improve patients’ care. However, the absence of standard data formats and information structuring schemas severely hinders this potential, preventing the efficient sharing of data collected in different hospitals and affecting the quality of multicentric studies. The 10-year Health Big Data (HBD) project aims to address these issues to foster the collaboration of 51 Italian research hospitals (IRCCSs). To address the seven main challenges identified for health data sharing, seven Working Groups (WGs) were created, with the WG2 being responsible for the definition of standardization and harmonization pipelines for signals, bioimages, and omics data. The present paper focuses on two ongoing works of the WG2, namely the implementation of a pipeline to extract and map information from electrocardiographic (ECG) signals into the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) and the development of a harmonization pipeline to reduce the center effect in multicentric Magnetic Resonance Imaging (MRI) studies. We show interesting results and insights concerning the implementation of both pipelines. Besides, we highlight the main difficulties we encountered on our path toward health data sharing and suggest possible solutions.",10.1109/MELECON56669.2024.10608617,P. Reali; A. Carotenuto; D. Piantella; L. Tanca; P. Plebani; M. G. Signorini
A data lifeCycle model for smart cities,2016,1,Topic_1_data_big_big data,1.0,"Smart Cities are the most challenging and promising technological solutions for absorbing the increasing pressure of population growth, while simultaneously enforcing a sustainable economic progress as well as a higher quality of life. Several technologies are involved in a potential Smart City deployment, although data are the fuel to achieve the demanded and mandatory smartness. Data can be obtained from multiple sources, in large quantities, and with a variety of formats, therefore, an appropriate management is critical for their effective usage. Data life cycle models constitute an effective trend towards developing an integral and efficient data management framework, from data creation to data consumption and removal. In this paper we present the Smart City Comprehensive Data LifeCycle (SCC-DLC) model, a data management architecture generated from a comprehensive scenario agnostic model, tailored for the particular scenario of Smart Cities. We define the management of each data life phase, and describe its implementation on a Smart City with Fog-to-Cloud (F2C) resources management, an architecture that combines the advantages of both cloud and fog strategies.",10.1109/ICTC.2016.7763506,A. Sinaeepourfard; J. Garcia; X. Masip-Bruin; E. Marin-Tordera; X. Yin; C. Wang
Research on Technologies in Data Fabric,2023,1,Topic_1_data_big_big data,0.5124364985963817,"With the continuous advancement of technologies like big data, artificial intelligence, and cloud computing, enterprises are increasingly encountering challenges related to the heterogeneity of data from various sources, difficulties in querying data assets, and understanding business insights. To address these issues, data fabric emerges as a solution that leverages distributed data management. By harnessing metadata and facilitating its activation, data fabric enables organizations to efficiently and accurately manage and locate data assets. However, it is important to note that this concept is still under development and faces obstacles such as establishing connections with additional data sources and unifying semantics. Nonetheless, overall, the concept of data fabric exhibits remarkable potential in assisting enterprises in transitioning towards becoming data-driven organizations.",10.1109/TrustCom60117.2023.00309,Q. Hu; Z. Yin; T. Tao; J. Wang; Z. Chen; B. Ai; Y. Liu; C. Liu
Making Case for Using RAFT in Healthcare Through Hyperledger Fabric,2021,-1,Outliers,0.27524623841158885,"Blockchain technology is enabled by consensus algorithms to manage the relationships among several economic or business operators without human intervention. With the help of consensus algorithms, distributed systems can reliably reach agreement even if part of the system is faulty. Blockchain yields many benefits, among others, traceability, transparency, and security. We consider using the RAFT consensus algorithm to achieve robust and scalable decentralized applications, with focus on healthcare. We propose a stylized healthcare network, enabled by RAFT and built upon Hyperledger Fabric to showcase the use of RAFT in healthcare blockchain. However, RAFT is by no means limited to healthcare record systems, and can be applied to any other record system and value chain. Our paper offers several insights to those working in value chains and information management-related fields. In addition, we end our study with some future research avenues that may inspire managers and scholars to build or refine new decentralized systems in healthcare and other related fields.",10.1109/BigData52589.2021.9671934,A. Alexandridis; G. Al-Sumaidaee; R. Alkhudary; Z. Zilic
INTAP: Integrated Network Traffic Analysis Pipeline for LAN Monitoring System,2021,1,Topic_1_data_big_big data,0.8302492517811798,"Network security has become more important at present, due to an increase of cyber threats. One of the methods to protect our local area network from cyberattacks is a network monitoring system. It can capture traffic flowing in a network. However, the analysis of data from such network monitoring system is challenging, because it generates a large amount of data. Thanks to many tools developed to manage and process big data, we can solve that difficulty. In this paper, we present our design of the data analysis pipeline called Integrated Network Traffic Analysis Pipeline (INTAP) to overcome the challenges by the integration of the workflow management system, the distributed computing framework, and the data visualization tool. The analysis result with the network traffic data is presented in order to demonstrate the pipeline. The data source is our network monitoring system which is capturing network traffic from local area networks in the real world.",10.1109/ICIM52229.2021.9417147,P. Chirupphapa; H. Esaki; H. Ochiai
Digital Trends in the Mining Industry Value Chain,2022,3,Topic_3_industry_manufacturing_chain,1.0,"The term digitalization in the mining sector refers to the implementation of computerized or digital machinery, data, or systems, all of which are intended to reduce costs, boost corporate productivity and safety, and revolutionize industrial operations. The demand on top management to embrace the potentials offered by the digital economy to consider digital transformation a strategic priority is growing across all sectors of the economy, including the mining industry as the main production industry after oil and gas. It is widely acknowledged that digitalization is already a driving force within the mining sector and that it has a significant potential to modify enterprises’ and stakeholders’ relationships at each step of the value chain. Even though, there seems to be a lack of understanding of what needs to be studied from a managerial point of view in order to undertake digital transformation projects successfully. This paper aims to cover a brief overview of the current trends with reference to emerging technologies in the mining industry. Mining sector growth necessitates major reductions in production costs, a shift to lean production principles, and increases in state-of-the-art organizational performance. Besides the huge demand and the need for new low-carbon technologies (green technologies), it can be argued that the digital economy is considered a key driving force in the digital transformation of mining operations.",10.1109/eSmarTA56775.2022.9935469,M. Yaqot; B. C. Menezes
Public Broadcaster's Big-data Technology Trajectories: The Case of NHK and BBC,2018,1,Topic_1_data_big_big data,1.0,"The competitive environment of the broadcasting sector is changing; under this change, public broadcasters have to adapt to keep being relevant to their users. Big-data technologies play an essential part in the technological side of these changes. Our objective is to identify the public broadcasters' big-data technology trajectories in this changing environment. We propose two research questions to narrow down the objective: What are the big-data technology trajectories of public broadcasters? Also, which are the directions of big-data technologies proposed by the public broadcasters? We propose as the method, to analyze scientific paper's keywords and combine it with network analysis. We compare two datasets, big-data, and public broadcasters. The big-data set is borrowed from a previous work done by the authors which detected big-data keywords proxy of knowledge convergence. The public broadcasters' dataset is created from the scientific publications reported by BBC and NHK. We match the big-data converging keywords to the keywords of the BBC and NHK publications and visualize their behavior over the time (2008-2016). We analyze the documents linked to the shared keywords on both datasets to identify the big-data technology trajectories and propose future directions. We identified as big-data technological trajectories for BBC, Linked open data, recommender system, semantic web, and Image processing; For NHK, speech recognition, Generate metadata to index NHK's programs and Augmented reality (AR). Concerning their future, the detected trajectories are expected to be useful for broadcasters an organizations related to their value chain.",10.23919/PICMET.2018.8481989,R. -N. Santiago; M. Kumiko
Apache Kafka on Big Data Event Streaming for Enhanced Data Flows,2024,1,Topic_1_data_big_big data,0.9237127509805256,"Apache Kafka's distributed architecture and message queuing capabilities offer significant improvements in real-time and batch data processing efficiency and reliability. This research aims to optimize Kafka setups, data partitioning, and Kafka Connect integration to create a robust and scalable data streaming infrastructure. The study focuses on enhancing data input, processing, and dissemination across systems and applications. By optimizing Kafka configurations and leveraging its capabilities, the research seeks to achieve significant improvements in data processing speed, real-time analytics, and scalable data pipelines. The evaluation of Kafka Event Stream Throughput Over Time and Latency Distribution Across Brokers demonstrates the system's performance and efficiency. The results indicate a throughput of 750-1340 events per hour and a latency distribution of 6-15 milliseconds. Additionally, Consumer Lag Over Time analysis reveals consistent performance with values ranging from 70-140. This research contributes to the advancement of big data processing by demonstrating the effectiveness of Apache Kafka in creating a robust and efficient data streaming infrastructure. The findings provide valuable insights for organizations seeking to optimize their data pipelines and leverage the power of real-time analytics.",10.1109/I-SMAC61858.2024.10714884,K. Padmanaban; T. R. Ganesh Babu; K. Karthika; B. Pattanaik; D. K; C. Srinivasan
Evaluate the Informationizalized Effect of Market Demand on Drug Control Based on Target Multiple Structural Analysis Model,2016,3,Topic_3_industry_manufacturing_chain,0.6962632269527985,"Based on the market demand and the competition pressure, using the value chain theory and multi level structural analysis model as the main direction of drug control, and analysis the dual reconstruction properties proceeding from numerical fitting of a resource and a variety of resources. Finally, on the basis of the analysis of the integrated management organization mode, the reconstruction is put forward to realize the value network, and through the analysis of the market value of the multimode resources, the management method of pharmaceutical enterprises for drug control is obtained. According to the method-of pharmaceutical enterprises for drug control, the informationizalized effectiveness is evaluated. Through the establishment of multiple structural analysis model of target analyzing constraints, the informationizalized effects of the drug control of a resource and a variety of resources were compared and analyzed. On this basis, this paper puts forward the suitable direction of pharmaceutical enterprises for drug control in our country, so as to improve the effectiveness of the drug price control of our government.",10.1109/ICITBS.2016.107,S. Yu; T. Li; G. Chen; F. Shang; K. Huang
Towards an Autonomous Application of Smart Services in Industry 4.0,2021,3,Topic_3_industry_manufacturing_chain,0.6265168529952232,"Today's high complexity and required expertise in various disciplines for data-based evaluations of shop-floor assets is challenging. This paper describes the ongoing development towards an Industry 4.0 ecosystem enabling Smart Services and shop-floor assets to network autonomously. Three partial solutions are combined for this purpose: Industry 4.0 digital twins, automated data streams and a Smart Service toolbox. A prototypical implementation proves the general practicability. Furthermore, future work is outlined to achieve full autonomy.",10.1109/ETFA45728.2021.9613369,M. Redeker; C. Klarhorst; D. Göllner; D. Quirin; P. Wißbrock; S. Althoff; M. Hesse
Navigating Data Governance in the Telecom Industry,2023,1,Topic_1_data_big_big data,0.36742685560690025,"Data governance plays a crucial role in the telecommunications industry, where effective management of data assets is vital for operational efficiency, customer satisfaction, and regulatory compliance. This paper presents a comprehensive roadmap for data governance in the telecom sector, addressing the challenges and encompassing all pillars of data governance. The roadmap outlines step-by-step guidelines for establishing a data governance program, assessing current state and defining governance policies, implementing data stewardship and quality management, ensuring privacy and regulatory compliance, enhancing data security and risk management, managing data lifecycle, enabling data integration and interoperability, and overcoming industry-specific challenges. Finally, a case study demonstrates the successful implementation of data governance principles in the network domain, following a roadmap approach, resulting in significant improvements and positive outcomes.",10.1109/ICAEA60387.2023.10414472,F. Rahimi; F. J. Kaleibar; F. Feizi; A. H. Nia; H. Kashfi
Validating Crowdsourced Flood Images using Machine Learning and Real-time Weather Data,2022,1,Topic_1_data_big_big data,0.49269550209229823,"Within the large field of Crowdsourced Big Data, one of the largest problems is validating data automatically. In this paper, we address this problem for users that crowdsource uploaded flood images for flood prediction in Vietnam. We begin by creating a website that allows users to upload images of floods, allowing for crowdsourced data collection of flood images. Next, we automatically retrieve all new images from a backend in AWS, run a Convolutional Neural Network model on the image, and come up with a prediction for whether it is a flood or not. Next, we compare the prediction to the real-time weather data forecast to make sure that images are realistic for the given location at the given time. To ensure that the data retrieval can be scaled, we also developed a multi-threaded approach to real-time weather collection that achieved a speedup of over 8 times, meaning that parallelized calls can also be made when the system scales. After using 5-fold Cross Validation to evaluate the model with 1500 images, the CNN algorithm achieved a 93.47 percent accuracy. This validation method can be scaled across any problem in the world, thus securing access to crowdsourced big data, one of the largest and most reliable forms of big data in the world.",10.1109/BigDataSE56411.2022.00011,A. Gupta; A. Kim; A. Karande; S. Yan; S. Manandhar; N. R. Nguyen
Self-organizing tool for smart design with predictive customer needs and wants to realize Industry 4.0,2016,3,Topic_3_industry_manufacturing_chain,0.6418838642986596,"Following the first three industrial revolutions, Industry 4.0 (I4) aims at realizing mass customization at a mass production cost. Currently, however, there is a lack of smart analytics tools for achieving such a goal. This paper investigates this issues and then develops a predictive analytics framework integrating cloud computing, big data analysis, business informatics, communication technologies, and digital industrial production systems. Computational intelligence in the form of a self-organizing map (SOM) is used to manage relevant big data for feeding potential customer needs and wants to smart designs for targeted productivity and customized mass production. The selection of patterns from big data with SOM helps with clustering and with the selection of optimal attributes. A car customization case study shows that the SOM is able to assign new clusters when growing knowledge of customer needs and wants. The self-organizing tool offers a number of features suitable to smart design that is required in realizing Industry 4.0.",10.1109/CEC.2016.7748366,A. A. F. Saldivar; C. Goh; W. -n. Chen; Y. Li
Identifying smart design attributes for Industry 4.0 customization using a clustering Genetic Algorithm,2016,3,Topic_3_industry_manufacturing_chain,0.66893094658669,"Industry 4.0 aims at achieving mass customization at a mass production cost. A key component to realizing this is accurate prediction of customer needs and wants, which is however a challenging issue due to the lack of smart analytics tools. This paper investigates this issue in depth and then develops a predictive analytic framework for integrating cloud computing, big data analysis, business informatics, communication technologies, and digital industrial production systems. Computational intelligence in the form of a cluster k-means approach is used to manage relevant big data for feeding potential customer needs and wants to smart designs for targeted productivity and customized mass production. The identification of patterns from big data is achieved with cluster k-means and with the selection of optimal attributes using genetic algorithms. A car customization case study shows how it may be applied and where to assign new clusters with growing knowledge of customer needs and wants. This approach offer a number of features suitable to smart design in realizing Industry 4.0.",10.1109/IConAC.2016.7604954,A. A. F. Saldivar; C. Goh; Y. Li; Y. Chen; H. Yu
Attribute identification and predictive customisation using fuzzy clustering and genetic search for Industry 4.0 environments,2016,3,Topic_3_industry_manufacturing_chain,0.5830742757714366,"Today s factory involves more services and customisation. A paradigm shift is towards “Industry 4.0” (i4) aiming at realising mass customisation at a mass production cost. However, there is a lack of tools for customer informatics. This paper addresses this issue and develops a predictive analytics framework integrating big data analysis and business informatics, using Computational Intelligence (CI). In particular, a fuzzy c-means is used for pattern recognition, as well as managing relevant big data for feeding potential customer needs and wants for improved productivity at the design stage for customised mass production. The selection of patterns from big data is performed using a genetic algorithm with fuzzy c-means, which helps with clustering and selection of optimal attributes. The case study shows that fuzzy c-means are able to assign new clusters with growing knowledge of customer needs and wants. The dataset has three types of entities: specification of various characteristics, assigned insurance risk rating, and normalised losses in use compared with other cars. The fuzzy c-means tool offers a number of features suitable for smart designs for an i4 environment.",10.1109/SKIMA.2016.7916201,A. A. F. Saldivar; C. Goh; Y. Li; H. Yu; Y. Chen
Leveraging Data Build Tool for Efficient Data Management in Data Center Architectures,2025,1,Topic_1_data_big_big data,0.8816935650792199,"Data Build Tool simplifies data transformation, modelling, and analytics. Optimizing data pipelines ensures data processing integrity, scalability, and automation. Data Build Tool integration with current data warehouses improves decision-making with real-time data processing, version control, and collaboration. Data Build Tool improves data governance, quality assurance, and workflow efficiency via modular and SQL-based transformation. Automating dependency resolution and repeatability using Data Build Tool makes data engineering more dependable and human-free. Scalable infrastructure enables resource optimisation and cost-effective data center operations. Data Build Tool allows flexible data processing, systematic data management, and performance tuning. For digital transformation and cloud environments, automation and real-time analytics provide flexible and intelligent data center designs.",10.1109/ICCSP64183.2025.11089387,A. S. Murugan; S. K. Saravanan; G. S. Nidhya; K. Sasikala; P. Pandey; E. Arasan
Infusing Data Feeds and Models on Blockchain Contracts Through Oracle Based Infrastructure,2024,-1,Outliers,0.33440952241943017,"Recent technological advancements have made many industries more reliant on technology, which has led to an exponential increase in the amount of data generated every day. However, this surge in data has been driven by concerns including data breaches and lack of data management. Data security, integrity, efficient management, and advanced data analysis are the issues that are yet to be solved effectively. The extensive use of Artificial Intelligence (AI) and Centralized data storage have made it more accessible to sensitive information on computer networks and data breaches. This paper aims to address these challenges by ensuring secure data storage, sharing and computation within the systems, services and web browsers by using Big Data tools and frameworks along with AI and merge of Blockchain. The adoption of Blockchain Technology is of paramount importance in providing tamper-proof, ownership-guaranteed data, ensuring secure and dependable data sharing. Additionally, onchain Oracles have been used to improve connectivity from real-world data sources. The use of Big Data frameworks and tools, creates a strong infrastructure for data administration, monitoring, predictions, and analysis. The approach provides increased security, efficient management, and better connection in a decentralized manner.",10.1109/TENSYMP61132.2024.10752172,S. Gupta; K. Patiyar; A. Shrivastava; A. K. Bairwa
Enabling Product Circularity Through Big Data Analytics and Digitalization,2022,3,Topic_3_industry_manufacturing_chain,0.6277183756289137,"Circular Economy (CE) is an approach to economic development that eliminates waste and brings in circularity in use of resources. Apart from benefitting the environment and society, it offers companies an opportunity to turn inefficiencies of a linear value-chain into business value. But the transformation is not easy as it requires shift in business practices and mindset across the ecosystem. Digital technologies and solutions can enable and ease such transitions. In this paper we present key barriers to Circular Economy, particularly for circularity of products, and discuss resulting opportunities for using Big Data Analytics and digitalization to address them.",10.1109/MWSCAS54063.2022.9859280,U. Dayal; M. Gupta; D. Ghosh; D. Wadhawan; A. Morrow; S. Horiguchi; H. Wang; A. Rao; A. Osling; C. Gupta; R. Vennelakanti; A. Kumar
A Data Governance Framework for Industry 4.0,2021,3,Topic_3_industry_manufacturing_chain,0.787803830222983,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",10.1109/TLA.2021.9480156,J. Yebenes Serrano; M. Zorrilla
Covid-19 and electricity demand: focus on Milan and Brescia distribution grids,2020,-1,Outliers,0.13576787445113547,"In Italy, the government countermeasures against Covid-19 have had a large impact on citizens, companies and industries, affecting the electric energy consumption too. Lombardy is surely the main affected region in terms of Covid-19 cases and slow-down of production activities of its cities. For two of them, Brescia and Milan, the impact on the electric distribution is analysed keeping the entry into force of the government actions as relevant time references. The different response to the emergency of the two cities points out the prevalent services activities of Milan and the industrial character of Brescia. Moreover, for Brescia it is studied the variation of electric demand and its daily load profile for specific feeders that are representative of a consumer category. This analysis underlines the need of the Distribution System Operators to build an infrastructure able to manage, through a data driven approach (Big Data, machine learning), the unexpected variation of daily profiles in order to make the electric grid more resilient.",10.23919/AEIT50178.2020.9241083,E. Bionda; A. Maldarella; F. Soldan; G. Paludetto; F. Belloni
Energy Consumption for IoT Streaming Applications,2020,1,Topic_1_data_big_big data,0.8436912732477602,"Usage of Internet of Things (IoT) devices are growing rapidly, resulting in the evolution of IoT infrastructure and efficient energy consumption. Organizations processing IoT big data in real-time for insights require various platforms that process high-velocity data to obtain incremental results. Streaming workflow applications designed consists of different data pipelines to support the decision-making process. These applications run on virtual machines provided by IaaS (Infrastructure as a Service) similar to the on-premise data center. The I/O operations performed by storage systems on these virtual machines result in considerable energy consumption. This paper calculates the energy consumption of storage systems during I/O operations of streaming applications.",10.1109/CSCloud-EdgeCom49738.2020.00037,M. Muniswamaiah; T. Agerwala; C. C. Tappert
Research on the Integration of Management Accounting tools Based on Cloud Financial Sharing,2022,3,Topic_3_industry_manufacturing_chain,0.44583008746098235,"Based on contingency theory and resource-based theory, this paper explores the relationship between management accounting tool integration, dynamic capabilities and enterprise value creation under big data cloud financial sharing, proposes research hypotheses, reasonably selects variables and designs questionnaires through literature and related materials, and uses structural equation model 129 samples from businesses were analyzed. The study found that the higher the integration degree of enterprise management accounting tools, the higher the enterprise value creation ability, and confirmed that dynamic ability played a partial mediating effect in it. The construction of enterprise management accounting tools and the integration framework of strategy, organization, technology and business process is conducive to the identification of internal and external environmental changes, the rational allocation of resources, and the establishment of learning organizations, thereby effectively stimulating enterprise innovation vitality and improving Enterprise risk management level and value creation capability.",10.1109/ICCNEA57056.2022.00033,Y. Ji
Research on Influencing Factors of Benign Development of Manufacturing in Guangdong Province under the Background of Digital Innovation,2022,3,Topic_3_industry_manufacturing_chain,1.0,"The manufacturing in Guangdong Province has a solid foundation for development. As China further accelerates the pace of reform and opening up, Guangdong Province has made remarkable achievements in manufacturing by virtue of its good location advantages and export-oriented economic characteristics. However, in the context of Global trade, the manufacturing problems with labor-intensive characteristics have gradually emerged. With the advent of the digital information age, the influence of digital economy on the development of manufacturing is more and more prominent. Advanced information technologies such as the Internet, artificial intelligence and big data are widely used in all aspects of enterprise production and operation. Digital innovation will become the key to the development of manufacturing in the future. Combined with the current situation of digital innovation development in Guangdong Province, this paper discusses the relationship between digital innovation and the benign development of manufacturing in Guangdong Province, and analyzes the influencing factors of benign development of manufacturing in Guangdong Province under the background of digital innovation, so as to provide reference for the realization of benign development of manufacturing in Guangdong Province.",10.1109/ICDSBA57203.2022.00053,X. Li
GeoMatch: Efficient Large-Scale Map Matching on Apache Spark,2018,1,Topic_1_data_big_big data,1.0,"We contribute by developing GeoMatch as a novel, scalable, and efficient big-data pipeline for large-scale map matching on Apache Spark. GeoMatch improves existing spatial big data solutions by utilizing a novel spatial partitioning scheme inspired by Hilbert space-filling curves. Thanks to the partitioning scheme, GeoMatch can effectively balance operations across different processing units and achieve significant performance gains. We demonstrate the effectiveness of GeoMatch through rigorous and extensive benchmarks that consider data sets containing large-scale urban spatial data sets ranging from 166, 253 to 3.78 billion location measurements. Our results show over 17-fold performance improvements compared to previous works while achieving better processing accuracy than current solutions (97.48%).",10.1109/BigData.2018.8622488,A. Zeidan; E. Lagerspetz; K. Zhao; P. Nurmi; S. Tarkoma; H. T. Vo
An Audit Framework for Data Lifecycles in a Big Data context,2018,1,Topic_1_data_big_big data,0.49447774040443093,"Data management is becoming increasingly difficult for businesses especially with the proliferation of cloud computing and the increasing needs in analytics for big data such as data generated by the Internet of Things. Indeed., tasks such as data collection., analysis., or visualization become very complicated for companies that have difficulty identifying the data lifecycle that fits their data usage context and that also allows to transform this data into knowledge. To deal with this situation and in order for companies to be able to identify the most appropriate cycle for their context or even improve it., they must be able to evaluate it to determine its advantages and disadvantages. The contribution of this paper is part of this perspective to help companies choose their data lifecycle. In this sense., we have designed an audit framework for data lifecycles. This framework could constitute an efficient guide for companies to evaluate their Big data lifecycles.",10.1109/MoWNet.2018.8428883,M. E. ARASS; I. Tikito; N. Souissi
A DevSecOps-based Assurance Process for Big Data Analytics,2022,2,Topic_2_data_privacy_security,0.40068238574075643,"Today big data pipelines are increasingly adopted by service applications representing a key enabler for enterprises to compete in the global market. However, the management of non-functional aspects of the big data pipeline (e.g., security, privacy) is still in its infancy. As a consequence, while functionally appealing, the big data pipeline does not provide a transparent environment, impairing the users’ ability to evaluate its behavior. In this paper, we propose a security assurance methodology for big data pipelines grounded on the DevSecOps development paradigm to increase trustworthiness allowing reliable security and privacy by design. Our methodology models and annotates big data pipelines with non-functional requirements verified by assurance checks ensuring requirements to hold along with the pipeline lifecycle. The performance and quality of our methodology are evaluated in a real walkthrough analytics scenario.",10.1109/ICWS55610.2022.00017,M. Anisetti; N. Bena; F. Berto; G. Jeon
Data Lifecycle Management in Evolving Input Distributions for Learning-Based Aerospace Applications,2023,1,Topic_1_data_big_big data,0.5031363804744984,"Learning-enabling components are increasingly popular in many aerospace applications, including satellite pose estimation. However, as input distributions evolve over a mission lifetime, it becomes challenging to maintain performance of learned models. In this work, we present an open-source benchmark of a satellite pose estimation model trained on images of a satellite in space and deployed in novel input scenarios (e.g., different backgrounds or misbehaving pixels). We propose a framework to incrementally retrain a model by selecting a subset of test inputs to label, which allows the model to adapt to changing input distributions. Algorithms within this framework are evaluated based on (1) model performance throughout mission lifetime and (2) cumulative costs associated with labeling and model retraining. We also propose a novel algorithm to select a diverse subset of inputs for labeling, by characterizing the information gain from an input using Bayesian uncertainty quantification and choosing a subset that maximizes collective information gain using concepts from batch active learning. We show that our algorithm outperforms others on the benchmark, e.g., achieves comparable performance to an algorithm that labels 100% of inputs, while only labeling 50% of inputs, resulting in low costs and high performance over the mission lifetime.",10.1109/AERO55745.2023.10115970,S. Banerjee; A. Sharma; E. Schmerling; M. Spolaor; M. Nemerouf; M. Pavone
Software-Defined Infrastructure for Decentralized Data Lifecycle Governance: Principled Design and Open Challenges,2019,-1,Outliers,0.348223141777157,"Exploring and mining the explosive burst of ""big data"" has already generated a lot of innovative applications, especially the recent advances of AI applications, and thus produced big values to the human society and civilization. However, due to the centralized patterns of data governance activities, including creation, sharing, exchange, management, analytics, tracing, and accounting, the potential values of big data distributed on the Internet are far away from being adequately explored. The recent announcement of data protection policies/laws such as GDPR makes the problem even more challenging. We are now at a moment of truth where the data governance infrastructure should be reconsidered and redesigned. In this paper, we propose a software-defined infrastructure design in a decentralized fashion: data owners are able to implement and deploy their own rules to the application systems where the data are produced for further governance activities. Such a fashion is quite similar to the popular software-defined networking where users are allowed to deploy rules of switches and customize the use. Our principled infrastructure design can radically reform the current data governance activities into a decentralized topology. On the one hand, data can be separated from the application that generates the data, and data owners can have the full rights to decide where their data should be stored and how the data can be shared. On the other hand, data users can search, discover, integrate, and analyze the data from various data sources according to their application requirements and scenarios. As a result, we argue that our infrastructure can establish a new generation of responsive decentralized data governance that can promote the innovation of linking data to better adapt the open environment and diverse user requirements. With this perspective, we briefly discuss some key insights and enumerate several related new technologies and open challenges.",10.1109/ICDCS.2019.00166,G. Huang; C. Luo; K. Wu; Y. Ma; Y. Zhang; X. Liu
ETL Technologies for Big Data: A Comparative Study,2023,-1,Outliers,0.31280211164165916,"There is a significant increase in the generation of massive data worldwide. Various sources such as social media applications, blockchain technology, and numerous other systems are responsible for generating data. This information can be structured or unstructured, originating from different sources. In this context, the ETL (Extract, Transform, and Load) process plays a crucial role where demands for efficient business decisions in contemporary systems are grown.During the processing phase, the entire method is organized in a pipeline structure to ensure that the resulting data contains valuable and pertinent insights. Indeed, selecting an appropriate ETL technology can be challenging due to the abundance of options available. This article delves into several technologies used for performing ETL processes with the aim of examining their strengths when weaknesses. This can provide assistance to researchers and industry professionals in making informed decisions and choosing the most suitable technologies to meet their specific requirements.",10.1109/ADACIS59737.2023.10424341,L. Zineb; F. Rachid
Replication as Lineage Mechanism for Materialized Views in Lakehouse Architectures,2024,1,Topic_1_data_big_big data,0.5736336535330568,"Nowadays, the volume of data collected from online activity is continuously growing, starting from ecommerce websites, mobile applications or even IoT devices and sensors. All this data must be processed by more and more complex Big Data applications that gather and provide most useful information out of it. Consequently, the storage systems have evolved to keep up with all these changes, transitioning from traditional Data warehouses to Data Lakes and now to Lakehouses. All these applications consist of various transformations on stored data that result in another improved version of it, enriched with some new knowledge that brings value to the user. Based on the type of transformation, one can distinguish at least two types. The first can be simply executed on demand at runtime to get some insights from the collected data, such as the events count or some aggregated reports. The other type is more complex by its nature because it involves some complex processing of that data solely or in conjunction with other data. So, the result of such a transformation is both expensive and not feasible to be computed at runtime every time it is needed. Therefore, it must be persisted on storage once it is computed and then, kept up to date with the latest changes. This persisted computed version of the data is named materialized view, and multiple materialized views can be chained in data pipelines if the result of one transformation represents the input of the next one. Replication is the mechanism that adds lineage semantics to the materialized view transformation, independent of how many chained materialized views are kept, such as in the case of the bronzesilver-gold tables in the medallion architecture. Furthermore, this identifiability feature added to the materialized data facilitates both the propagation of all new incoming changes and the at-least-once processing semantics of the system in case of data reprocessing or in the case this reprocessing is the actual purpose of the service. The proposed replication solution consists of two types of system generated ids, one that uniquely identifies a unit of data and one that points to the original unit of data representing its source.",10.1109/INISTA62901.2024.10683854,D. -I. Sirbu; A. -T. Taleanu; F. Pop
Improving Sequential Recommender Systems with Online and In-store User Behavior,2024,1,Topic_1_data_big_big data,0.675064967246838,"Online e-commerce platforms have been extending in-store shopping, which allows users to keep the canonical online browsing and checkout experience while exploring in-store shopping. However, the growing transition between online and in-store becomes a challenge to online sequential recommender systems for future online interaction prediction due to the lack of holistic modeling of hybrid user behaviors (online & in-store). The challenges are two-fold. First, combining online & in-store user behavior data into a single data schema and supporting multiple stages in the model life cycle (pre-training, training, inference, etc.) organically needs a new data pipeline design. Second, online recommender systems, which solely relies on online user behavior sequences, must be redesigned to support online and in-store user data as input under the sequential modeling setting. To overcome the first challenge, we propose a hybrid, omnichannel data pipeline to compile online & in-store user behavior data by caching information from diverse data sources. Later, we introduce a model-agnostic encoder module to the sequential recommender system to interpret the user in-store transaction and augment the modeling capacity for better online interaction prediction given the hybrid user behavior.",10.1109/BigData62323.2024.10825717,L. Ma; A. Padmanabhan; A. Ganesh; S. Tang; J. Chen; X. Li; L. Morishetti; K. Nag; M. Patel; J. Cho; S. Kumar; K. Achan
Project Artifacts for the Data Science Lifecycle: A Comprehensive Overview,2022,1,Topic_1_data_big_big data,0.33014292245998195,"Through knowledge extraction from data with various methods, Data Science (DS) allows organizations to achieve improvements in performance. The execution of these projects is mainly supported by DS process models such as CRISP-DM. As a high percentage of DS undertakings are failing, revisions to current DS project management practices become necessary. Amongst others, ensuring traceability, reproducibility, and knowledge retention across the project present important success factors in DS projects. Some of the DS process models feature documentation artifacts for this purpose but not comprehensively for the complete DS lifecycle. Accordingly, in this research, existing documentation deliverables for DS are identified and examined by means of a literature review. Based on the established best practices from the process models, the contents of 18 derived project artifacts for DS documentation are synthesized for the DS lifecycle to improve DS project management.",10.1109/BigData55660.2022.10020291,C. Haertel; M. Pohl; D. Staegemann; K. Turowski
A new reality requiers new ecosystems,2015,1,Topic_1_data_big_big data,0.4718454296286614,We live in a time where borders between organizations are vanishing. A time where division equals multiplication. Where Open Source is the new alternative for a patent application. And in a time where Big Data offers new ways to add value to business and society. Such a time requires new ways of collaboration and organization of partners in the value chain. What we need is ecosystems that assure responsible application of data analysis and stimulate quality and innovation. In such a way that participants can easily plug in and out.,10.1109/HPCSim.2015.7237115,S. Klous
The Quantitative Indicators Analysis of the Information Marketing Strategy's Internal and External Environment,2016,3,Topic_3_industry_manufacturing_chain,0.6733630699534506,"The challenge of the information age is the information processing methods and the effective use of information, the traditional marketing strategy needs to rely on the development and use of information technology. The trend and degree of information technology means to the marketing strategy plan is the guarantee of the current industry marketing. In this paper, from the angle of quantitative analysis of the impact of internal factors and external factors of marketing, marketing strategy construction scheme evaluation index system, using the SWOT quantitative model of market advantages, weaknesses, opportunities and threats in the study, systematic and effective balance of various factors. Based on computer and information technology, marketing strategy design method developed a comprehensive consideration of the external macro environment and internal fine structure information, provide the basis for the development of effective marketing strategy, to make the industry advantage in the market, seize opportunities, avoid threats, make up the defects.",10.1109/ICITBS.2016.108,M. Zhao
From 5Vs to 6Cs: Operationalizing Epidemic Data Management with COVID-19 Surveillance,2020,1,Topic_1_data_big_big data,0.46753423608065137,"The COVID-19 pandemic brought to the forefront an unprecedented need for experts, as well as citizens, to visualize spatio-temporal disease surveillance data. Web application dashboards were quickly developed to fill t his g ap, b ut a ll of these dashboards supported a particular niche view of the pandemic (ie, current status or specific r egions). I n t his paper, we describe our work developing our COVID-19 Surveillance Dashboard, which offers a unique view of the pandemic while also allowing users to focus on the details that interest them. From the beginning, our goal was to provide a simple visual tool for comparing, organizing, and tracking near-real-time surveillance data as the pandemic progresses. In developing this dashboard, we also identified 6 key metrics which we propose as a standard for the design and evaluation of real-time epidemic science dashboards. Our dashboard was one of the first released to the public, and continues to be actively visited. Our own group uses it to support federal, state and local public health authorities, and it is used by individuals worldwide to track the evolution of the COVID-19 pandemic, build their own dashboards, and support their organizations as they plan their responses to the pandemic.",10.1109/BigData50022.2020.9378435,A. S. Peddireddy; D. Xie; P. Patil; M. L. Wilson; D. Machi; S. Venkatramanan; B. Klahn; P. Porebski; P. Bhattacharya; S. Dumbre; E. Raymond; M. Marathe
State of Industry 4.0 Across Six French Companies,2018,3,Topic_3_industry_manufacturing_chain,0.6875152479773975,"Industry 4.0 is a polarizing subject, affecting industries transnationally since 2011. To determine the state of Industry 4.0 across French companies in 2018, a pilot study was carried out dealing with six Industry 4.0 dimensions: Product Development Process, Steering & Control, Manufacturing & Operations, Smart Services, Big Data, and Organization Structure. Specialist knowledge and implementation of advanced technology in all dimensions is prerequisite to achieve a high standard in Industry 4.0. Beyond that, the interconnection and the integration of all elements along the value chain is crucial. Combined with the existing literature on these dimensions, the researchers assess and describe the current state and the existing challenges. The focus of the survey was to get insights in the activities of French companies towards Industry 4.0, and thereby assessing their state in each dimension. Good approaches were considered, but there is a lack of knowledge about the extensive aspects and the significance of Industry 4.0. Especially in cases, where Industry 4.0 technologies enable new business models such as Big Data or Smart Services, the knowledge is limited. Best known and partially implemented are elements in Product Development and Production. However, there is a lack of understanding for the key aspect across all interviewed companies; the integration of various objects in the value chain. The six dimensions cannot be treated separately, neither in theory nor in practice. It has become abundantly clear that there is room for improvement, and that Industry 4.0 is a highly relevant topic the companies need to confront in-depth and in detail.",10.1109/ICE.2018.8436256,Z. Chengula; M. A. Rubio Morato; T. Thurner; Y. Wiedensohler; L. Martin
Streaming Big Data with Open-source: A Comparative Study and Architectural Recommendations,2023,1,Topic_1_data_big_big data,0.9119255618996608,"This study focusses on providing state-of-the-art infrastructure for data pipelines in e-Commerce sector, especially for online stores. With people going digital and also latest impact of Covid-19, daily e-Commerce companies are dealing with large amount of data (terabytes to petabytes). With growing Internet of Things, systems of computing devices which are interrelated. The inter-relation may be between mechanical and digital machines, objects or people. The interrelated objects will be provided with unique identifiers and the ability to transfer data over a network without requiring human-to-human or human-to-computer interaction. Growth of big data poses several challenges and opportunities in every field of its usage. Realtime analysis of data and its inference gives a competitive edge over its partners in every business field especially in e-commerce. Recent advances in technology and tools have exposed new opportunities to get actionable insights from historical data like market data, customer demographics, along with real-time data. Advancement in distributed streaming technology makes it important to investigate existing streaming data pipeline capabilities in eCommerce sector with a focus on online stores. This study analyzes the published research works on streaming data pipelines in e-commerce sector also to facilitate e-commerce’s variety of data streaming applications requirement. A state-of-the-art lambda architecture for streaming is proposed completely based on open-source technologies. Challenge in proprietary owned streaming platforms are vendor lock-in, limited ability to customize, cost, limited innovation & support. Proposed reference architecture will address many streaming use cases compared to its competitors, it has support of large open-source community in providing the inter-operability between streaming & related technologies like connectors, apart from providing better performance apart from other open-source based product advantages.",10.1109/ICSCDS56580.2023.10105025,G. Vijayakumar; R. K. Bharathi
A Review of Multimodal Learning Analytics Architectures,2018,-1,Outliers,0.16753269387377231,"There is an increasing interest in Multimodal Learning Analytics (MMLA), which involves complex technical issues in gathering, merging and analyzing different types of learning data from heterogeneous data sources. However, there is still no common reference architecture to face these technical challenges of MMLA. This paper summarizes the state of the art of MMLA software architectures through a systematic literature review. Our analysis of nine architecture proposals highlights the uneven support provided by existing architectures to the different activities of the analytics data value chain (DVC). We find out in those infrastructures that data organization and decision-making support have been under-explored so far. Based on the lessons learnt from the review, we also identify that design tensions like architecture distribution, flexibility and extensibility (and an increased focus on data organization and decision making) are some of the most promising issues to be addressed by the MMLA community in the near future.",10.1109/ICALT.2018.00057,S. K. Shankar; L. P. Prieto; M. J. Rodríguez-Triana; A. Ruiz-Calleja
An Adaptive Scalable Data Pipeline for Multiclass Attack Classification in Large-Scale IoT Networks,2024,1,Topic_1_data_big_big data,0.6277965508758504,"The current large-scale Internet of Things (IoT) networks typically generate high-velocity network traffic streams. Attackers use IoT devices to create botnets and launch attacks, such as DDoS, Spamming, Cryptocurrency mining, Phishing, etc. The service providers of large-scale IoT networks need to set up a data pipeline to collect the vast network traffic data from the IoT devices, store it, analyze it, and report the malicious IoT devices and types of attacks. Further, the attacks originating from IoT devices are dynamic, as attackers launch one kind of attack at one time and another kind of attack at another time. The number of attacks and benign instances also vary from time to time. This phenomenon of change in attack patterns is called concept drift. Hence, the attack detection system must learn continuously from the ever-changing real-time attack patterns in large-scale IoT network traffic. To meet this requirement, in this work, we propose a data pipeline with Apache Kafka, Apache Spark structured streaming, and MongoDB that can adapt to the ever-changing attack patterns in real time and classify attacks in large-scale IoT networks. When concept drift is detected, the proposed system retrains the classifier with the instances that cause the drift and a representative subsample instances from the previous training of the model. The proposed approach is evaluated with the latest dataset, IoT23, which consists of benign and several attack instances from various IoT devices. Attack classification accuracy is improved from 97.8% to 99.46% by the proposed system. The training time of distributed random forest algorithm is also studied by varying the number of cores in Apache Spark environment.",10.26599/BDMA.2023.9020027,S. Saravanan; U. M. Balasubramanian
Architecture Description Framework For Data-Intensive Applications,2023,1,Topic_1_data_big_big data,0.5211571244791052,"Managing data has become increasingly difficult due to its exponential growth and multiple sources. In the business world, Data Architecture provides a systematic approach to describing, collecting, storing, processing, and analyzing data to meet business needs. It plays a crucial role in transforming data into valuable information by providing an abstract view of data-intensive applications. A framework for data-intensive applications is presented in this article, which utilizes model-driven engineering to achieve this goal. The framework's effectiveness was evaluated through five thorough case studies, with valuable feedback provided by seven practitioners and two prominent companies on its capabilities.",10.1109/IDSTA58916.2023.10317869,M. Abughazala; H. Muccini; M. Sharaf
Industrial Internet of Things: A Review,2019,3,Topic_3_industry_manufacturing_chain,0.9162098775171039,"For the past few years internet of things (IOT) has become of great interest for both academic and industrial research. The term industrial IOT evolved when IT is integrated with industrial automation and control system. The review in this paper provides with the insights into an overview of industrial internet of things (IIOT), evolution of IOT, transformation of IOT to IIOT, architecture of IIOT, different case studies & IIOT in business. This research study aimed to evaluate up to date position of IIOT in present scenario and providing some research recommendation for future researchers.",10.1109/OPTRONIX.2019.8862436,A. Karmakar; N. Dey; T. Baral; M. Chowdhury; M. Rehan
Unstructured Transportation Safety Board Findings Categorization Using the Knowledge Graph Pipeline,2023,-1,Outliers,0.41912740525683245,"In this study, the Transportation Safety Board’s (TSB) Findings data was analyzed to assist Transport Canada Civil Aviation (TCCA) in better informing safety policy decision-making. As the TSB Findings data was unstructured, various methods to categorize and analyze unstructured data were explored in the existing literature. It was found that Knowledge Graphs (KGs), in combination with Deep Learning and Natural Language Processing (NLP) models, such as Neuralcoref and REBEL, were versatile and adaptable to different data needs, which could provide insights into the analysis of the TSB data. This paper first emulated and validated the KG pipeline using the BBC News dataset and then applied the KG pipeline technique to the unstructured TSB Findings Reports data consisting of 4,121 rows, each containing text for an incident or accident. The results showed that the model detected an average of 1.03 entities per row of the data and a total of 5,484 relationships or 1.33 relationships per row. Further, the top-four relationships in the graph database structure obtained from Neo4j accounted for 50% of all relations, though not all relations were found to be valuable. However, a few less-frequent relations were also found to be valuable due to their ability to capture critical components of aviation safety. The results of this data pipeline can be used for further analysis and categorization of TSB’s Findings data to improve aviation safety.",10.1109/BigData59044.2023.10386776,R. Panday; C. -H. Lung
Data Engineering: An Overview from a Future Perspective,2023,1,Topic_1_data_big_big data,0.7654376708335554,"Data engineering is now an essential subject for handling, processing, and analysing big data as the amount of data collected is increasing exponentially. This paper gives a future-focused overview of data engineering. The creation, building, upkeep, and optimization of data architecture, infrastructure, and pipelines are all essential components of data engineering, a field within data science. Data pipelines that can effectively process massive volumes of data from diverse sources and formats must be built using data engineering techniques that are reliable, scalable, and flexible. Data warehousing, data modelling, SQL databases, NoSQL databases, and Apache Hadoop and Spark are just a few examples of the tools and technologies that fall under the broad umbrella of data engineering. Data scientists, analysts, and business users should have rapid access to correct data, which is the main objective of data engineering. In addition to optimizing data storage and performance, data engineering also entails guaranteeing the quality, integrity, and security of the data. Data engineering is an essential phase in the workflow for data scientists since it enables them to carry out advanced analytics, machine learning, and other data-driven operations. In conclusion, data engineering is a vital part of the ecosystem for data science, which offers the tools and infrastructure required for data processing and analysis. Companies may build strong, scalable data pipelines that foster business insights and innovation by utilizing best practices in data engineering. Data engineering is an important field that will continue to grow and transform in the coming years. Data engineers and organizations that can adapt to change and take advantage of emerging technologies and trends will be in the best position to succeed in a data-driven future.",10.1109/IC3I59117.2023.10398128,A. Jain; Sumit; C. Monga; S. Mittal
Survey on Technologies Driving the Smart Energy Sector,2021,-1,Outliers,0.20930614962103106,"The European electricity system undergoes significant changes driven by the EU common rules for the internal market for electricity, as well as by the climate action agenda. The future smart electricity system will build upon a combination of a broad range of new and old technologies together working in novel ways. This paper provides an analysis of novel concepts and technologies for building smart energy management applications from the landscape of technologies from two perspectives: Smart Grids and Energy Efficient Building Operation.",10.1109/TELFOR52709.2021.9653403,J. Stöckl; M. Makoschitz; T. Strasser; L. M. Blanes; V. Janev; P. Lissa; F. Seri
Unlearn Success or Failure Beliefs?: How Do Big Data Analytic Capabilities Affect the Incumbents’ Business Model Innovation in Deep Uncertainty,2024,3,Topic_3_industry_manufacturing_chain,0.3475432565069384,"Research investigating the underlying mechanisms and boundary conditions under which Big Data analytic capabilities (BDACs) influence business model innovation (BMI) in incumbents remains largely underdeveloped. Drawing on the dynamic capabilities view (DCV), we developed a moderated multimediation model in which unlearning success beliefs and unlearning failure beliefs were theorized as the different mechanisms underlining why incumbents are more likely to engage in BMI under the influence of BDACs. We further proposed that deep uncertainty is an important boundary condition that affects such a relationship. Multisource data from a multiwave survey was analyzed using structural equation modeling to test the theoretical framework. The results indicated that BDACs positively affect incumbents’ BMI through not only unlearning success beliefs but also unlearning failure beliefs. Furthermore, the results provided evidence for that deep uncertainty positively moderates the mediation of unlearning success beliefs. Notably, although the moderating effect of deep uncertainty on the mediation of unlearned failure beliefs is negative, it is insignificant. Our study contributes theoretically to the research on BDACs, organizational unlearning, BMI, and DCV, while practical implications are also discussed.",10.1109/TEM.2024.3457874,S. Liao; Z. Xie
Towards a Digital Twin Platform for Industrie 4.0,2021,-1,Outliers,0.16308624648690034,"In an Industrie 4.0 (I4.0), rigid structures and architectures applied in manufacturing and industrial information technologies today, should be replaced by highly dynamic and self-organizing networks. Today's proprietary technical systems lead to strictly defined engineering processes and value chains. Interacting Digital Twins (DT) are considered an enabling technology that could help to increase flexibility based on semantically enriched information. Nevertheless, for interacting digital twins to become a reality, their implementation should be based on existing standards like the Asset Administration Shell (AAS). Additionally, DT Platforms could accelerate development, deployment and ensure resilient operation of DT. This paper presents such a platform based on a microservices architecture and offering solutions for continuous deployment, data infrastructure and I4.0 business services. The platform is evaluated in the use case scenarios platform-based manufacturing and collaborative condition monitoring. As a result, implemented AAS-based mi-croservices organize manufacturing, and submodels of the AAS enable cross-company data sharing for collaborative condition monitoring. Future work should focus on fault-management and service recovery, as well as integration of the AAS into lower platform layers, e.g. for improving data usage control.",10.1109/ICPS49255.2021.9468204,M. Redeker; J. N. Weskamp; B. Rössl; F. Pethig
Connecting the Path Between Open Innovation and Industry 4.0: A Review of the Literature,2024,3,Topic_3_industry_manufacturing_chain,0.7454460057661259,"The role of digitalization in industrial and entrepreneurial processes has acquired more and more attention in the last few years. The technologies and platforms behind the digital economy allow generating a global connection between consumers, employees, enterprises, and industries. Therefore, there is a growing interest in digital knowledge and technological tools in the context of Industry 4.0 (I4.0) from an open innovation (OI) perspective. Indeed, the OI model and related initiatives that refer to external exploitation of knowledge are fully in line with the needs of an integrated digital business model. Although the characteristics of digital technologies suggest OI as a strategic option, due to its characteristic of channeling mutual information flows, the relationship between I4.0 and the OI initiatives is still unclear. This article aims to find evidence from the literature and understand how I4.0 technologies can support OI initiatives. The article investigates the evolution of OI and I4.0 paradigms through a systemic literature review process. Afterwards, we highlight the link between OI and I4.0 by building a taxonomy and a framework of I4.0 technologies supporting OI practices. Finally, we conclude and outline the gaps that emerged from the literature and avenues for future research.",10.1109/TEM.2021.3139457,S. Strazzullo; L. Cricelli; M. Grimaldi; G. Ferruzzi
Technology Issues Which Prevent Implementation of Enterprise Resource Planning Systems in LDCs’ Organizations,2023,3,Topic_3_industry_manufacturing_chain,1.0,"Southeast Asia’s lease developed countries’ poor infrastructure prevents the use of enterprise resource planning (ERP) systems. This study emphasis on several technology issues faced by Myanmar small and medium enterprise (SME) which prevent implementation of ERP system. A modernized cloud-based SaaS version of ERP can bring a lot of benefits to organizations by data integration and best practices. A lot of white paper, publications and textbooks are used as secondary sources of information in this article. Two corporate interviews are conducted to gather primary information in this article to know why Myanmar SMEs discontinue or do not implement ERP. This study found out the technology issues are poor user interface design, difficulties in integrating ERP with existing systems, managers’ lack of other functional knowledge, lack of proper training and poor support in decision making and/or supply chain management etc. This article investigates these difficulties mainly contribute the real inefficiencies and ineffectiveness for the those who uses ERP and who supplies ERP. Predictions had also been made about these issues would still be continued if the LDCs’ infrastructures are not upgraded and if the countries’ culture and organizations’ change management cannot support ERP implementations.",10.1109/InCIT60207.2023.10412894,S. Yu; K. Osathanunkul
From Ad-Hoc Data Analytics to DataOps,2020,1,Topic_1_data_big_big data,0.4170662930499259,"The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.",,A. R. Munappy; D. I. Mattos; J. Bosch; H. H. Olsson; A. Dakkak
AI-Ready Multimodal Data Pipeline to Enrich Cancer Care,2024,-1,Outliers,0.36462373486075456,"This study reports on the progress in designing and developing a framework for integrating heterogeneous datasets—structured, semi-structured, and unstructured—into an AI-ready multimodal data pipeline aimed at predicting radiation therapy interruptions (RTI) and enhancing patient care navigation. The AI-Ready dataset incorporates a broad set of information, including patient demographics, health data, clinical notes, medical imaging, and data on social determinants of health. Preliminary results indicate that this pipeline effectively integrates diverse distributed data sources, providing a foundation for training AI models capable of generating reliable and actionable predictions.",10.1109/BigData62323.2024.10825353,R. Rashid; S. Hashtarkhani; F. A. Kumsa; L. Chinthala; B. M. White; J. A. ZinK; C. L. Brett; R. L. Davis; D. L. Schwartz; A. Shaban-Nejad
An Energy Efficiency Marketplace for Buildings: The ENERGATE System Architecture,2023,1,Topic_1_data_big_big data,0.43918047494732254,"Nowadays it becomes evident that there is a lack of Information Technology systems that decision making as the creation of financing schemes for the building sector. In this context an information system acting as a marketplace enables data driven components that will be exploited by end users to perform bids and offers for building investments. This work focuses on the functional description of the EU-funded project ENERGATE marketplace for energy efficiency investments. ENERGATE System Architecture fosters data sharing, interoperability and the integration of data driven techniques, underlying the design principles to be followed having alignment with existing EU standards and protocols.",10.1109/IISA59645.2023.10345882,P. Kapsalis; A. Papapostolou; K. Touloumis; I. Andreoulaki; Z. Mylona; H. Doukas
Implementation of Big Imaging Data Pipeline Adhering to FAIR Principles for Federated Machine Learning in Oncology,2022,-1,Outliers,0.36202325822071235,"Cancer is a fatal disease and one of the leading causes of death worldwide. The cure rate in cancer treatment remains low; hence, cancer treatment is gradually shifting toward personalized treatment. Artificial intelligence (AI) and radiomics have been recognized as one of the potential areas of research in personalized medicine in oncology. Several researchers have identified the capabilities of AI and radiomics to characterize phenotype and there by predict the outcome of treatment in oncology. Although AI and radiomics have shown promising initial results in diagnosis and treatment in oncology, these technologies are also facing challenges of standardization and scalability. In the last few years, researchers have been trying to develop a research infrastructure for federated machine learning that increases the usability of Big Data for clinical research. These research infrastructures are based on the findable, accessible, interoperable, and reusable (i.e., FAIR) data principles. The India-Dutch “big imaging data approach for oncology in a Netherlands India collaboration” (BIONIC) is a jointly funded initiative by the Dutch Research Council (NWO) and the Indian Ministry of Electronics and Information Technology (MeitY), aiming to introduce radiomic-based research into clinical environments using federated machine learning on geographically dispersed collections of FAIR data. This article described a prototype end-to-end research infrastructure implemented through the BIONIC partnership into a leading cancer care public hospital in India.",10.1109/TRPMS.2021.3113860,A. K. Jha; S. Mithun; U. B. Sherkhane; V. Jaiswar; Z. Shi; P. Kalendralis; C. Kulkarni; M. S. Dinesh; R. Rajamenakshi; G. Sunder; N. Purandare; L. Wee; V. Rangarajan; J. van Soest; A. Dekker
A Novel Architecture for Efficient Fog to Cloud Data Management in Smart Cities,2017,1,Topic_1_data_big_big data,1.0,"Traditional smart city resources management rely on cloud based solutions to provide a centralized and rich set of open data. The advantages of cloud based frameworks are their ubiquity, (almost) unlimited resources capacity, cost efficiency, as well as elasticity. However, accessing data from the cloud implies large network traffic, high data latencies, and higher security risks. Alternatively, fog computing emerges as a promising technology to absorb these inconveniences. The use of devices at the edge provides closer computing facilities, reduces network traffic and latencies, and improves security. We have defined a new framework for data management in the context of smart city through a global fog to cloud management architecture; in this paper we present the data acquisition block. As a first experiment we estimate the network traffic during data collection, and compare it with a traditional real system. We also show the effectiveness of some basic data aggregation techniques in the model, such as redundant data elimination and data compression.",10.1109/ICDCS.2017.202,A. Sinaeepourfard; J. Garcia; X. Masip-Bruin; E. Marin-Tordera
Towards a Data-Centric Research and Development Roadmap for Large-Scale Science User Facilities,2017,1,Topic_1_data_big_big data,0.6757378517882165,"The U.S. Department of Energy (DOE) Office of Science (SC) operates approximately four dozen large-scale science user facilities (SUFs), each of which generates a tremendous amount of scientific data from experiments, observations and computations. To better understand the data needs and challenges, DOE has run many workshops in recent years to identify and articulate data-centric challenges and opportunities at varying resolution, from facility to community scale. Building on those workshop reports, as well as others from elsewhere in the community, this article goes beyond the findings-recommendations typical of workshop reports to consider how one might structure a broad, technology- and data-centric, coordinated research effort that would realize progress towards solutions that address the well documented challenges and opportunities. We focus on identifying practical issues of strategic relevance, along with offering a view about the focal points for a coordinated research and development effort that would target meeting data-centric needs of a broad set of science users and SUFs. These focal points would, by their nature, engage a spectrum of researchers from computer science, computational and experimental sciences, and data science in a coordinated fashion.",10.1109/eScience.2017.72,E. W. Bethel
API-Sociology and Google Global society: confluence of factors for political and economic awareness,2018,1,Topic_1_data_big_big data,0.8780199132858105,"Text Big Data Analytics Study is presented (frequency-morphological analysis of open texts from Google). Visualization of Morphological Matrix, cluster analysis (hierarchical method of dendrograms creation), and Value Chain Map were implemented as the set of tools for analytics. World Economic Forum's Global Risks Report was taken as a model in studying global issues. Global Trends Map was created and is presented; it indicates global social persistent inequality. Our results were compared with Global Risks Report and Multiple Futures Project-2030. Value Chain Map, based on Text Big Data Analytics for eight countries, helped to assess economic situation in terms of competitiveness at the global market.",10.1109/ICAICT.2018.8747036,L. Mazelis; A. Mazelis; N. Shamshurina; Y. Kolesnichenko; D. Yakovleva; O. Kolesnichenko; A. Zykova; I. Grigorevsky
Big Data Processing Verification,2025,1,Topic_1_data_big_big data,0.5032723480775166,"The Subject of the paper is the application of temporal logic to verify data processing processes in big data processing and storage systems, including large industrial systems of the digital economy. The purpose of verification is to identify errors and inconsistencies in the data processing and storage processes at the information processing logic level. In this paper, the authors analyze temporal logics and verification approaches. The choice of linear time logic and the TLA+ tool is justified, and a formalization of the big data processing lifecycle model using the selected mathematical apparatus is proposed. A test bench has been developed that performs automatic specification generation in the TLA+ language. Experimental testing was conducted using the example of a data processing process involving the sharing of SCADA data and data from other enterprise information systems.",10.1109/RusAutoCon65989.2025.11177319,M. Poltavtseva; A. Podorov; D. Zegzhda
Research on Data Security Protection Technology Based on e-commerce Platform,2025,2,Topic_2_data_privacy_security,0.5880611485054742,"With the rapid development of e-commerce, data security has become a core issue in ensuring user trust and sustainable platform development. This paper analyzes the data security risks faced by e-commerce platforms, proposes multi-layered technical protection strategies, and verifies their effectiveness through practical cases, aiming to provide theoretical and practical references for data security protection in the industry.",10.1109/ICSP65755.2025.11086818,L. Jia; Y. Yang; B. Huang
An Approach to Evaluate Load Balancing and Crucial Data Analysis Through Hadoop Framework,2023,1,Topic_1_data_big_big data,1.0,"The tools of big-data analysis like the Hadoop framework, MapReduce, Hive, HBase, Spark, and Pig programming play a vital role in data processing and synchronization that help to find insights from huge amounts of data generated from different resources, including social media, the health sector, the finance sector, education, research and innovation wings, and many more public platforms. These platforms are responsible for rapidly generating huge amounts of data that would be impossible to handle by any traditional method of data analysis. The paper’s objective is to describe the importance of critical data analysis in big data and also specify how the load balancing factor of cloud computing will perform the comparisons between different factors of offloading, based on different parameters such as execution time of jobs, task offloading in the number of virtual machines, and energy consumption by each process to be executed.",10.1109/OTCON56053.2023.10114022,P. K. Shriwas; S. Kuraiya; P. Diwan; V. Kumar; B. Dewangan
UASDAC: An Unsupervised Adaptive Scalable DDoS Attack Classification in Large-Scale IoT Network Under Concept Drift,2024,1,Topic_1_data_big_big data,0.6051108100511255,"Day by day, the number of devices in IoT networks is increasing, and concurrently, the size of botnets in IoT networks is also expanding. Currently, attackers prefer IoT-based botnets to launch DDoS attacks, as IoT devices offer a vast attack surface. Many researchers have proposed machine and deep learning-based classifiers to classify DDoS and benign network traffic in online streams from IoT devices. However, the performance of the traditional machine and deep learning algorithms deteriorates when sudden concept or data drift occurs in the online streams and the volume and velocity of IoT network traffic increases. To address these challenges, we propose UASDAC, an adaptive and scalable data pipeline designed specifically to handle concept drift and detect DDoS traffic in real-time in massive online streams originating from IoT devices. UASDAC incorporates three key components: an online network stream collector for data collection, an online network stream analyzer with an unsupervised drift detector for detecting drift and DDoS traffic, and an online network stream repository for storing streams for future analytics. UASDAC leverages big data technologies to implement all the three components to achieve scalability. Additionally, UASDAC introduces an effective and efficient retraining technique to adapt to novel patterns in online streams in the presence of concept drift. We evaluated the performance of UASDAC in different concept drift scenarios using the benchmark dataset NSL-KDD and the latest IoT dataset IoT23. Our results demonstrate that UASDAC effectively identifies DDoS traffic in the presence of concept drift, achieving an accuracy range of 99.7% to 99.9%.",10.1109/ACCESS.2024.3397512,S. Selvam; U. Maheswari Balasubramanian
On the Appropriate Methodologies for Data Science Projects,2021,-1,Outliers,0.155095420756154,"Data science is an emerging discipline with a particular research focus on improving the available techniques for data analysis. While the number of data science projects is growing, unfortunately, there is a slight consideration of how a team performs a data science project. Although the existence of a repeatable well-defined process could deal with many challenges of data science projects, researches conducted in recent years indicate a convergence of the results to agile methodologies as the appropriate ones for the projects. In this paper, first, the tasks and roles of individuals in data science projects are addressed; then, some research conducted for the methodologies used in the projects are studied. The study shows that agile methodologies could resolve many issues of data science projects by increasing the communications and cooperation of the team members and investors.",10.1109/ICIT52682.2021.9491712,A. Karimi Dastgerdi; T. Javdani Gandomani
Maturity Assessment Model for Industrial Data Pipelines,2023,-1,Outliers,0.2796684874354568,"Data pipelines can be defined as a complex chain of interconnected activities that starts with a data source and ends in a data sink. They can process data in multiple formats from various data sources with minimal human intervention, speed up data life cycle operations, and enhance productivity in data-driven organizations. As a result, companies place a high value on strengthening the maturity of their data pipelines. The available literature, on the other hand, is significantly insufficient in terms of providing a comprehensive roadmap to guide companies in assessing the maturity of their data pipelines. Therefore, this case study focuses on developing a data pipeline maturity assessment model that can evaluate the maturity of data pipelines in a staged manner from maturity level 1 to maturity level 5. We conducted empirical research in order to develop the maturity assessment model on the basis of five different determinants to address the specific needs of each data pipeline maturity level. Accordingly, it aims to support organizations in assessing their current data pipeline maturity, determining challenges at each stage, and preparing an extensive roadmap and suggestions for data pipeline maturity improvement. In future work, we plan to employ the maturity model in different companies as a case study to evaluate its applicability and usefulness.",10.1109/APSEC60848.2023.00062,A. R. M; J. Bosch; H. H. Olsson
Research on Data Acquisition and Processing Under the Coordination of Multiple Value Chains in the Manufacturing Industry—Taking the Electric Vehicle Manufacturing Industry as an Example,2021,3,Topic_3_industry_manufacturing_chain,0.3789772081011865,"With the rapid development of economic globalization and the continuous changes of digital technology and the Internet, the electric vehicle manufacturing industry has become an effective way to improve its own competitive advantage by strengthening the collaborative management of many industrial chains and value chains, while the construction of data space is to increase the company’s An effective tool for chain collaboration. Based on this, this paper first summarizes the characteristics of the data sources of the electric vehicle manufacturing industry from three aspects: “large quantity”, “multiple categories”, and “rapid change”. Secondly, based on the characteristics of the data sources of electric vehicle manufacturers, the theory of data acquisition and processing is explained, and big data methods are proposed to solve the problem of data acquisition in the electric vehicle manufacturing industry. Then, in the context of the rapid growth of the data volume of electric vehicle manufacturers and the increasingly complex data types, the main problems of using big data to obtain data from electric vehicle manufacturers are analyzed. Finally, the countermeasures for data acquisition methods of electric vehicle manufacturers in the future are proposed.",10.1109/CCIS53392.2021.9754677,S. Geng; Y. Liu; D. Niu; X. Guo
Database Integrated Analytics Using R: Initial Experiences with SQL-Server + R,2016,1,Topic_1_data_big_big data,0.9316174238960276,"Most data scientists use nowadays functional or semi-functional languages like SQL, Scala or R to treat data, obtained directly from databases. Such process requires to fetch data, process it, then store again, and such process tends to be done outside the DB, in often complex data-flows. Recently, database service providers have decided to integrate ""R-as-a-Service"" in their DB solutions. The analytics engine is called directly from the SQL query tree, and results are returned as part of the same query. Here we show a first taste of such technology by testing the portability of our ALOJA-ML analytics framework, coded in R, to Microsoft SQL-Server 2016, one of the SQL+R solutions released recently. In this work we discuss some data-flow schemes for porting a local DB + analytics engine architecture towards Big Data, focusing specially on the new DB Integrated Analytics approach, and commenting the first experiences in usability and performance obtained from such new services and capabilities.",10.1109/ICDMW.2016.0009,J. L. Berral; N. Poggi
CIGO! Mobility management platform for growing efficient and balanced smart city ecosystem,2016,1,Topic_1_data_big_big data,0.7624926874217461,"The massive amount of tourists, citizens and traffic in big cities usually collapse busy areas causing transport inefficiency, unbalanced economic growth, crime, and nuisance among citizens and visitors. Therefore, the Smart City strategies such as Smart Mobility and Smart Governance naturally arise as means to improve mobility in urban areas. In this paper we propose a novel mobility management platform and business model that can attract numerous actors and still be orchestrated by the city government. The proposed platform integrates mobility data from various sources such as Open Data, mobile applications, sensors and government data, allowing for its visualisation and analysis while making it actionable through associated third party mobile applications. We propose to inject the city mobility policies to the third party mobile applications which provide services related to the city resources. In this way we form a value chain which connects different actors (city governments, mobile application providers, POI owners, companies that require logistics in cities, and final users) who both take a part in improving the mobility in urban areas, and benefit from the way mobility policies being executed. In this paper we discuss the business model and logical architecture of the proposed platform which has been already deployed in the city of Barcelona.",10.1109/ISC2.2016.7580750,P. Mrazovic; I. De La Rubia; J. Urmeneta; C. Balufo; R. Tapias; M. Matskin; J. L. Larriba-Pey
Using process models to support customer specific production processes,2016,3,Topic_3_industry_manufacturing_chain,0.9300310114294786,Todays production processes are becoming more and more flexible. Customers frequently demand specific production steps that fit to their processes. This leads to a needed flexibility for systems that are used to support decision making processes and the production itself. In this paper an approach is presented that uses process definition coming from an integrated enterprise modelling for connecting information systems and production systems modules. An architecture is presented that uses these models for each process step to provide the needed information for decision makers.,10.1109/ETFA.2016.7733645,A. Dennert; M. Wollschlaeger; P. Gering; T. Knothe; A. Lemcke
Sensing Terminal for Big Data Oil and Gas Pipeline Corrosion Monitoring,2020,1,Topic_1_data_big_big data,0.6561458776922517,"Facing the construction requirements of big data pipelines, a distributed sensor terminal for multi-parameter monitoring of oil and gas pipeline anti-corrosion was designed. The terminal uses the microprocessor S3C2440A as the core, and several sensors form a multi-parameter sensor array monitoring circuit, and realizes the data transmission of the Internet of Things through the 4G DTU communication module and TCP protocol. The device can sense the temperature value, reference potential value, programmable power supply current and voltage output value of the long-distance pipeline in real time, and realize the data interaction with the cloud server. Tests show that the device is practical in realizing the application of big data pipeline monitoring value.",10.1109/ICCNEA50255.2020.00064,L. Li; Y. Wang
Building Relationship between IoT Devices and Consumer,2024,3,Topic_3_industry_manufacturing_chain,1.0,"An analysis of establishing a relationship between IOT devices and consumers acting as a bridge for the development company was tested in this study. This research focuses on how the Internet of Things (IoT) and Industrial Internet of Things (IoT) are bringing changes to the value chain and impacting industrial automation organizations to create a connection smart and sustainable using technologies such as IPv6, Cloud Computing, Fog, Computer Science, Big Data and Big Data Analytics. The main objective is to know how these responsible organizations develop business plans for EO to save time, money and support smart business efforts for quality control, green practices and sustainability, supply chain traceability and overall supply chain efficiency. This empirical study is exploratory and takes a qualitative approach with two data sources: semi-structured interviews providing primary data, mainly the opinions of managers in different positions in the organizations surveyed. To gather employee opinions, it is very important to determine how the organization engages its employees in implementing the OT mission, vision and goals through various programs/projects with status as one of the party's main stakeholders.",10.1109/ICONSTEM60960.2024.10568593,R. Akila; R. Sasikala
Evaluation of technological development for the definition of Industries 4.0,2018,3,Topic_3_industry_manufacturing_chain,1.0,"The impact on the industry generated by the new ICTs stimulates the detection of failures, the improvement of processes, and the acceleration in production times, all variables that significantly alter the levels of productivity in the different industrial sectors where they are implemented.Although nowadays companies use different types of technologies in their administration, marketing or finance processes, among other areas, the insertion of technologies in the automation and control of production processes in the manufacturing industry is generating a new industrial revolution that changes the paradigm of process control by collecting probabilistic samples to work with the totality of the data, managing to store and analyze large volumes of data in real time, impacting on the optimization of production processes.Technologies that permit data capture by sensors with the insertion of IoT, the processing of large volumes of data with Big Data techniques, as well as the integration of complete production information with areas of purchase, sale or logistics allows a comprehensive management of the companies in what is known as the industry 4.0.The present article introduces a taxonomy that identifies which ICTs are specifically implemented in the industry at present, according to the functions that they fulfill in the organizational structure and which are the technologies that are being developed that directly impact on productivity levels and would allow to transform a company into an industry 4.0.",10.1109/CACIDI.2018.8584187,A. Mon; H. R. Del Giorgio; E. De María; M. Querel; C. Figuerola
Design of Automation Systems in Industrial Production under Artificial Intelligence Technology,2022,3,Topic_3_industry_manufacturing_chain,0.8001067035976627,"Among all new generation of information technologies that are rapidly evolving, including mobile broadband internet, big data and cloud computing, artificial intelligence (AI) technology has emerged as the highlight. As the world is at the critical juncture of transforming from a market-driven development model to an innovation driven one, this paper explores the function of AI in intelligent manufacturing, characteristics and edges of applying AI in industrial production and how AI can be applied to designing automation systems in industrial production with assistance of specific technologies. In other words, this paper discovers the connotation, characteristics and edges of applying AI to industrial production. It finds that industrial production is advancing towards “digitalization”, “networking” and “intelligence”. In this regard, application of AI in industrial production not only has technological basis like sensing technology and modeling technology, but also can optimize the design of automation systems in industrial production, creating a more productive intelligent manufacturing system.",10.1109/IPEC54454.2022.9777306,J. Zeng
End-to-End Data Quality Assessment Using Trust for Data Shared IoT Deployments,2022,-1,Outliers,0.3662794859232214,"Continued development of communication technologies has led to widespread Internet-of-Things (IoT) integration into various domains, including health, manufacturing, automotive, and precision agriculture. This has further led to the increased sharing of data among such domains to foster innovation. Most of these IoT deployments, however, are based on heterogeneous, pervasive sensors, which can lead to quality issues in the recorded data. This can lead to sharing of inaccurate or inconsistent data. There is a significant need to assess the quality of the collected data, should it be shared with multiple application domains, as inconsistencies in the data could have financial or health ramifications. This article builds on the recent research on trust metrics and presents a framework to integrate such metrics into the IoT data cycle for real-time data quality assessment. Critically, this article adopts a mechanism to facilitate end-user parameterization of a trust metric tailoring its use in the framework. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd-sourced or other unreliable data collection techniques such as that in IoT. The article further discusses how the trust-based framework eliminates the requirement for a gold standard and provides visibility into data quality assessment throughout the big data model. To qualify the use of trust as a measure of quality, an experiment is conducted using data collected from an IoT deployment of sensors to measure air quality in which low-cost sensors were colocated with a gold standard reference sensor. The calculated trust metric is compared with two well-understood metrics for data quality, root mean square error (RMSE), and mean absolute error (MAE). A strong correlation between the trust metric and the comparison metrics shows that trust may be used as an indicative quality metric for data quality. The metric incorporates the additional benefit of its ability for use in low context scenarios, as opposed to RMSE and MAE, which require a reference for comparison.",10.1109/JSEN.2022.3203853,J. Byabazaire; G. M. P. O’Hare; D. T. Delaney
On a Pipeline-Based Architecture for Parallel Visualization of Large-Scale Scientific Data,2016,1,Topic_1_data_big_big data,0.5682184293036507,"Many extreme-scale scientific applications generate colossal amounts of data that require a large number of processors for parallel visualization. Among the three well-known visualization schemes, i.e. sort-first/middle/last, sort-last, which is comprised of two stages, i.e. image rendering and composition, is often preferred due to its adaptability to load balance. We propose a very-high-speed pipeline-based architecture for parallel sort-last visualization of big data by developing and integrating three component techniques: i) a fully parallelized per-ray integration method that significantly reduces the number of iterations required for image rendering, ii) a real-time over operator that not only eliminates the restriction of pre-sorting and order-dependency, but also facilitates a high degree of parallelization for image composition, and iii) a novel sort-last visualization pipeline that overlaps rendering and composition to completely avoid waiting time between these two stages. The performance superiority of the proposed parallel visualization architecture is evaluated through rigorous theoretical analyses and further verified by extensive experimental results from the visualization of various real-life scientific datasets on a high-performance visualization cluster.",10.1109/ICPPW.2016.28,D. Chu; C. Q. Wu
"Ultron-AutoML: an open-source, distributed, scalable framework for efficient hyper-parameter optimization",2020,1,Topic_1_data_big_big data,0.5264757454030706,"We present Ultron-AutoML, an open-source, distributed framework for efficient hyper-parameter optimization (HPO) of ML models. Considering that hyper-parameter optimization is compute intensive and time-consuming, the framework has been designed for reliability - the ability to successfully complete an HPO Job in a multi-tenant, failure prone environment, as well as efficiency - completing the job with minimum compute cost and wall-clock time. From a user's perspective, the framework emphasizes ease of use and customizability. The user can declaratively specify and execute an HPO Job, while ancillary tasks - containerizing and running the user's scripts, model checkpointing, monitoring progress, parallelization - are handled by the framework. At the same time, the user has complete flexibility in composing the code-base for specifying the ML model training algorithm as well as, optionally, any custom HPO algorithm. The framework supports the creation of data-pipelines to stream batches of shuffled and augmented data from a distributed file system. This comes in handy for training Deep Learning models based on self-supervised, semi-supervised or representation learning algorithms over large training datasets. We demonstrate the framework's reliability and efficiency by running a BERT pre-training job over a large training corpus using pre-emptible GPU compute targets. Despite the inherent unreliability of the underlying compute nodes, the framework is able to complete such long running jobs at 30% of the cost with a marginal increase in wall-clock time. The framework also comes with a service to monitor jobs and ensures reproducibility of any result.",10.1109/BigData50022.2020.9378071,S. Narayan; C. S. Krishna; V. Mishra; A. Rai; H. Rai; C. Bharti; G. S. Sodhi; A. Gupta; N. Singh
Exploration of Workflow Management Systems Emerging Features from Users Perspectives,2019,1,Topic_1_data_big_big data,0.5272321995577051,"There has been a recent emergence of new workflow applications focused on data analytics and machine learning. This emergence has precipitated a change in the workflow management landscape, causing the development of new dataoriented workflow management systems (WMSs) in addition to the earlier standard of task-oriented WMSs. In this paper, we summarize three general workflow use-cases and explore the unique requirements of each use-case in order to understand how WMSs from both workflow management models meet the requirements of each workflow use-case from the user’s perspective. We analyze the applicability of the two models by carefully describing each model and by providing an examination of the different variations of WMSs that fall under the task driven model. To illustrate the strengths and weaknesses of each workflow management model, we summarize the key features of four production-ready WMSs: Pegasus, Makeflow, Apache Airflow, and Pachyderm. To deepen our analysis of the four WMSs examined in this paper,we implement three real-world use-cases to highlight the specifications and features of each WMS. We present our final assessment of each WMS after considering the following factors: usability, performance, ease of deployment, and relevance. The purpose of this work is to offer insights from the user’s perspective into the research challenges that WMSs currently face due to the evolving workflow landscape.",10.1109/BigData47090.2019.9005494,R. Mitchell; L. Pottier; S. Jacobs; R. F. d. Silva; M. Rynge; K. Vahi; E. Deelman
BigDL 2.0: Seamless Scaling of AI Pipelines from Laptops to Distributed Cluster,2022,1,Topic_1_data_big_big data,0.8861117257087806,"Most AI projects start with a Python notebook running on a single laptop; however, one usually needs to go through a mountain of pains to scale it to handle larger dataset (for both experimentation and production deployment). These usually entail many manual and error-prone steps for the data scientists to fully take advantage of the available hardware resources (e.g., SIMD instructions, multi-processing, quantization, memory allocation optimization, data partitioning, distributed computing, etc.). To address this challenge, we have open sourced BigDL 2.0 at https://github.com/intel-analytics/BigDL/ under Apache 2.0 license (combining the original BigDL [19] and Analytics Zoo [18] projects); using BigDL 2.0, users can simply build conventional Python notebooks on their laptops (with possible AutoML support), which can then be transparently accelerated on a single node (with up-to 9.6x speedup in our experiments), and seamlessly scaled out to a large cluster (across several hundreds servers in real-world use cases). BigDL 2.0 has already been adopted by many real-world users (such as Mastercard, Burger King, Inspur, etc.) in production.",10.1109/CVPR52688.2022.02076,J. J. Dai; D. Ding; D. Shi; S. Huang; J. Wang; X. Qiu; K. Huang; G. Song; Y. Wang; Q. Gong; J. Song; S. Yu; L. Zheng; Y. Chen; J. Deng; G. Song
Pipeline Based Big Data Sketching,2021,1,Topic_1_data_big_big data,0.7330089663619027,"Big Data pipeline is an architecture for big data applications where all the sub-modules are composed in a sequential manner. In this architecture, output of one stage forwarded as an input to the next stage of the pipeline. Although, each stage of the pipeline has relevant information, but the data present at intermediate stages is not sufficient to predict inference due to uncertainty present at that stage. Therefore, a probabilistic inference based method is proposed for making strong local inference. For sketching probabilistic inference of Big Data pipeline, this work contributes probabilistic pipeline sketching of inference methods using variable elimination and bucket elimination inference.",10.1109/ISCON52037.2021.9702516,A. Pal; A. Chowdhury; S. Singh; M. Kumar
Towards Data Interoperability: Turning Domain Specific Knowledge to Agnostic across the Data Lifecycle,2016,1,Topic_1_data_big_big data,0.7920094203874167,"Today's rich digital environment is characterised by the exponential increase of the amount of ""born digital"" data, following the penetration of real (e.g. sensors, wearable devices, etc.) and virtual (e.g. online platforms, user generated content, etc.) Internet connected sources. While the majority of the research outcomes is focused on the processing and connectivity aspects, a key question relates to data utilization in cross-application cases: If everything is connected and identified uniquely, how is it possible to use the data from a sensor of an unidentified domain in a different application domain? In this paper an approach enabling the interconnection and utilization of domain specific data to a considerably different domain is presented. A functional scenario of that approach is proposed, trying to address the challenges and the needs emerging in the Internet of Things era, from a view point of increasing data utilization out of a specific's data lifecycle. Semantic data interoperability, integration, annotation, management, discovery, and analysis are issues being considered in the proposed approach.",10.1109/WAINA.2016.69,A. Kiourtis; A. Mavrogiorgou; D. Kyriazis; I. Maglogiannis; M. Themistocleous
Anovos: A Scalable Feature Engineering Library,2022,1,Topic_1_data_big_big data,0.6136443944128074,"In the current era of big data, the amount of data a company can acquire is growing exponentially. However, the data are only meaningful if they are used wisely. This paper introduces Anovos, an open-source library built on top of Apache Spark. It is designed to perform efficient, end-to-end feature engineering at scale (with TBs of Data), and helps implement a systematic and procedural data pipeline with enterprise data at one end and model-ready features at the other. Besides improving the current exploratory data analysis process, we have also introduced a few key innovations in Anovos: the concept of data stability index, a single-metric indication of the stability of an independent variable in a longitudinal way, as well as Feature Explorer and Feature Mapper, powered by semantic similarity-based AI models, in order to solve the cold-start problem of building high-quality predictive features for the model training process.",10.1109/BigData55660.2022.10020731,A. Datta; K. Sangaralingam; S. Chen; D. Tran; S. Sen; R. Ranjan
State of the art of technology in the food sector value chain towards the IoT,2016,-1,Outliers,0.4154547703253237,"The food sector is challenged to provide safe and qualitative food to consumers at affordable price and to feed appropriately increasing population using natural resources, like soil and water, in a sustainable way. Consumers awareness about food origin, nutritional and wellness properties, attention to processed meals ingredients, due to health issues, and requests of new customized portions formats and receipts, related to habits changes, are also demanding trends in the sector. Several technologies can help to address those responsibilities of efficient, safe and environmental respectful production, and strict communication and connection with the consumers. This paper provides a state of the art of smart and other emerging technologies framed in the whole food supply-chain, to create a picture of the added value that the technology can bring to the sector. Moreover, the evolutions towards the Internet of Things (IoT) paradigm adoption are presented.",10.1109/RTSI.2016.7740612,L. Ramundo; M. Taisch; S. Terzi
Smart Services Maturity Level in Germany,2018,3,Topic_3_industry_manufacturing_chain,0.6901836548033985,"Digitization of business processes is changing the industry. Enterprises using data as efficiently and smart as possible, can often create a clear competitive advantage and be more responsive to their customer needs. Through the smart use of data, new business models can be created. The most well-known changes of digitization are taking place in the B2C market. The B2B market is affected by this change too. Enterprises recognize the ongoing shift to a service-oriented society and therefore should be able to react directly to the customer needs. Service based business models, such as implementing Smart Services, are developed and intergarted in existing enterprise offerings. This paper reports upon a study of the maturity level regarding Smart Services of three selected German enterprises. The aim is to highlight the the impact of the ongoing digital transformation and map the maturity levels with regards to digitization and Smart Services. The case studies offer a first assessment about the current situation of Smart Services, using sub-categories to pinpoint developments, in manufacturing enterprises. The analysis shows that the maturity levels of these enterprises depend on their technology management, financial resources and corporate culture. The three cases have a high maturity level and already implemented Smart Services in their business model strategy. Nevertheless, there is still potential for improvement, especially regarding `Simultaneous Engineering' and `Automation Level'. It can be stated that within these enterprises financial resources, critical awareness as well as know-how of digitization and Smart Services are available. The cases were organizationally rather set in rigid structures. In order to reach an even higher degree of maturity and thus to remain competitive in the future, the enterprises should fundamentally rethink their corporate culture. These case studies may be considered as a pilot study into this topic, with refinements and improvements to further studies possible.",10.1109/ICE.2018.8436329,F. Kaltenbach; P. Marber; C. Gosemann; T. Bölts; A. Kühn
Towards Edge-Cloud Computing,2018,1,Topic_1_data_big_big data,1.0,"In this paper, we propose edge-cloud computing as a new framework for organizing the data pipeline functions, i.e., acquisition, processing and analytics, in the Internet of Things systems. Edge-cloud computing consists of three layers, namely smart edge devices, fog computing layer and cloud computing layer. Being an intrinsically hierarchical framework, edge-cloud computing is able to tackle the complexities of Internet of Things systems more fundamentally. In particular, we discuss the main roles of orchestration and intermediary that fog computing undertakes for microservices and cloud services, and the fundamental underpinning function - automated service provisioning in fog computing.",10.1109/BigData.2018.8622052,H. Tianfield
End-to-End Circular Economy in Onion Farming with the Application of Artificial Intelligence and Internet of Things,2022,3,Topic_3_industry_manufacturing_chain,0.391134619966448,"The agricultural sector in Indonesia plays an essential role in economic development and food security in Indonesia, with an average contribution of 13.5% to Gross Domestic Product (GDP). One of the strategic agricultural commodities in daily needs is onions. Agriculture in Indonesia has not been able to produce onions that allow the fulfillment of fluctuations in people's consumption needs. Onion production is sometimes abundant, so it is wasted and makes prices fall. On the other hand, there are times when the availability of onions in the community is deficient, so the price becomes very high. It is possible to solve this by building a system that can integrate all the nodes involved in supply chain management (SCM) and customer relationship management (CRM). Furthermore, this will create a balance between supply and demand, and revive the various micro, small and medium enterprises (MSME) involved in the value chain. The demand is quite significant, forcing Indonesia to still require imports of several agricultural commodities the community needs in their daily lives. However, this is one of the factors related to competitiveness with local farmers' yields and unstable onion prices. This is an early study about on how to model the optimization end-to-end value chain of onions. Optimizing the agricultural cycle in Indonesia, such as optimizing agricultural land, harvest cycles, supply and demand for crops, storage and delivery of crops, and determining commodity prices can improve food security in Indonesia.",10.1109/iSemantic55962.2022.9920447,P. N. Andono; F. Ocky Saputra; G. F. Shidik; Z. Arifin Hasibuan
Towards the Development of Resilient Manufacturing Systems: A Systematic Literature Review,2024,3,Topic_3_industry_manufacturing_chain,0.8255175183895792,"In this era of global competitiveness, volatility, and uncertainty there is an increasing quest for the development of resilient manufacturing systems that are resistant to disruptions or that are quick to recover from disruptions. Resilient manufacturing systems are crucial, and this was accentuated during the covid-19 pandemic and the war between Russia and Ukraine, where global value chains were severely disrupted. These events exposed the loopholes in the manufacturing system and presented a gap to improve the manufacturing systems. This study reviewed some existing solutions geared towards achieving resilient manufacturing. A systematic literature review method was used to gather information on the state of the art and the requirements necessary for the development of resilient manufacturing systems. The reviewed literature inclines to the use of big data analytics, machine learning and artificial intelligence as viable options to achieving resilient manufacturing. This study also highlights different resilient manufacturing frameworks and approaches that manufacturing industries can adopt or adjust based on their unique requirements. Future studies can consider the application of resilient manufacturing strategies to achieve a robust manufacturing system using a specific case study.",10.1109/SEB4SDG60871.2024.10630447,Y. Magodi; I. Daniyan; K. Mpofu; F. Ale
IoT for Rail Transportation: The Case of Railigent,2022,3,Topic_3_industry_manufacturing_chain,0.8924381369483462,"Internet of Things (IoT) platforms are often designed horizontally to serve requirements of various vertical industries, but some are custom-built and implemented only by the market participants in a specific industry vertical. This paper takes a case study approach to investigate the IoT platform Railigent, which facilitates IoT solutions exclusively for the rail transportation ecosystem. Offered by Siemens Mobility GmbH, the platform's architecture, business model and platform ecosystem management are analyzed based on an archival analysis of publicly available material. The case findings emphasize the usefulness of IoT platforms in practice to enable data-driven maintenance optimization, potentially supported by a platform verticalization strategy.",10.1109/BigData55660.2022.10020620,S. F. Dietlmeier; R. J. Floetgen; J. Bock; F. Urmetzer
AI Strategic Development for FQASC,2025,3,Topic_3_industry_manufacturing_chain,0.3580873991532346,"They used the examples of types of artificial intelligence, integration of artificial intelligence in the food value and supply chain, other types of artificial intelligence-embedded technologies, barriers for artificial intelligence implementation in the food value and supply chain and solutions for barriers. Provide a summary of its analysis and the horizontal and vertical integration of artificial intelligence into the entire food supply and value chain. Artificial intelligence is increasing its role in this field, and advanced technologies such as robotics, drones, smart and autonomous machines are already transforming different stages of the chain. As a general guidance from this exploration of systematics literatures, the available studies suggest that artificial intelligence does not stand alone but interacts with other technical advancements such as big data, machine learning, service institution, agribots, industrial robots, sensors and drones, digital platforms, driverless movement implements, machinery and nanotechnology, thus providing different capabilities for different processes. But there is a social, technological and economic hurdle to the application of artificial intelligence. All of this can cross the bridge by effectively educating farmers in financial and digital terms and through generating best practices by the components of the food supply chain and value chain. This discussion will highlight different artificial intelligence roles across the different segments of the food value and supply chain. Its - through the means of precision agriculture- the agriculture segment and further adds and optimizes the fertile use of several raw materials which in turn would increase - these outputs. Another segment which is being helped from AI technologies is food processing which is converting raw food material into edible food and taking care of quality control to sustain edible status: the departments are basically setup in manufacturing; the technology is getting adapted widely to ensure which the obtainable attribute quality parameters are monitored and rectified which has resulted in this automation of the imaging, the analysis and the indexing. With every green product shed in an efficient distribution route, transactions are produced, and for supply chain leaders, it makes matters on logistics easier for them through route optimization and predictive analytics, which assists in real-time stock management, optimizing cost considerations accordingly, improving operability, and reducing damage to surrounding ecosystem as a result. Another way, AI is enhancing customer experience is by ensuring a personalized experience for customers through big-data insights.",10.1109/IC363308.2025.10956927,L. Hussein; V. Y. Bharadwaj; P. R. Kiran; H. R; V. A. D; D. Aggarwal
The Study on the Motivation of T2O E-Commerce Model's Development,2015,3,Topic_3_industry_manufacturing_chain,0.6784856338283943,"T2O is one of the e-commerce business modes which developed from the context of media convergence. The paper explained the essence and motivation of T2O through the research methods of qualitative research, logical inference, case analysis and so on. Firstly, the paper introduced the concept and mechanism of the T2O. Then, it analyzed the differences between the traditional e-commerce modes and T2O, and pointed the uniqueness of T2O. Finally, it launched the motivation of T2O's development from seven perspective, that are the policy-driven, internet thinking, technology-driven, industrial convergence, market demand, business integration and value chain integration.",10.1109/DCABES.2015.37,X. Shuo; Y. Yi
Harnessing Insights from Streams: Unlocking Real-Time Data Flow with Docker and Cassandra in the Apache Ecosystem,2024,1,Topic_1_data_big_big data,1.0,"Real-time data streaming pipelines are immensely valuable in today's data-driven world since they enable continuous data processing and analytics. This research paper provides a comprehensive exploration of the architecture, development, and deployment of an advanced real-time data streaming pipeline. It utilizes Docker for containerization, Apache Kafka for distributed streaming, Apache Spark for dynamic data transformation, and Cassandra for efficient NoSQL storage. The study outlines the intricacies of integrating these technologies, examining the pipeline's components, functionalities, performance metrics, and potential applications. Through this case study, the paper show-cases the efficacy of open-source tools in constructing highly scalable and resilient data streaming pipelines.",10.1109/RAICS61201.2024.10690115,J. Oza; A. Patil; C. Maniyath; R. More; G. Kambli; A. Maity
Location Data Analytics in the Business Value Chain: A Systematic Literature Review,2020,1,Topic_1_data_big_big data,0.5193409226548139,"Context information has become a significant asset to optimize the value obtained from information systems. Location is an important type of context information that refers to the place in which an event occurs. In business environments, the implementation of location-based analytics systems to aid decision making processes is of paramount importance for business development. However, after an exhaustive literature review, we found that researchers and practitioners still lack a comprehensive characterization of location-based data analytics systems that have been effectively applied to business processes. This paper presents the results of a systematic literature review (SLR), in which we characterized a total of 168 location-based and business oriented analytics solutions that were published between 2014 and 2019. To conduct this SLR we defined three characterization dimensions: business aspects, through which we identified value chain business processes or activities that may be benefited with the proposed solution; data source, which allowed us to report on the data used in each of the studies; and data analytics, through which we report on the analytics techniques and validation strategies implemented by the studied approaches. The contribution of our SLR is twofold. First, it provides business and data analytics practitioners with a comprehensive catalog of location-based data analytics approaches that could be applied to improve value generation, at different levels, along their businesses' value chains. And second, it provides researchers with a complete landscape of recent advancements and open challenges in the field.",10.1109/ACCESS.2020.3036835,L. E. Ferro-Díez; N. M. Villegas; J. Díaz-cely
The H2020-ECSEL Project “iRel40” (Intelligent Reliability 4.0),2021,3,Topic_3_industry_manufacturing_chain,0.9131294521821194,"Building on many discoveries and inventions, electronics started affecting people’s everyday lives in a significant fashion following the invention of the first solid state transistor in late 1940s. The miniaturization paved way for the mass electronics production and later the digital revolution, the outcomes of which are visible to all members of the public today. After about a two-decade-long swing around 2000s from hardware towards software regarding what affects lives more, a point has now been reached where electronics is more important to all and its use is more ubiquitous and crucial than ever before. In most if not all of end user or industrial applications, the capability and quality of electronics hardware are the key determining factors.The European electronics components and systems (ECS) industry has traditionally had a high base line for electronics innovation. However, the industry is now compelled, partly due to competition and partly due customer demand, to manufacture even more reliable electronics products than before. Guaranteeing the reliability of electronics hardware entails the entire ECS value chain to undergo a paradigm shift to holistically address reliability as a key issue. The European ECS industry previously adopted overseas outsourcing considerably, however it is now taking steps to reshape itself into a more coherent value chain with the aim of having not only the electronics designs but also the electronics manufacturing made in Europe.H2020-ECSEL programme successfully funds highly competitive projects in the area of electronics components and systems. We present here a prologue to a similarly funded project entitled Intelligent Reliability 4.0 (""iRel40""), by providing a background to the topic of ECS, project objectives, and the methodologies and implementations we plan to undertake during the 36-month period of this ongoing project.",10.1109/DSD53832.2021.00054,K. Pressel; J. Moser; S. Rzepka; K. Brinkfeldt; S. Zhao; W. van Driel; P. Giammatteo; B. Bulut; M. Soyturk; L. Pomante
IoT Integration in Agricultural Infrastructure: From Fields to Clouds,2023,-1,Outliers,0.4081973524658776,"The IoT integration with agricultural infrastructure marked the beginning in a new era of smart farming, changing conventional practices into data-driven, effective, and sustainable operations. This study examines the enormous effects of IoT on agriculture, minimizing the gap between the fields and the clouds. IoT goes beyond isolated farms, as well. By incorporating IoT throughout the whole agricultural value chain, supply chain transparency, traceability, and food safety are all improved. Sharing data and insights amongst farmers, academics, and stakeholders promotes collaboration and environmentally friendly practices. The article covers a wide range of technological components related to IoT in agriculture. It describes the main elements of smart farming based on IoT. It concentrates mainly on the current issues that farmers are facing and their solutions in the realm of agriculture. IoT is imperative for smart agriculture. Because monitoring environmental conditions is essential to improving the output of productive crops. This study explores the multiple function of IoT integration in agricultural infrastructure, emphasizing the revolutionary transition from the fields where data is gathered to the clouds where insights are cultivated. It clarifies the possibilities, difficulties, and opportunities of the Internet of Things' significant influence on contemporary agriculture via case studies, analysis, and conversations.",10.1109/RMKMATE59243.2023.10369026,R. Rajora; A. Rajora; R. Singh; R. Gupta
Health 4.0: On the Way to Realizing the Healthcare of the Future,2020,-1,Outliers,0.34823651265773004,"Health 4.0 establishes a new promising vision for the healthcare industry. It creatively integrates and employ innovative technologies such as the Internet of Health Things (IoHT), medical Cyber-Physical Systems (medical CPS), health cloud, health fog, big data analytics, machine learning, blockchain, and smart algorithms. The goal is to deliver improved, value-added and cost-effective healthcare services to patients and enhance the effectiveness and efficiency or the healthcare industry. Health 4.0 (adapted from the Industry 4.0 principles) changes the healthcare business model to enhance the interactions across the healthcare clients (the patients), stakeholders, infrastructure, and value chain. This effectively will improve the quality, flexibility, productivity, cost-effectiveness, and reliability of healthcare services in addition to increasing patients’ satisfaction. However, building and utilizing healthcare applications that follow the Health 4.0 concept is a non-trivial and complex endeavor. In addition, advanced potential applications based on Health 4.0 capabilities are not yet being investigated. In this paper we define the main objectives of Health 4.0 and discuss advanced potential Health 4.0 applications. To have a clear understanding of these applications, we categorize them in 4 groups based on the primary beneficiary of these applications. Thus we have patient targeted applications, applications supporting healthcare professionals, resource management applications and high-level healthcare systems management applications. In addition, as we studied the different applications, we realized that these is a certain collection of services that these most of them need regardless of their goals or business context. Services supporting data collection and transfer, security and privacy, reliable operations are some examples. As a result we propose creating a service-oriented middleware framework to offers the common services to the applications developers and facilitate the integration of different services to build applications under the Health 4.0 umbrella.",10.1109/ACCESS.2020.3038858,J. Al-Jaroodi; N. Mohamed; E. Abukhousa
Fostering innovation in Digital Health a new ecosystem,2017,3,Topic_3_industry_manufacturing_chain,0.4172162629586224,"Digital Health is driving the uptake in wearable devices replete with biometric sensors. Digital Health is concerned with improving human health by utilizing such high-profile applications of wearable and implantable technology, web and email, mobile technology and social networking, and data management and analytics. As one of the most significant drivers of the consumer health-tech revolution, Digital Health innovation occurs at the intersection of consumer products, advanced technology, medicine, and big data. The multi-industry, multi-discipline, and multi-national character of Digital Health, requires a unique environment in which innovation can occur. Investors and entrepreneurs from the tech sector, health care, and consumer products are creating these new environments. This paper surveys the most interesting Digital Health innovation ecosystems and the funding mechanisms to foster discovery though commercialization through adoption.",,M. K. Hudes
Key Enablers of Industry 4.0 Development at Firm Level: Findings From an Emerging Economy,2023,3,Topic_3_industry_manufacturing_chain,0.6020914304202353,"Organizations in both developed and developing economies are paying great attention to the Industry 4.0 revolution and associated uses of technologies due to its potential benefits to the manufacturing industry. However, there are a limited number of empirical studies due to its early stage of adoption around the world, especially regarding the key technological factors that are necessary. This article addresses this research gap by identifying the factors that enable successful Industry 4.0 technologies adoption in an emerging economy country, grouping them, and ranking the groups based on priorities for adoption. The study adopts a mixed-method research methodology. Q-sort technique and analytic hierarchy process, respectively, were used to group enabling factors and prioritize the groups for Industry 4.0 technologies adoption. Thereafter, semistructured interviews of key stakeholders in the manufacturing sector in Thailand were carried out to validate and support findings from the quantitative analysis. Five industry experts from automotive and electronic parts/components manufacturers were interviewed. The results show that human capital is the most important readiness dimension for Industry 4.0 technologies implementation. Interoperability and data handling were found to be the next in importance. On the contrary, hardware and technology systems, such as data security and technological infrastructure, were identified as the least important of the technology readiness dimensions. These findings provide a different perspective to extant studies that posited that technology-based factors as the most important for Industry 4.0 success.",10.1109/TEM.2020.3046764,D. Adebanjo; T. Laosirihongthong; P. Samaranayake; P. -L. Teh
AEGLE: A big bio-data analytics framework for integrated health-care services,2015,-1,Outliers,0.45990951581173234,"AEGLE project1 targets to build an innovative ICT solution addressing the whole data value chain for health based on: cloud computing enabling dynamic resource allocation, HPC infrastructures for computational acceleration and advanced visualization techniques. In this paper, we provide an analysis of the addressed Big Data health scenarios and we describe the key enabling technologies, as well as data privacy and regulatory issues to be integrated into AEGLE's ecosystem, enabling advanced health-care analytic services, while also promoting related research activities.",10.1109/SAMOS.2015.7363682,D. Soudris; S. Xydis; C. Baloukas; A. Hadzidimitriou; I. Chouvarda; K. Stamatopoulos; N. Maglaveras; J. Chang; A. Raptopoulos; D. Manset; B. Pierscionek; R. Kayyali; N. Phillip; T. Becker; K. Vaporidi; E. Kondili; D. Georgopoulos; L. -A. Sutton; R. Rosenquist; L. Scarfo; P. Ghia
Leveraging Scalable Cloud Infrastructure for Autonomous Driving Data Lakes and Real-Time Decision Making,2025,1,Topic_1_data_big_big data,0.7028993416140342,"Autonomous driving technology relies heavily on the effective management of vast datasets generated by various sensors and vehicle systems. As such, leveraging scalable cloud infrastructure becomes paramount for improving data handling and decision-making capabilities. In this paper, we introduce the Autonomous Driving Data Lakes (ADDL) framework, designed to streamline the storage, retrieval, and processing of extensive driving data in real-time. By utilizing cloud technology, ADDL ensures tight integration of data from diverse sources to enhance situational awareness for autonomous systems. Our architecture features robust data pipelines that support real-time analytics and machine learning applications, which are crucial for timely and accurate decision-making. Extensive experiments with large-scale datasets demonstrate how our approach significantly boosts processing efficiency, data accessibility, and decision-making reliability. The findings highlight advancements in autonomous driving technologies, addressing the challenges associated with data management and enhancing operational effectiveness in changing driving environments.",10.1109/AIITA65135.2025.11048068,J. Chen
Using patents cluster method to facilitate open innovation from the perspective of project portfolio management,2017,3,Topic_3_industry_manufacturing_chain,0.35900036716772193,"Nowadays, under the circumstance of open innovation, R&D organizations of dual-coupled mode have been used in China to solve such problems existing in R&D organizations as repetition and fragmentation of institutions and research programs, low commercialization rate of scientific research results, single operating mode and the phenomenon of research lagging behind the development of industry. Such R&D organizations have become more efficient in providing public welfare, but there are still problems in defining projects and aligning them with innovation strategy and needs of the industry. This paper proposes a patents cluster method from the perspective of project portfolio management for R&D organizations running on dual-coupled mode. The patents cluster method uses intellectual property rights big data to find out the cluster of patents related to a value chain development through data mining, to identify patent owners through related patents and then acquire the related R&D teams, thus achieving the goal of configuring sectoral value chain with the innovation chain. In this research, BICI is selected as a case of R&D organization with dual coupled mode and the patents cluster method is designed and tested. The results indicate that the patents cluster method can be used in facilitating open innovation in R&D organizations running on dual-coupled mode from the perspective of project portfolio management.",10.1109/STC-CSIT.2017.8099430,Z. Jiang; Y. Xue
Network Traffic Identification with Convolutional Neural Networks,2018,1,Topic_1_data_big_big data,0.5478491743527254,"Network traffic identification plays a major role in modern-day network monitoring systems. Most network systems identify traffic based on features such as, flow statistics, static signatures and port numbers. Identifying network traffic is essential, because plenty of information regarding a network flow can be learned by knowing the application protocol associated with it. However, the challenge for traffic classification is to identify features in the network flow data. This paper explores the issue of network traffic identification with neural network and deep learning. A convolutional neural network (CNN) with different optimization algorithms is trained to identify application protocols based on network flow data. The image and text processing hypothesis of the CNN model is extended to naturally fit to the curated dataset. Protocol labels with a high frequency distribution are easily detected by the model, and the results show that the CNN model works equally well on network flow data. A discussion on the performance of different optimization algorithms used with the CNN model is presented.",10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00142,A. V. Jain
An industrial equipment maintenance support system for networked collaborative evolution in the smart product service ecosystem,2022,3,Topic_3_industry_manufacturing_chain,0.9034841815299431,"The traditional model of industrial equipment operation and maintenance has changed with the development of industrial internet technologies such as edge computing, cloud-edge collaboration, big data, artificial intelligence, digital twins, etc. Meanwhile, researchers are beginning to focus on the range of influence of equipment asset reliability, which is shifting from local performance to organizational goals. The networked collaborative evolution of equipment assets and value chain management is becoming the core value proposition of industrial internet platforms. From the perspective of system architecture, this paper discusses a novel industrial equipment maintenance support system based on the industrial internet platform, detailing the design of the resource and edge layer, cloud platform layer, big data governance and service layer, modeling and simulation layer, information model and collaboration layer, and organization, business, and service layer. The proposed system architecture and solution are expected to provide possible guidance for group-type enterprises and cross-industry service providers to help them innovate equipment asset management solutions based on the industrial internet platform.",10.1109/DTPI55838.2022.9998954,Y. -G. Bao; Z. -H. Sun; X. -Q. Liao; Y. Chang; S. Qiu; X. Ming
HPopt: A Hybrid Parallel Optimization Scheduling Approach for Distributed DNN Training,2023,1,Topic_1_data_big_big data,0.5241462299871557,"Recent deep learning relies on large-scale training of Deep Neural Networks (DNNs), which can be time-consuming and computationally intensive. To improve DNN training efficiency, GPU clusters have been used to perform distributed training. However, distributed training has many limitations, including synchronization overhead in data parallelism, computational bottlenecks in model parallelism, and memory constraints in pipeline parallelism, all of which hinder training efficiency. Recently, hybrid parallel distributed training emerged as a solution. However, it doesn’t optimize the size of pipeline parallelism and data parallelism to further improve training efficiency. Our paper introduces HPopt, a method that combines data-parallel and pipeline-parallel training to enhance DNN training efficiency. We’ve designed a heuristic algorithm to improve scheduling decisions in hybrid parallelism. Experimental results on CIFAR-10 show that HPopt is 1.5 times faster than data parallelism and 4.3 times faster than Torchgpipe.",10.1109/CBD63341.2023.00048,C. Weng; X. Li; J. Zhang; H. Yu
PyDaQu: Python Data Quality Code Generation Based on Data Architecture,2023,1,Topic_1_data_big_big data,0.5456035384965517,"Accurate and dependable data is critical when making crucial business decisions. However, verifying the accuracy of complex and extensive datasets can be both error-prone and time-consuming when done manually. We developed a PyDaQu, our automated framework that creates data quality checks code based on a standardized template. PyDaQu offers a variety of quality assurance measures, including validation, completeness, and consistency checks. These measures ensure exceptional data quality while simultaneously streamlining your data management processes. With PyDaQu, creating data quality checks requires significantly less time and effort. We have thoroughly evaluated PyDaQu using data from two different industry domains.",10.1109/MODELS-C59198.2023.00020,M. Abughazala; H. Muccini; K. Qadri
"Smart City, Smart Transportation: Recommendations of the Logistics Platform Construction",2015,3,Topic_3_industry_manufacturing_chain,1.0,"Smart transportation is one aspect of the smart city. Because the low efficiency, the transportation has became the second largest industry of carbon emissions. It's not only affect the smart transportation, but also affect the smart environment. So, improve the efficiency of the transportation is very important to the smart transportation and smart city. The logistics platform is a new business model, which can be improve the un-load rate and decrease the carbon emissions. But, the logistics platforms are at the early development. How to build is a very important issue. This paper has given three suggestions to it. Firstly, to clarify the roles what you will to be, secondly, to choose the model of the platform which fit for your company, thirdly, using an iterative development model to design the product.",10.1109/ICITBS.2015.184,J. Lingli
A DIN Spec 91345 RAMI 4.0 Compliant Data Pipelining Model: An Approach to Support Data Understanding and Data Acquisition in Smart Manufacturing Environments,2020,3,Topic_3_industry_manufacturing_chain,0.6151400703822874,"Today, data scientists in the manufacturing domain are confronted with various communication standards, protocols and technologies to save and transfer various kinds of data. These circumstances makes it hard to understand, find, access and extract data needed for use case depended applications. One solution could be a data pipelining approach enforced by a semantic model which describes smart manufacturing assets itself and the access to their data along their life-cycle. Many research contributions in smart manufacturing already came out with with reference architectures like the RAMI 4.0 or standards for meta data description or asset classification. Our research builds upon these outcomes and introduces a semantic model based DIN Spec 91345 (RAMI 4.0) compliant data pipelining approach with the smart manufacturing domain as exemplary use case. This paper has a focus on the developed semantic model used to enable an easy data exploration, finding, access and extraction of data, compatible with various used communication standards, protocols and technologies used to save and transfer data.",10.1109/ACCESS.2020.3045111,K. Nagorny; S. Scholze; A. W. Colombo; J. B. Oliveira
Engineering Large Wearable Sensor Data towards Digital Measures,2023,-1,Outliers,0.42744761625832883,"Developing digital biomarkers requires handling unprecedented quantities of digital data generated from digital health technologies that utilize a combination of computing platforms, connectivity, software, and sensors. These collected data need to be transformed and transported into a meaningful and useful format before further being derived into health indicators for understanding disease state and life quality. The unique challenges for this class of data engineering tasks are due to the complexity and volume of the digital data we are handling, the data quality and fidelity required to enable subsequent analysis, and the repeated cycles of trial and error to achieve the desired results. This paper presents a family of systems, pipelines, and methods we have designed and built to facilitate these tasks in a typical digital data engineering lifecycle in the context of digital biomarker development.",10.1109/ICBDA57405.2023.10104923,H. Zhang; G. Ruan; R. Giesting; L. Miller; N. Patel; J. Ji; Y. L. Yang; J. Yang
Industry 4.0 and Business Model Innovation: A Scoping Review,2020,3,Topic_3_industry_manufacturing_chain,0.6893373157422151,"The term Industry 4.0 is associated with terms such as Cyber-Physical Systems (CPS), Internet of Things (IoT), Internet of Services (IoS), Robotics, Big Data, Cloud Manufacturing and Augmented Reality and will narrow the gap between the physical and the digital world. Industry 4.0 sets the scene for business model innovation (BMI). The aim of this article was to explore current literature regarding Industry 4.0 when considered from a business model perspective. A scoping review was conducted to explore current research at the intersect of the Business model and Industry 4.0 research domains. The research question we pose thus is a general one: What is the nature of the evidence for Business model innovation in the context of Industry 4.0? The main impacts of Industry 4.0 on 1) industry, 2) products and services, 3) economy, 4) work environment and 5) skills development were mapped. We then conclude by reflecting on these drivers that Industry 4.0 holds a huge potential for business models to become renewed, improved, changed and novel. New value creation will not only originate from new products or services, but also the availability of data to the benefit of the business for example new value propositions like personalization, prediction of customer demand.",10.1109/ICE/ITMC49519.2020.9198424,S. King; S. S. Grobbelaar
A Collaborative System Business Model for Ambient Assisted Living Systems,2018,-1,Outliers,0.4810971019196934,"This paper presents first results of a project which enables elderly people to stay longer in their homes before moving to nursing facilities. Using market available devices an Ambient Assisted Living (AAL) system is developed for the collection of vital data. A minimally invasive radio transmission system and smart home sensors are installed. The data will be recorded, transferred and stored/managed in a private cloud portal [1]. In addition to intelligent data aggregation and the development of innovative functions in the age-appropriate technology-based living, suitable, profitable and fair collaborative business models are necessary to achieve a sustainable establishment. An important key component in the development of a business model is the analysis and description of the value proposition. A scenario analysis was carried out with the aim of assessing potential resident groups and their respective needs, requirements and previous knowledge with regard to the use of AAL technologies.",10.1109/IDAACS-SWS.2018.8525684,J. Bleja; U. Grossmann; H. Langer
Research on the Construction of Smart Factory for Mass Personalization Production,2020,3,Topic_3_industry_manufacturing_chain,0.6707246229989601,"According to the concept and characteristics of the mass personalization production mode, combined with the concept of “Internet + Manufacturing” and related technologies of the industry 4.0 strategy, this paper proposes an overall framework of a smart factory for mass personalization production, and analyzes the key technologies and functional modules of the framework in detail. Taking the production of personalization bicycles in a bicycle intelligent manufacturing laboratory as an example to illustrate the process of achieving mass personalization production.",10.1109/TOCS50858.2020.9339751,M. Xia; Y. He
dShed - Smart Load Shedding Orchestrator for DERM of DERMS,2021,-1,Outliers,0.13608029977075095,"Net-zero carbon emissions goals are rapidly organising every energy requirement around renewable energy sources that produce electricity. There is a push for rapid electrification of the economy to meet this demand. Dynamic Energy Resource Management Systems (DERMs) are going to operate in a federated structure to build a DERM of DERMs, and dock with the distribution management system (DMS) of the Distribution System Operators (DSO). The ability to monitor the network using real time data for protection and control purposes will be prime. This paper focuses on laying the groundwork for it. A Phasor Measurement Unit (PMU) is a device for estimating the phase angle and magnitude of an electrical phasor quantity in the electricity grid using a common time source for synchronisation. Real-time streaming, however, has not been attempted and applications of PMU data in under-frequency load shedding (UFLS) events have been scarce. This paper focuses on ‘dShed’ - our cloud and distributed streaming application while proposing a rate of change of frequency (ROCOF) method to shed load smartly during a UFLS event. Following the implementation of this method, a case study involving the 2011 UFLS event caused by the loss of generation at the Huntly (NZ) Power Station is used to test the method. A comparison between the existing load shedding scheme and the new proposed scheme has identified a 10% reduction in the amount of load to be shed.",10.1109/TENCON54134.2021.9707318,A. R. Chopra; R. Verghese; N. -K. C. Nair
Empowering Industrial Development Through the Smart City Paradigm,2025,3,Topic_3_industry_manufacturing_chain,0.8787694214703337,"This article explores the impact of smart city development on industrial growth, focusing on the transformative potential of digital technologies in urban environments. Smart cities leverage IoT, AI, and advanced infrastructure to enhance industrial efficiency, foster innovation and promote sustainable economic growth. The study examines how the integration of smart city solutions creates opportunities for industrial sectors to adapt to modern challenges and adopt more energy-efficient, resource-conscious practices. Attention is given to the role of urbanization and smart infrastructure in establishing competitive industrial ecosystems. By analyzing these dynamics, the research highlights strategies for aligning smart city policies with industrial advancement. The findings provide valuable insights for decision-makers in both urban planning and industrial sectors, emphasizing the mutual benefits of collaborative innovation.",10.1109/MIPRO65660.2025.11131745,V. Omelyanenko; I. Pidorycheva; Y. Revtiuk; O. Omelianenko; V. Voronenko
Societal Dimension Hurdle to the Adoption of Industry 5.0,2025,3,Topic_3_industry_manufacturing_chain,0.6668768944568124,"This is the first of its kinds owing to the fact that it aimed at identifying key social dimension hurdles to the adoption of the new concept industry 5.0. To date, organisations face numerous challenges as globalisation reduces profit margins, while also being compelled to deliver premium goods and services. Streamlining routine chores guarantees that processes are executed promptly, without overlooking the deadline. Automation primarily assumes control of all labour-intensive jobs, enabling an organisation to significantly accelerate production with minimal mistakes. Improved efficacy leads to higher capacity, facilitating the scaling of operations as the business expands. Further, Industrial revolutions have consistently induced significant transformations in human existence. The Fourth Industrial Revolution has transformed manufacturing output via digitalisation and significantly affected service processes as well. Nonetheless, the Industry 4.0 technologies utilised within the framework of the fourth industrial revolution facilitate the attainment of new objectives that were not explicitly delineated in this paradigm: sustainability, resilience, and human-centricity. This article addresses the technology available for achieving sustainability, robustness, and human-centricity, as well as the primary research directions for developing a sustainable production and service environment. The research findings of this study identified Eight crucial societal challenges to the adoption of industry 5.0 among these the most major concern were Technology acceptance and trust as well as youth unemployment.",10.1109/SmartNets65254.2025.11106797,N. Y. Mulongo
Digital Transformation – Opportunity for Industrial Growth,2019,3,Topic_3_industry_manufacturing_chain,0.9179517245828311,"The rapid development of digital technologies in recent years has led to digital transformation of business and business processes. Faced with new dynamic business models and growing competition in its markets, many companies face the need for a comprehensive review of their existing strategies. Digitization plays a key role in this process, and digital technologies and the opportunities they create are the driving force for success in an increasingly digital society. As a result of digital transformation, business processes, decision making, interaction with partners and end-users are facilitated. It is essential for the growth and development of organizations that management should decide on the consistency, scope and scale for making digital-based changes.",10.1109/CREBUS.2019.8840065,T. Gigova; K. Valeva; V. Nikolova-Alexieva
The Impact of Artificial Intelligence and Internet of Things in the Transformation of E-Business Sector,2019,3,Topic_3_industry_manufacturing_chain,0.918330620998673,"This study explores the impact of Artificial Intelligence (AI) and Internet of Things (IoT) in the transformation of E-Business Sector in South Africa. AI and IoT are beginning to shape the future of many industries globally by generating an unprecedented amount of data. In the case of South Africa, we observe that in e-business new value can be created by the ways in which transactions are enabled. In this study we use the principles and applications of AI and IoT to determine the impacts in the transformation of E-Business sector in South Africa. The objective of this study is not to reproduce experiments, but to investigate and quantify the impact AI and IoT has in the transformative process of change in the E-Business sector. This study employed a qualitative research approach and data was collected through a systematic literature review using the snowballing search method. 18 peer reviewed papers were identified and analyzed in relation to their relevance to the study.",10.1109/SIEDS.2019.8735644,T. A. Malapane
Postal Development: Literature Review into Adoption Models,2019,3,Topic_3_industry_manufacturing_chain,1.0,"The changing postal landscape and rise of digitalization powered by the multiple technology revolutions of the 21st century has prompted Postal Operators across the world to expand their services well beyond the original (traditional) service of merely delivering letters which started more than 100 years ago. However, according to the Postal Development Report of 2018, compiled by the Universal Postal Union (UPU), the majority of Postal Operators are under performing on the Integrated Index for Postal Development (2IPD) that measures four dimensions of postal development: relevance, resilience, reach and reliability. The purpose of the study was to review literature on the 2IPD, Industry 4.0, Technology Adoption and Technology Diffusion as key determinants that could lead to postal excellence. The literature points to postal development as a function of postal excellence. Literature further points to adoption and diffusion variables as key determinants for adoption and diffusion of disruptive technologies.",10.1109/IEEM44572.2019.8978531,K. Mokgohloa; M. G. Kanakana-Katumba; R. W. Maladzhi; J. A. Trimble
Design Guidelines for Apache Kafka Driven Data Management and Distribution in Smart Cities,2022,1,Topic_1_data_big_big data,0.819196581308532,"Smart city management is going through a remarkable transition, in terms of quality and diversity of services provided to the end-users. The stakeholders that deliver pervasive applications are now able to address fundamental challenges in the big data value chain, from data acquisition, data analysis and processing, data storage and curation, and data visualisation in real scenarios. Industry 4.0 is pushing this trend forward, demanding for servitization of products and data, also for the smart cities sector where humans, sensors and devices are operating in strict collaboration. The data produced by the ubiquitous devices must be processed quickly to allow the implementation of reactive services such as situational awareness, video surveillance and geo-localization, while always ensuring the safety and privacy of involved citizens. This paper proposes a modular architecture to (i) leverage innovative technologies for data acquisition, management and distribution (such as Apache Kafka and Apache NiFi), (ii) develop a multi-layer engineering solution for revealing valuable and hidden societal knowledge in smart cities environment, and (iii) tackle the main issues in tasks involving complex data flows and provide general guidelines to solve them. We derived some guidelines from an experimental setting performed together with leading industrial technical departments to accomplish an efficient system for monitoring and servitization of smart city assets, with a scalable platform that confirms its usefulness in numerous smart city use cases with different needs.",10.1109/ISC255366.2022.9922546,T. P. Raptis; C. Cicconetti; M. Falelakis; T. Kanellos; T. P. Lobo
Audit Computer Information Management and Software Development System in the Internet Era,2021,-1,Outliers,0.24691497321600742,"Management information system is a system that uses computers to perform management and decision-making. Since the 1960s, with the progress of computer science, system science, and information science, it has been a discipline to adapt to modern management. In order to study the role of auditing computer information management and software development in the Internet era, this article is based on distributed computing, using case analysis, literature analysis and other methods to collect data from databases such as CNKI, Wanfang Database, and SSCI, and build A model for computer information management and software development. The experimental results prove that the efficiency of auditing information through auditing computers and software has been greatly improved, and the growth rate is about 30%. The accuracy and application efficiency of audit information have also been improved, which is about 20% higher than that of traditional audit methods. This shows that computer information management and software development can play an important role in audit work.",10.1109/IWCMC51323.2021.9498823,P. Xie; Y. Wu
Smart Production Systems: Methods and Application,2022,3,Topic_3_industry_manufacturing_chain,1.0,"Modern production systems if want to survive in the tough market must implement new technologies, which enable real-time decision making. In that way, they can react on time to overcome difficulties that arise with random probability distribution. There are different kinds of methods and technologies which are frequently used in production system processes. In this paper methods, analysis and application of different cutting-edge technologies are represented.",10.1109/INFOTEH53737.2022.9751262,M. Lazarević; G. Ostojić; D. Lukić; M. Milošević; A. Antić
A Study to Evaluate the Industry 4.0 Implementation Status Towards the Sustainability of Indian SME's,2022,3,Topic_3_industry_manufacturing_chain,1.0,"This paper determines to recognize the various barriers to implementation critical success factors in SME and ME in manufacturing organizations. Because of the growing demand for personalisation and high-quality products, enterprises must digitise their operations. With the introduction of computers and Internet of Things (IoT) devices, processes have developed and real-time monitoring has become more convenient. Accurate findings are produced and accurate losses are discovered because of enhanced process monitoring, which helps to increase production. The current manufacturing revolution, known as Industry 4.0, is the introduction of computers and interaction as machines and computers, in which the organisation has a complete mechanism over the entire value sequence of a product's life cycle. Finding the minimize the loss due to automation adoption in SME and ME in their Quality Enhancement = $1.88+ 0.480$ Automation process+ 0.048 Increase in Production + 0.048 Increase in Production+ 0.056 Smooth Manufacturing - 0.089 Loss MinimizationAccording this study automation and customer satisfaction has been increased Customer satisfaction = 1.37 - 0.034 Automation process+ 0.269 Increase in Production+ 0.269 Increase in Production + 0.390 Smooth Manufacturing + 0.058 Loss MinimizationThe paper addresses the many approaches to implementing the notion as well as the tools that may be used in SME and ME.",10.1109/SMART55829.2022.10047174,M. Nazim; H. S. Sodhi; H. Kumar
Supporting Mechatronics Academic Curriculum Beyond Technical Skills,2022,3,Topic_3_industry_manufacturing_chain,0.7232853178755946,"Industry 4.0 is presented as integrating and optimizing manufacturing, logistics, marketing, and procurement in the supply chain. In this perspective, the customization of products and services is crucial. Indeed, according to the rapid changes due to the fourth industrial revolution, firms need to be increasingly reactive in interpreting their customer needs, involving them in designing and developing products and services. This paper tackles the opportunity to include notions such as social creativity and crowd-based applications in the mechatronics academic curriculum.",10.1109/SEEDA-CECNSM57760.2022.9932929,G. Marzano
Industry 4.0: Underlying Technologies. Industry 5.0: Human-Computer Interaction as a Tech Bridge from Industry 4.0 to Industry 5.0,2023,3,Topic_3_industry_manufacturing_chain,0.8188170949301723,"This article is particularly devoted to underlying information and industrial technologies and the conceptual basis for the Industry 4.0 and Industry 5.0 concepts. The report shows that the Base Concepts, Underlying Information, and Industrial Technologies for the Industry 4.0 Concept are the technological platforms, which are the transformations of technologies reached over the previous stages of industrial development. The report also shows that human-computer interaction is a technological bridge leading from Industry 4.0 to Industry 5.0. A practical example of the computer control system’s engineering intended for protection against unauthorized physical access to the data server is also given in this paper. The presented control system implementation includes human-computer interaction, the Internet of Things, and cyber-physical systems and has the properties of Industry 4.0 and Industry 5.0 systems.",10.1109/ICWR57742.2023.10139166,A. Y. Zalozhnev; V. N. Ginz
Aligning Digital Transformations with Value Creation Based on International Integrated Reporting,2022,3,Topic_3_industry_manufacturing_chain,0.6734312228212807,"Owing to pervasiveness of low-cost high-performance computers and communication networks, the whole society is digitalizing. Digitalization has transformed the way in which businesses operate and individuals behave; Digitalized interactions between businesses and individuals on a multi-sided platform bring about big data, then the businesses and individuals are linked together through the data on the platform. In a digital society, businesses and people are inextricably linked on global scale. Companies are confronted with rapidly changing business environments and their business models are required to adapt to such change while expected to sustainably create value over the short, medium, and long time. Integrated reporting provides a framework for companies to disclose how to create value over time. In this framework, value is thought not to be created by or within an organization alone. It is influenced by the external environment, created through relationships with stakeholders, and dependent on various resources. To make digital transformations through big data more effective and sustainable, this paper proposes a framework to align digital transformation of a company with value creation over time based on the International Integrated Reporting Framework. This paper also points out that highly digital businesses should recognize license to user data and content as intangible assets on their balance sheets.",10.23919/PICMET53225.2022.9882809,A. Tomita
Leveraging Structured and Unstructured Data for Tabular Data Cleaning,2024,1,Topic_1_data_big_big data,0.5818583626201796,"The quality of data is essential for the success of any data pipeline, as even small inconsistencies can lead to significant errors in downstream tasks. However, real-world data is often incomplete and noisy, requiring effective data cleaning strategies to ensure the accuracy of downstream operations consuming this data (e.g., training ML models, visualization). A large class of data-cleaning systems relies on user input and/or predefined integrity constraints (e.g., Functional Dependencies) to detect and repair errors in tables. As a result, user expertise is usually needed to specify integrity constraints or provide curated training datasets used to build data cleaning models for specific datasets and use cases. Furthermore, existing approaches do not leverage other data modalities (e.g., text documents), which may contain a wealth of information that helps fix erroneous cells in input tables. In light of these limitations, we propose our work-in-progress system, Beaver, that can clean input tables by learning ""good repairs"" from a large collection of tables and text documents that are used to fine-tune a Large Language Model (LLM). Unlike traditional methods, which rely on user expertise, Beaver benefits from the capabilities of LLMs to identify and learn complex patterns and relationships between different attribute values from structured and unstructured data. Beaver repairs an input table by performing the following steps: (1) Generating question and answer pairs based on the input text documents and tables provided by the user. (2) Fine-tuning the LLM model (e.g., T5) using these generated question-and-answer pairs. (3) Once the model has been trained, Beaver detects erroneous cells in an input dirty table. (4) Invoking the fine-tuned LLM to generate predictions on the erroneous cell values in the input table.",10.1109/BigData62323.2024.10825257,P. Mehra; E. K. Rezig
Predicting the Quality of High-power Connector Joints with Different Machine Learning Methods,2020,-1,Outliers,0.28400586536699535,"State-of-the-art manufacturing processes used in the electric drive production show a high degree of automation and provide a large amount of process data. Often these data remain unused even though they contain potentially valuable process information. The efficient processing and evaluation of these data bears enormous potential for improving the electric drives production, for example with regard to contacting processes. Innovative machine learning (ML) methods already proved to be a powerful tool for big data set evaluation and continuously enter the manufacturing domain. However, the comprehensive and feasible ML application in manufacturing is hindered by the large effort necessary for adequate data preparation. This work lays the foundation for ML application in ultrasonic metal welding and related contacting techniques, which play an important role in the electric drives production. Therefore, on the one hand side, a data pipeline is developed which covers necessary steps of data preparation. On the other hand side, two data sets generated with a validated US metal welding process model are processed with the data pipeline and quality prediction is performed with three different regression methods, which include classical linear regression as well as advanced ML methods as a neural network. For quality prediction, the mean absolute percentage error reaches values as low as 6.9 %.",10.1109/EDPC51184.2020.9388211,E. Birgit Schwarz; F. Bleier; J. -P. Bergmann
Implementing industry 4.0 — A technological readiness perspective,2017,3,Topic_3_industry_manufacturing_chain,0.6250893750434121,"This paper identifies the relative importance of key enabling factors for implementing industry 4.0 from a technological readiness perspective. The research involves the identification of enabling factors, their categorization into technological readiness dimensions, followed by the determination of the relative importance of both technological readiness dimensions and key objective measures. The results show a strong relationship between technological readiness and design principles of Industry 4.0. The findings suggest that process-related objectives are more important than economic-related and environmental-related objectives when implementing industry 4.0. The results also show that “the knowledge of humans in technology and how to leverage it” and “improving the ability of machines and devices in connecting to the internet” are the most important factors for achieving all objective measures. Practitioners can use the apparent relationship between process related objectives and key technological dimensions for setting appropriate strategies and policies when moving towards Industry 4.0.",10.1109/IEEM.2017.8289947,P. Samaranayake; K. Ramanathan; T. Laosirihongthong
Technical potentials and challenges within internal logistics 4.0,2018,3,Topic_3_industry_manufacturing_chain,1.0,"The industrial environment is characterized by a permanent change. As a result, the production requirements and their processes are constantly changing. The present paper takes up the latest transformation of industry through digitization and describes future “smart solutions” for new value-added concepts. The focus is on the impacts of changes on the logistics sector within Industries 4.0 as well as on the technical potentials for internal logistics. In this context a new description approach is developed. The aim is to show a new approach that ranges from key elements of elementary logistics to the diverse new requirements of Logistics 4.0 Furthermore, the future role of humans in the industrial working environment is controversially discussed with proceeding autonomy and digitization.",10.1109/GOL.2018.8378072,N. Schmidtke; F. Behrendt; L. Thater; S. Meixner
South Korean Smart Manufacturing Strategy,2022,3,Topic_3_industry_manufacturing_chain,1.0,"Manufacturing companies are pushing for smart manufacturing strategies that combine next-generation new technologies and manufacturing technologies beyond the existing factory automation level. Accordingly, domestic and foreign manufacturing companies are rapidly developing tasks such as product planning, R&D, production, process, quality, facilities, supply chain management, and customer management from traditional manufacturing methods. In this rapidly changing global economic environment, domestic and foreign companies are rushing to make digital transformation to secure their own manufacturing competitiveness. In this rapidly changing global economic environment, domestic and foreign companies are rushing to make digital transformation to secure their own manufacturing competitiveness. Recently, the Korean manufacturing industry is also rapidly changing due to the convergence of the 4th Industrial Revolution technology with the traditional manufacturing method. The manufacturing environment is changing due to the application of ICT technology and digital transformation throughout corporate activities, and this acceleration of innovative digital transformation activities will break the value chain not only inside the company but also across the industry and lay the foundation for Korea’s continued growth.",10.1109/BCD54882.2022.9900787,S. J. Lee; H. J. Cho
An Interface to Heterogeneous Data Sources Based on the Mediator/Wrapper Architecture in the Hadoop Ecosystem,2018,1,Topic_1_data_big_big data,0.6036608297510577,"Heterogeneity of data sources is a challenge for Big Data applications. This contribution presents an approach based on the conceptual foundation of the mediator/wrapper architecture where source data is transformed into a global schema and transferred to one or more data sinks with Apache Kafka. Given appropriate wrappers, arbitrary data sources and sinks can be connected with one another in a flexible fashion. The design and implementation of a prototype for a data set from a metagenomics study of are described.",10.1109/BIBM.2018.8621111,K. -D. Schmatz; K. Berwind; F. Engel; M. L. Hemmje
"DQA: Scalable, Automated and Interactive Data Quality Advisor",2019,-1,Outliers,0.48873524046188704,"Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.",10.1109/BigData47090.2019.9006187,S. Shrivastava; D. Patel; A. Bhamidipaty; W. M. Gifford; S. A. Siegel; V. S. Ganapavarapu; J. R. Kalagnanam
SpeciServe. a gRPC Infrastructure Concept,2024,1,Topic_1_data_big_big data,1.0,"Smart city projects require data to be transferred from one destination to the next using a number of different network protocols. The data pipelines involved in these smart city projects often have limited bandwidth or compute resources due to the low power nature of most embedded hardware. The data transferred between devices in these types of embedded systems are often structured in non-standard data schemata. Remote procedure calls (RPC) are implemented to transfer data between devices and switching between RPC implementations can be tricky due to the lack of standardization. There is no guarantee that an existing data schema will work with a different RPC implementation. This makes it difficult for a researcher or system developer to benchmark and compare different RPC im-plementations. In this paper, a conceptual infrastructure named SpeciServe is introduced where gRPC is used as a communication backbone due its support for flatbuffers and multiple server modes. Multiple software services are described to allow for dissimilar RPC implementations to be run in parallel. This system is intended to allow for researchers in machine learning, smart cities, and Internet of Things (loT) to be able test different versions of RPCs and provide support for system developers to define the functions of an edge service.",10.1109/SERA61261.2024.10685565,C. Carthen; A. Zaremehrjardi; Z. Estreito; A. Tavakkoli; F. C. Harris; S. M. Dascalu
Smart design for ships in a smart product through-life and industry 4.0 environment,2016,3,Topic_3_industry_manufacturing_chain,0.8009490013677972,"As the world becomes more connected and customer's `needs and wants' lean more towards bespoke products, the way they are designed and manufactured must adapt. Recognised as the main driver for the future of the manufacturing value chain, Industry 4.0 (i4) is rapidly gaining momentum worldwide. One of the key enablers of the i4 design model to achieve mass customisation manufactured at a mass production cost is the computational intelligence based Computer-Automated Design (CAutoD). This paper demonstrates how CAutoD realises the i4 concept for smart design of future ships and smart ships through-life. Following an overview of the i4 and CAutoD interface, a smart ship design technique is introduced to form an automated closed-loop approach to the entire ship design process. Then, key challenges and future directions on this roadmap are discussed. Lastly, a framework in which the concepts of morphing and free-form deformation are embedded into an evolutionary algorithm is developed to automate the design and optimisation process of the hull form.",10.1109/CEC.2016.7748364,J. H. Ang; C. Goh; Y. Li
Trust Issues for Big Data about High-Value Manufactured Parts,2016,-1,Outliers,0.2690738588912898,"In the world of high-value manufacturing, an imperative for productivity and quality are driving the manufacturing community towards integration of all information captured about a product along the manufacturing value chain, from its design and manufacture through usage, maintenance, and decommissioning. Much of this information is already captured, but it is scattered across many organizations, with significant variation among levels of security consciousness and information technology sophistication. Any weak link in this chain of organizations constitutes a threat that can have a major negative impact for organizations all along the chain. In this paper, we explain the reasons for the move towards integration of information about high-value manufactured products. We introduce the concept of a digital thread, which is the entre set of information about the life history of a manufactured object. We outline several key threats to digital threads that have not been fully addressed in previous work on securing provenance information, and propose digital-threads-as-a-service (DTaaS) as a potential way to mitigate several of the open issues.",10.1109/BigDataSecurity-HPSC-IDS.2016.50,A. Adhikari; A. Hojjati; J. Shen; J. -T. Hsu; W. P. King; M. Winslett
A datacube approach to agro-geoinformatics,2017,-1,Outliers,0.1777471967172808,"Agricultural maps produced from earth observation data are useful at several stages in the agricultural value chain and allow farmers to make rational and comprehensive decisions when planning, planting and growing new crops. Towards this direction, a platform for the online, real-time analysis of big earth observation data for precision agriculture applications has been designed and implemented. This platform currently provides services like vegetation detection, calculation of standard vegetation indices, canopy greenness estimation, land surface temperature estimation and time-series analytics. The service interface utilizes the OGC Web Coverage Processing Service (WCPS) standard which establishes analytics on spatio-temporal datacubes. Implementation of the service is based on the open-source scalable array engine, rasdaman.",10.1109/Agro-Geoinformatics.2017.8047015,A. Tzotsos; A. Karmas; V. Merticariu; D. Misev; P. Baumann
Revenue Optimized Capacity Management for Integrators in Air Freight Industry Under Uncertainty,2018,3,Topic_3_industry_manufacturing_chain,0.6224838629724474,"With the increasing adoption of innovative business practices the product lifecycle management is becoming incredibly volatile and challenging. In order to reduce the transportation time many supply chain managers are relying on express air freight based transport services. With the increasing average demand for capacity provided by the air freight service providers, various business entities are appearing in the value chain, such as service providers, integrators. Typically, an integrator owns airbuses to provide service to their clients as well as they can buy extra capacities from other commercial airlines based on need. As per the state of the art practices the capacity for the long term clients are provided through a deterministic model and priced through a bid based mechanism which maximizes the revenue. The demand that realizes close to the time of actual shipment is fulfilled through allocating the capacity through spot market. The spot market capacity allocation is performed based on a popular newsvendor framework. This paper proposes a holistic framework to develop a stochastic model to allocate the capacity for different types of capacity. The objective is to maximize the expected revenue with uncertainties in the system. This problem is fundamentally a dynamic programming problem. We implement an affine controller to develop a computationally tractable formulation of this dynamic programming problem. We conduct experiments from real life data obtained from a global player in this industry to demonstrate the superiority of the proposed model, over the state of the art practices.",10.1109/BigData.2018.8622173,D. Paul; X. Chi; S. Junxian; J. Zeng
An Improved Distributed Computing Time-Series Short-term Load Forecasting Pipeline for MV Transmission Planning,2023,-1,Outliers,0.1996882724190146,"Planning, maintenance, and power flow analysis of modern power systems generally require the application of load prediction as a critical component. This paper presents a general, scalable, end-to-end big data pipeline, that uses the strong correlation between energy consumption, date and time information, and weather data for 24-hour-ahead load forecasting. The suggested approach is validated using actual energy usage data gathered from more than 100,000 distribution transformers in the Spanish electrical smart grid and implemented seamlessly using the parallelization capacity of distributed computing cluster leading to a dramatic decrease in the execution time. This paper introduces the following interesting findings: i) in the context of big data where hundreds of thousands of distribution transformers data exist, load forecasting at aggregated level can be done using linear regression models where more than 85% of the total energy consumption, across all distribution transformers in Spain, is forecasted with an $R^{2}\geq 70\%$. ii) slicing the data into $\text{24}$ pieces to develop a forecasting model for each hour and connect these models together to reflect the inter-hours correlation is better than building one regression model for all 24 hours together. iii) computation cost and time are the most important driving factor in choosing the best regression model in short-term load forecasting at the aggregation and production levels.",10.1109/ISNCC58260.2023.10323748,H. Kanaan; M. S. -E. Galarraga; M. Houchati; J. O. Ruiz; S. B. Lopez; M. R. Asensio
Evolution of Enterprise Architecture for Digital Transformation,2018,3,Topic_3_industry_manufacturing_chain,0.7327046417302235,"The digital transformation of our life changes the way we work, learn, communicate, and collaborate. Enterprises are presently transforming their strategy, culture, processes, and their information systems to become digital. The digital transformation deeply disrupts existing enterprises and economies. Digitization fosters the development of IT systems with many rather small and distributed structures, like Internet of Things, Microservices and mobile services. Since years a lot of new business opportunities appear using the potential of services computing, Internet of Things, mobile systems, big data with analytics, cloud computing, collaboration networks, and decision support. Biological metaphors of living and adaptable ecosystems provide the logical foundation for self-optimizing and resilient run-time environments for intelligent business services and adaptable distributed information systems with service-oriented enterprise architectures. This has a strong impact for architecting digital services and products following both a value-oriented and a service perspective. The change from a closed-world modeling world to a more flexible open-world composition and evolution of enterprise architectures defines the moving context for adaptable and high distributed systems, which are essential to enable the digital transformation. The present research paper investigates the evolution of Enterprise Architecture considering new defined value-oriented mappings between digital strategies, digital business models and an improved digital enterprise architecture.",10.1109/EDOCW.2018.00023,A. Zimmermann; R. Schmidt; K. Sandkuhl; D. Jugel; J. Bogner; M. Möhring
Innovation Strategies to Counter Client-Led Business Disruption,2022,3,Topic_3_industry_manufacturing_chain,0.6247740892615147,"The availability of big data, supported by advanced technologies, has given rise to a more informed and empowered global consumer, resulting in extreme pressure on organisations to continuously find new innovative ways to serve these clients. When operating under tough economic conditions companies tend to challenge the timing of innovative initiatives. The truth is that they simply have no choice. Clients now have more information and more choices than ever and an ever-growing list of demands and expectations. Simultaneously, there is increased competition for the same share of wallet. They must rise to the challenge, gear up for the battle and understand that they need to innovate and operate differently to survive. Qualitative research was conducted to assess the impact of client-led business disruption on a diverse and decentralised international logistics solutions organisation. This article will detail the proposed strategies that were derived from the business experience of the authors, to not only counter the threats of the client-led disruption, but also to embrace the opportunities that this will create for the organisation.",10.23919/PICMET53225.2022.9882801,A. -M. v. Wyk; J. -H. C. Pretorius; L. Pretorius
Leveraging Edge Computing and Differential Privacy to Securely Enable Industrial Cloud Collaboration Along the Value Chain,2021,2,Topic_2_data_privacy_security,0.7966749655393391,"Big data continues to grow in the manufacturing domain due to increasing interconnectivity on the shop floor in the course of the fourth industrial revolution. The optimization of machines based on either real-time or historical machine data provides benefits to both machine producers and operators. In order to be able to make use of these opportunities, it is necessary to access the machine data, which can include sensitive information such as intellectual property. Employing the use case of machine tools, this paper presents a solution enabling industrial data sharing and cloud collaboration while protecting sensitive information. It employs the edge computing paradigm to apply differential privacy to machine data in order to protect sensitive information and simultaneously allow machine producers to perform the necessary calculations and analyses using this data.",10.1109/CASE49439.2021.9551656,A. Giehl; M. P. Heinl; M. Busch
Recent Advances in Data Engineering for Networking,2022,1,Topic_1_data_big_big data,0.38989439114455765,"This tutorial paper examines recent advances in data engineering, focusing on aspects of network management and orchestration. We provide a comprehensive analysis of standardization efforts as well as platform development activities related to data engineering driven network design. We then focus on the integration aspects of the data engineering ecosystem and telecommunication networks. The results of our tutorial investigation show that despite various efforts towards standardization and network management and orchestration platforms, there is still a significant gap in applying recent developments in the evolving data engineering world to the telecommunication domain. New advanced functionalities in data engineering as well as clear separations between the building blocks of data engineering pipelines within the proposed standardized architectures have been overlooked or not explored in detail by the standardization or platform development bodies in the telecommunication domain. Therefore, at the end of the paper, we discuss these gaps and research challenges in the context of future development processes for data engineering-driven network design and applications of data engineering concepts in telecommunication networks. We also propose several recommendations for early adoption of these technologies and frameworks in telecommunication infrastructures and platforms.",10.1109/ACCESS.2022.3162863,E. Zeydan; J. Mangues-Bafalluy
"Alleviating I/O Inefficiencies to Enable Effective Model Training Over Voluminous, High-Dimensional Datasets",2018,1,Topic_1_data_big_big data,0.5403058193151912,"There has been an exponential growth in data volumes in several domains. Often these voluminous datasets encompass a large number of features. Fitting models to such high-dimensional, voluminous data allows us to understand phenomena and inform decision-making. The analytics process is naturally iterative as scientists explore the set of features, data fitting algorithms, portions of the dataspace, and the particular algorithm's hyperparameters to guide their model-building process. It often takes several model-fitting attempts before one arrives at a satisfactory solution that may then be subjected to further refinements. Each of these model-building attempts is itself time-consuming and dominated by I/O and data movement costs. In this study, we present our methodology for significantly alleviating I/O-induced inefficiencies during model training. Rather than work with the raw data, we generate and work with sketches of the data. Our framework, Fennel, is independent of the libraries or analytical engines preferred by users. Our empirical benchmarks have been performed with datasets from diverse domains (weather, epidemiology, and music) and we profile several aspects of our methodology.",10.1109/BigData.2018.8622279,D. Rammer; W. Budgaga; T. Buddhika; S. Pallickara; S. L. Pallickara
COTuner: Joint Optimization of Resource Configuration and Software Parameters for Recurring Streaming Jobs on the Cloud,2024,1,Topic_1_data_big_big data,1.0,"Considering the operational efficiency and cost-effectiveness, big data pipelines are increasingly deployed on the cloud. Since streaming jobs often recur at specific intervals, selecting appropriate resource configuration and software parameters for these recurring jobs can significantly reduce resource costs. However, the high-dimensional configuration space and the time and cost-consuming configuration evaluation together makes this joint optimization problem quite difficult if not impossible. To address this challenging problem, this paper introduces a framework called COTuner which is able jointly auto-tune the resource configuration and software parameters for recurring streaming jobs deployed on the cloud. Specifically, COTuner first introduces a pseudo-point mechanism into the BO_dropout algorithm to address the formulated high-dimensional black-box optimization problem while improving the convergence rate. Besides, COTuner is also able to be quick adaptive to different scenarios via the ability to adaptively adjusting its hyperparameters. We evaluated the effectiveness and efficiency of COTuner with three different stream processing jobs from Hibench under a local streaming pipeline consisting of Kafka and Flink. Experimental results show that COTuner can achieve better resource costs than all the other four baseline methods under the same performance constraint. In addition, we also leverage ablation experiments to analyze and illustrate the function of each proposed techniques in COTuner.",10.1109/CCGrid59990.2024.00019,H. Dou; S. Zhu; Y. Zhou; Y. Zhang; J. Mei; Y. Wu; J. Dai
Evolution of ETL Tools: Trends and Insights from On-Premises to Cloud Solutions,2025,1,Topic_1_data_big_big data,1.0,"Data preparation, including extraction, transformation, and loading (ETL), is a critical yet resource-intensive process in modern data-driven systems, particularly with the increasing volume of heterogeneous, high-velocity data from AI, cloud computing, and IoT. Traditional ETL tools often struggle with performance bottlenecks and scalability issues when processing large datasets from multiple sources. This study presents a high performance, multi-threaded ETL tool designed to address these challenges by optimizing local file ingestion and enabling parallel processing. The tool integrates flexible data cleaning and transformation mechanisms, enhancing data preparation efficiency for AI models and cloud-based systems. Through comprehensive evaluation across domains like healthcare and cloud computing, this research contributes a scalable, domain-agnostic solution that streamlines data preparation and supports modern data pipelines.",10.1109/ICSCDS65426.2025.11166785,K. S. Dinesh; S. Ghosh
Driving Safety & Awareness Cooperative Business Model exploiting the 5GMETA platform,2022,-1,Outliers,0.49402831480538834,"The introduction of the 5G in the automotive sector paves the way to new business opportunities and stakeholders collaboration, thanks to the availability of new connected vehicles technologies. To emphasise the opportunities of improved data sharing, different business modelling approaches exist, depending on the phase of the data value chain that technologies are able to cover. The 5GMETA open platform aims to leverage CCAM-based captured data to stimulate, facilitate and feed innovative products and services exploring the possibilities enabled by collaborative business models. This paper describes a new business model which focuses on a Use Case (UC) on “Driving Safety and Awareness” that exploit the 5GMETA platform, and highlight its business innovation. In this context, the value proposition relies in the ability to increase road safety thanks to a better driving-misbehaviour detection, thus providing benefits to a wide range of potential stakeholders. The UC has its foundation in the real-time data collected, in a scalable and reliable way, by the platform. The main innovation that the adoption of such business model will bring is the creation of new partnerships that may be enforced by a clear definition of the profit-sharing strategy between the actors involved.",10.1109/FNWF55208.2022.00059,C. Canova; F. Pacella; D. Brevi; M. Apruzzese; R. Cavicchioli
Machine learning-based predictive analytics and big data in the automotive sector,2023,-1,Outliers,0.312460289598384,"Data science and machine learning's inherent capacities to automatically learn and optimize processes and products are making them more indispensable to the automotive industry of the future. This essay will define data science and machine learning and draw parallels between the two fields. A definition of automatic optimization is provided, and its value as a tool when paired with data analytics is shown. It illustrates how these technologies are currently being applied in the industry and presents examples using the important subprocesses in the automotive value chain. The sector is only beginning to explore the many of applications for these innovations, therefore we utilize futuristic use cases to illustrate their transformative potential. The article concludes by demonstrating how these technological advancements may increase productivity in the automotive industry and fortify the sector's customer focus across the board, from product and creation process to customers and their interaction with the product.",10.1109/ICACITE57410.2023.10182665,M. Lourens; S. Sharma; R. Pulugu; A. Gehlot; G. Manoharan; D. Kapila
Designing Data-Intensive Application System for Production Plans Data Processing and Near Real-Time Analytics,2022,3,Topic_3_industry_manufacturing_chain,0.616032499160446,"In this work, benchmarking of production plan processing applications based on data storage and analytics solutions using open source technologies was performed. The functional and component architecture of a digital framework for processing production plan files is presented, with special attention to the performance analysis of data processing based on the measurement of processing time for each data volume and memory used. The proposed architecture of the application system consists of digital services, including the downloading data from hundreds of production plan files, processing these files, converting them into JSON objects and inserting SQL into an in-memory database for real-time queries, where they are analyzed using KPI -based methods. The proposed system allows processing and analyzing production plans almost in real time, which enables decision makers to check and monitor the status of production plans in real time and take the necessary actions on demand and on time. The proposed digital framework was tested on data collected from real production plans of a truck manufacturing company.",10.1109/CoDIT55151.2022.9804133,A. S. Suleykin; P. B. Panfilov
AI Integration in FI and VSC Technologies,2025,3,Topic_3_industry_manufacturing_chain,0.3627815025891466,"The authors looked at several types of AI, how it works with the food value and supply chain, other AI-using technologies, adoption barriers for AI in different contexts, and solutions for overcoming these barriers. This is because it illustrates how artificial intelligence's wide range of applications creates opportunities for vertical integration across the food supply and value chain. It affects the many stages of the chain in developing technologies. The methods by which artificial intelligence interacts with other technologies are demonstrated by a systematic literature analysis. These technologies include big data mining, machine learning, the Internet of services, agribots, industrial robots, sensors, drones, digital platforms, driverless cars and machinery, and nanotechnology. They also assign distinct capabilities to various phases of the technology. However, a variety of social, technical, and financial obstacles affect how it is used. However, these obstacles may be overcome by raising the farmers' level of financial and digital literacy and sharing best practices with all parties involved in the food supply and value chain. Artificial intelligence has tremendously changed the way food value and supply chain looked once. Therefore, a seismic move in the production, distribution, processing, or consumption of food has happened. The salient review of the role of AI through the food supply chain shows how it can be efficient, waste-free, and avert crises. AI-driven agricultural technologies, including predictive analytic and precision farming tools, help farmers control pests, sow more effectively, and make better use of resources in order to reap higher yields and lessen their environmental impact. In terms of processing, AI applications support sorting and packing procedures by using quality control features devised to work for the most advanced imaging and robotic systems. Similar to this is how AI supports optimization of logistics through better delivery and routing plans, which decrease waste because of more significant inventory control and, ultimately, also reduces the carbon impact of the transportation sector.",10.1109/IC363308.2025.10957349,H. M. Al-Jawahry; J. Sravanthi; J. Seetaram; S. S. J; K. S; M. Ramya
Shared Services Common Data Model to Deliver Advanced Analytics,2022,-1,Outliers,0.14512416688632085,"Smart cities generate large amounts of data from various services and hardware. Data is generated and collected from Advanced Meter Infrastructure (AMI), Weather Stations, SCADA systems, and transportation systems for example. Within each of these data domains cities have many hardware and software vendors to pick from, each with their own proprietary method to store and share data. High resolution data provided by technologies such as AMI meters create the opportunity to deliver new distribution network and customer analysis that were not possible previously; however, the delivery of these advanced analytics projects has been challenging. While the identification of advanced analytics use cases is generally agreed upon across utilities, execution of these services is typically delivered in a case-by-case basis depending on chosen vendors and ability to integrate the necessary systems and data. For smaller utilities this often renders the service costs out of reach. By creating and leveraging a standardized analytical data model focused on delivery of advanced analytics services rather than the vendor's software and hardware needs, economies of scale are realized across large and small utilities, reducing costs and the barrier of entry into advanced analytics and data driven decision making.",10.1109/ISC255366.2022.9922563,B. Gall; C. Tucker; B. Massey
Frontier exploration of internet thinking and case study,2016,3,Topic_3_industry_manufacturing_chain,0.6683966639968862,"Traditional enterprises should change their marketing strategy, business model, organizational structure and etc. to cope with the new market in the Internet age. Duo to the above requirement, the conception of Internet Thinking (IT) is created in China. Although many scholars have done a lot of research on this field about the company operating at the context of Internet and a few Chinese researchers begin to pay attention to Internet Thinking. The existing research focused on the specific points without a systematic view. This paper try to integrate different points of view and combine with the practice of Chinese market to figure out what is Internet Thinking. The first, some key words of Internet Thinking are extracted from literature review and practice in business world; the second, the components involved are analyzed and the definition is given; at last, the company of Xiaomi will be taken as the case to test the model.",10.1109/LISS.2016.7854402,L. Li; S. Liu; K. Zhang
Data lifecycles analysis: Towards intelligent cycle,2017,1,Topic_1_data_big_big data,0.45056242864684665,"As companies generate and handle increasingly large amounts of data along with the Big Data era, several model of data lifecycle have been proposed to deal with this situation. The analysis, the management and the use of data becomes more complicated or almost impossible in some cases for the companies. To transform these data to a knowledge, the choice of the adequate lifecycle that matches with the company expectations becomes essential. For this goal, this paper aims to be a guide to assist companies to choose a lifecycle that fits their data management vision. For this, we identify the relevant criteria of selection cycles and defined a rating system to each of these criteria. In this paper, we study the available lifecycles of data in the literature that we consider relevant. As a result of this study, we classify these cycles following two types: first analysis oriented phases and the second based on relevant criteria.",10.1109/ISACV.2017.8054938,M. E. Arass; I. Tikito; N. Souissi
Eliminating Data Collection Bottleneck for Wake Word Engine Training Using Found and Synthetic Data,2019,1,Topic_1_data_big_big data,0.49590131212635397,"Voice interfaces are fast becoming an important human-machine interfaces, and Wake Word Engines (WWEs) are a critical part of modern voice interfaces. There are recent advancements in applying Deep Learning (DL) or Deep Neural Network architectures for WWE construction. Similar to other applications of DL however, achieving good accuracy strongly depends on training using the right type of dataset - a task that traditionally requires significant time and human effort. In this paper, we present novel techniques for curating WWE datasets that significantly minimizes the need for data collection from humans. More specifically, we investigated two techniques: (1) Using “Found Data”: we have created automated data curation pipelines for locating and processing data from public sources (e.g., YouTube), and (2) Using Synthetic Data: we explored the use of Synthesized Speech (using Text to Speech and Voice Conversion) to synthetically generate WWE datasets. Using our techniques, we are able to train WWEs that demonstrate a level of performance that is on-par with the WWEs trained with data from expensive human collection (e.g., Mechanical Turk). For example, in our experiments using wake words `Computer' and `Shannon', for a given False Alarm rate of 1 per hour, WWEs trained with our novel methods exhibit a False Reject Rate, averaged across four different test environments, of 0.9% and 1.6% respectively compared to the 1.0% and 0.8% for the baseline trained using data from human collection. Cycle time savings of more than an order of magnitude is possible by utilizing these methods.",10.1109/BigData47090.2019.9006601,B. Ramanan; L. Drabeck; T. Woo; T. Cauble; A. Rana
Predictive Modeling for Early Identification of Disease Severity in Acute Respiratory Infections: A Case Study with COVID-19,2023,-1,Outliers,0.37611406522515295,"The COVID-19 pandemic, declared a global emergency by WHO in March 2020, has had a profound impact on global health. With millions affected, the pandemic's magnitude became evident. Even advanced healthcare systems grappled with resource shortages, from protective gear to ventilators. Late diagnosis and the inability to adapt to new SARS-CoV-2 variants led to misdiagnoses and high casualties. Overburdened healthcare facilities and the absence of a centralized patient data system hindered swift decision-making. The lack of real-time monitoring meant many didn't receive timely, personalized care, underscoring the need for an integrated healthcare response during global emergencies. To tackle these challenges and improve hospital resource management during pandemics, this study presents a web approach harnessing big data analytics to develop a machine learning predictive model. This model has been developed to enable hospitals to assess patients' severity in real-time, taking into account a wide range of factors such as symptoms, medical history, and demographics. To acquire data regularly, a streaming data pipeline was established using Apache Kafka and Apache Streaming. In predicting the severity levels of patients, both Multinomial Logistic Regression and Multilayer Perceptron (MLP) models were employed. One noteworthy finding was that using datasets with balanced classes yielded significantly higher accuracy for the MLP model compared to unbalanced datasets. Specifically, MLP achieved the highest accuracy of 60.4% when trained on balanced classes. This model's real-time severity insights have the potential to revolutionize decision-making and resource allocation in healthcare. It represents a significant step towards a more effective response to future pandemics and the strengthening of the global healthcare system.",10.1109/ICAC60630.2023.10417268,V. Kalapuge; D. Kasthurirathna
Internet of Things and Business Models,2015,3,Topic_3_industry_manufacturing_chain,0.8234201653253435,"Understanding implications on firm performance from the emergence of Internet of Things (IoT) is critical in order for firms to make rational decisions regarding business model configurations and investments in IoT. The transition between the current state and widespread adoption and diffusion of IoT is expected to be complex, and standardization is one of multiple barriers discussed in research regarding Internet of Things. In order to ensure interoperability between billions of heterogeneous devices, standardization may be a significant factor with potential impact on IoT investments. To expand our understanding of strategic implications of IoT investments, this paper analyzes the impact of IoT on business models and sources of value creation by applying a proposed framework to empirical illustrations. Further research is needed to support development of frameworks and to advance our understanding of how firms should act in order to meet challenges generated by the emergence of IoT.",10.1109/SIIT.2015.7535598,P. Hognelid; T. Kalling
Research on the Key Elements Deconstruction and Quality Evaluation Index System of Film and TV IP,2023,1,Topic_1_data_big_big data,0.3778360123453784,"Film and television serve as significant mediums for cultural expression. The identification and accurate evaluation of the key elements inherent in high-quality film and television intellectual property (IP) are of paramount importance for the high-quality development of cultural endeavors and cultural industries. This article leverages both knowledge and data as dual driving forces to deconstruct the key elements inherent in high-quality film and television IP and establish a comprehensive evaluation system. Guided by knowledge, the key elements of film and television IP in each stage are deconstructed from a value chain perspective, resulting in the development of an initial evaluation index system. Driven by data, the final index system underwent purification via factor analysis, and its validity was further confirmed through multiple linear regression and Backpropagation Neural Network models. Lastly, the CRITIC method is employed to reinforce the interpretability of the indicator system and address the interpretability issue of the model results, thereby accurately reflecting the contribution and significance of each indicator to the target variable.",10.1109/CoST60524.2023.00021,N. Luo; Z. Zhu; Y. Ni
Streamlining Social Music Analytics with Distributed Data Processing: Apache Airflow and MongoDB and Spark SQL,2025,1,Topic_1_data_big_big data,1.0,"In the era of data-driven applications, distributed data processing plays a crucial role in managing, analyzing, and deriving insights from vast amounts of user-generated data. This paper presents a scalable data pipeline that uses Apache Airflow, MongoDB Atlas, and Spark SQL to process and analyze Spotify music streaming data. Our pipeline enables a social media-like platform where small friend groups can share and compare their listening history. Using Airflow, we automate data ingestion from the Spotify API, store the data in Google Cloud Storage, and import it into MongoDB Atlas for further processing. We employ Spark SQL on Google Cloud Dataproc to generate analytical insights, such as songs listened to the most, shared music preferences, and user-specific listening patterns. The resulting analytics are then visualized on a dynamic dashboard. This research highlights the effectiveness of distributed data processing frameworks in building scalable and interactive applications while showcasing best practices for integrating cloud-based NoSQL databases with big data analytics tools.",10.1109/CIACON65473.2025.11189649,B. Srinivas; E. Bardak; I. Avila; J. Brungard; Y. Cao; M. B. Chaudhari
Production process adaptation to IoT triggered manufacturing resource failure events,2017,3,Topic_3_industry_manufacturing_chain,1.0,"Usage of raw data as an asset, from which value can be created to support business and manufacturing decision making, motivated a lot of scientists to explore the challenges on how to exploit this value. Such efforts are concentrated on the framework of “data value chain”, where approaches of architectures and applications aim at equipping enterprises with tools that gather, process and extract knowledge from raw data generated by their internal or external processes. In this context, the present paper deals with the way the production processes in a factory can be adapted to changes that are detected by the processing of raw data which are aggregated by IoT devices installed in the manufacturing environment. The proposed approach introduces a complex system that combines a network of IoT sensors and a high-level multi-agent system that contributes to the vertical integration of all the systems residing in the Enterprise/Factory.",10.1109/ETFA.2017.8247671,C. Alexakos; C. Anagnostopoulos; A. Fournaris; A. Kalogeras; C. Koulamas
Data Security Testing Method of Intelligent and Connected Vehicles Based on Typical Scenarios,2023,2,Topic_2_data_privacy_security,0.34910741609458523,"With the development of the intelligent and connected automobile industry, vehicles have become important information terminals integrated into the interconnection system. The data generated by intelligent and connected vehicles (ICVs) has many application scenarios and rapid changes. And there are lots of potential risks which threats the security of data. The data includes a large amount of personal information, such as vehicle trajectory data and in-vehicle camera video data. In order to avoid the risk of personal privacy security caused by the leakage, this paper analyzes the security risks in the data lifecycle, and chooses eight typical scenarios of ICVs data security, including Internet of Vehicles (IoV) data extraction, vehicle-machine data transfer, and three-party data sharing. A data security testing method for ICVs based on typical scenarios is proposed. This work contributes to discover and solve data security issues in ICVs and provides strong support for the security of ICVs.",10.1109/ICITE59717.2023.10733896,H. Ji; J. Wang; L. Wang; G. Sun; L. Jin; J. Fang
Real-Time Clickstream Data Processing and Visualization Using Apache Tools,2023,1,Topic_1_data_big_big data,0.8882554608395201,"In the modern digital generation, clickstream data analytics is a crucial component of online selling and purchasing platforms. It offers useful information for enhancing user experience, conversion rates, website design, and keeping track of the success of digital marketing efforts. A data warehouse is required to store and analyze clickstream data, which requires a lengthy and complex ETL process. Two of the leading large data processing tools and distributed streaming systems are Spark and Kafka. This paper presents a comprehensive examination of recent research pertaining to clickstream data analytics, real-time data visualization, data lifecycle management utilizing Apache technologies. Based on the review, the paper proposes an architecture that can aid organizations and local merchants in effectively visualizing the performance of their products. This paper provides a valuable resource for researchers and practitioners seeking to explore the latest developments in their future work.",10.1109/ICCUBEA58933.2023.10392270,T. B. Patil; K. Anand; A. Bhateja; K. Jamal; S. T. Sawant-Patil; P. Paygude
Circular Economy: Challenges and Potentials for the Manufacturing Industry by Digital Transformation,2019,3,Topic_3_industry_manufacturing_chain,0.8842819783425308,"The world is steering towards a situation, where resource scarcity will result in cost explosion of material and governments are overstrained to dispose the constantly accumulating amount of waste in their countries. One way to overcome the resulting challenges is the implementation of a circular system, in order to create a closed material cycle and eliminate the production of further waste. The ongoing digital transformation offers the potential to enable a transition towards a circular economy. Therefore, recent studies have been examined, which advocate that digital technologies are key enabling factors to realize the shift from a linear to a circular system. Despite this general agreement, the literature still neglects how digital technologies enable such a circular economy transition for the manufacturing industry. To fill the gap, this paper presents a research framework comprising nine success factors, based on digital transformation technologies. The research framework is composed of two dimensions. The first dimension describes the three phases (manufacturing, usage and reutilization/recycling) of a product lifecycle. The second dimension depicts the design levels of business engineering (strategic-, system- and information level). Therefore, the nine success factors have been described for each level of the framework along the product lifecycle in the context of digital transformation. Thus, the paper gives recommendations to overcome the challenges of manufacturing companies concerning the realization of a circular economy.",10.1109/TEMS-ISIE46312.2019.9074421,M. Riesener; C. Dölle; C. Mattern; J. Kreß
Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective,2024,2,Topic_2_data_privacy_security,1.0,"The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.CCS CONCEPTS• Software and its engineering Software architectures; • Information systems World Wide Web; • Security and privacy Privacy protections; • Social and professional topics Copyrights; • Computing methodologies Machine learning.",,D. Zhang; B. Xia; Y. Liu; X. Xu; T. Hoang; Z. Xing; M. Staples; Q. Lu; L. Zhu
Research on Privacy Security Risk Evaluation of Intelligent Recommendation Mobile Applications Based on a Hierarchical Risk Factor Set,2019,2,Topic_2_data_privacy_security,1.0,"Intelligent recommendation applications based on data mining have appeared as prospective solution for consumer's demand recognition in large-scale data, and it has contained a great deal of consumer data, which become the most valuable wealth of application providers. However, the increasing threat to consumer privacy security in intelligent recommendation mobile application (IR App) makes it necessary to have a risk evaluation to narrow the gap between consumers' need for convenience with efficiency and need for privacy security. For the previous risk evaluation researches mainly focus on the network security or information security for a single work, few of which consider the whole data lifecycle oriented privacy security risk evaluation, especially for IR App. In this paper, we analyze the IR App's features based on the survey on both algorithm research and market prospect, then provide a hierarchical factor set based privacy security risk evaluation method, which includes whole data lifecycle factors in different layers.",10.1109/ICMCCE48743.2019.00148,Q. Tu; Y. Jing; W. Zhu
Data engineering case-study in digitalized manufacturing,2021,-1,Outliers,0.3218652307341149,"The combination of big data and machine learning appears in the manufacturing context frequently. In a modern factory, data is collected everywhere. It is a challenge for the companies, finding their way to use the produced data. The model's quality is strongly dependent on the quality of the training dataset; the data engineer is responsible for the infrastructure, like providing context and quality input-data for machine learning algorithms. In the discussed case-study, a data pipeline is introduced as a potential solution. It proposes a strategy through the organization, from the shop floor to decision- makers.",10.1109/SAMI50585.2021.9378691,I. Pölöskei
Automated metadata extraction: challenges and opportunities,2022,1,Topic_1_data_big_big data,0.5196133323174995,"Proper application of the FAIR data principles is what separates a vibrant data ecosystem, in which research data are frequently shared and reused, from a lifeless data graveyard. Automated metadata extraction systems have been proposed as a means of bolstering the findability, interoperability, and reusability of data repositories with little or no human intervention. These extraction systems mine metadata by crawling a repository and applying lightweight extractors that, for various types of file (e.g., image, CSV file), extract or synthesize relevant attributes. In practice, however, the automated creation of generally useful metadata is fraught with challenges. Data consumers may have different perspectives as to what metadata representations are useful, the standards for recording metadata tend to change over time, and the software model for processing updates can introduce unnecessary human and computational effort. Thus, generalizing extraction for a broad audience of data consumers is a difficult and relatively unsolved problem. In this work, we explore these challenges faced by extraction systems in the context of constructing our own extraction system for science data. We first define the metadata extraction problem and provide context to the issues faced in generalizing metadata. Additionally, we identify potential research directions to help alleviate many of these challenges for all automated extraction systems. Ultimately, this work represents a first step in designing Ubiquitous metadata extraction systems that can maximize the value of research data while minimizing the human efforts required in doing so.",10.1109/eScience55777.2022.00088,T. J. Skluzacek; K. Chard; I. Foster
Management of Disruptive Technologies as Applied in Stages of Long-term Insurance Processes,2024,3,Topic_3_industry_manufacturing_chain,0.7254008965273622,"The long-term insurance industry is undergoing a transformative shift driven by disruptive technologies, presenting both challenges and opportunities for insurers. This paper explores the strategic management of disruptive technologies within the value chain stages of long-term insurance processes. The study investigates how an insurer effectively navigated this dynamic landscape to enhance operational efficiency, customer engagement, and overall competitiveness. The paper delves into the identification and categorization of disruptive technologies affecting various stages of the long-term insurance value chain, including product development, underwriting, policy administration, claims processing, and customer service. Drawing on a case study from a South African perspective, it provides a comprehensive analysis of how emerging technologies such as big data analytics, websites, social networks, digital platforms, robo-advice, application programme interface (API), robotic process automation (RPA), chatbots, voice recognition, artificial intelligence (AI), blockchain, telematics, and InsurTech solutions impact stages of the value chain. Furthermore, the paper outlines a strategic framework for insurers to proactively integrate and manage disruptive technologies into their existing processes. It emphasizes the importance of fostering a culture of innovation, establishing agile organizational structures, and nurturing cross-functional collaboration to swiftly adapt to technological advancements. The study explores potential challenges associated with the integration of disruptive technologies, such as data security concerns, regulatory compliance, and the need for upskilling the workforce. It offers insights into mitigating these challenges through robust risk management strategies, regulatory compliance frameworks, and targeted workforce development initiatives. In conclusion, the study provides a roadmap for long-term insurers to not only survive but thrive amidst the disruptions caused by emerging technologies. It encourages a forward-thinking approach to technology adoption, positioning disruptive innovations as enablers of growth and sustainable competitive advantage in the rapidly evolving landscape of long-term insurance.",10.23919/PICMET64035.2024.10653296,T. Moloi; A. F. Mulaba-Bafubiandi
Complements: Biotechnology and technobiology,2016,3,Topic_3_industry_manufacturing_chain,0.3774196683803229,"Engineering has and will continue to have a critical impact on healthcare; the application of technology-based techniques to biological problems can be defined to be technobiology - thus, it can be considered to be the complement to biotechnology, an all too encompassing and overused term. In addition to detailing the scope of technobiology, this paper expands on the technobiology approach of service systems engineering to the development of a healthcare service system that is integrated, adaptive and evidence-based; focuses on a range of example applications in regard to the technobiology areas of information, instrumentation, and insertion; underscores, as an example, the collaborative technobiology efforts between the College of Engineering and the Miller School of Medicine at the University of Miami; and concludes with some additional insights.",10.1109/WAC.2016.7582989,J. M. Tien; P. J. Goldschmidt-Clermont
Recommendation on Cybersecurity and Safety in the Hydrogen Economy,2024,2,Topic_2_data_privacy_security,0.21056898561701726,"This study examines the cybersecurity implications of the hydrogen economy, analyzing cyber incidents since 2010. It suggests a cybersecurity framework based on Internet of Things (IoT) and Edge computing to enhance infrastructure resilience against evolving threats. The digital transformation of energy systems, including smart grids and meters, has provided efficiency and clean energy benefits but also heightened cybersecurity risks. The research aims to bridge the cybersecurity gap in the hydrogen economy while acknowledging historical safety concerns related to hydrogen handling. Cyberattacks in the energy sector since 2010 have seen a significant upsurge, particularly peaking between 2020 and 2022. The study categorizes common cybersecurity threats, motivations, types, and consequences, and proposes an IoT model and edge computing framework to enhance cybersecurity defenses. This interdisciplinary approach integrates cybersecurity, IoT, Edge computing, and hydrogen technologies to address pressing challenges. Drawing insights from past incidents, this research equips stakeholders to secure the future of the hydrogen economy, emphasizing the dual role of IoT systems in increasing cyber threats and fortifying cybersecurity in this expanding energy sector.",10.1109/MIPRO60963.2024.10569632,R. Alfasfos; M. Ullah; J. Sillman; P. Nardelli; R. Soukka
A Digital Management System for Equipment Inventory Maintenance,2025,3,Topic_3_industry_manufacturing_chain,0.6140019291643946,"To overcome the persistent limitations inherent in conventional paper-based equipment maintenance systems and fragmented data management practices, a Digital Equipment Inventory Maintenance System is proposed. Centralized data governance is achieved through a meta-model-driven architecture that enforces standardized data acquisition protocols and automated validation procedures. Then this system establishes an integrated framework for comprehensive data lifecycle management, incorporating automated Extract-Transform-Load (ETL) pipelines with schema-mapping mechanisms to harmonize multi-source data streams. Cross-platform interoperability is enabled via WebGL-based digital-twin rendering, which facilitates interactive three-dimensional visualization of equipment inventories. A three-layer modular design - comprising Data Acquisition, Processing & Analytics, and Situational Awareness Dashboard - addresses the critical gap between industrial maintenance requirements and Industry 4.0 standards. Validation through industrial pilot cases demonstrates significant improvements in decision-making efficiency and error mitigation.",10.1109/AEMCSE65292.2025.11042572,X. Liu; Y. Zheng; Z. Liu; G. Lai
"Model and Data Driven Complex Product Development: from V, Double Vs to Triple Vs",2019,3,Topic_3_industry_manufacturing_chain,0.2647300735971339,"Complex product development framework and methodology is evolving with the development of industrial technology, information technology and management technology. The traditional product development framework and methodology is a waterfall style, which is usually described as the V model. In the V model, the design result of the earliest stage will be validated, verified and accredited at the last stage. In order to reduce the cycle of verification, validation and accreditation (VV&A) and increase the probability of successful design within fewest iterations, model-based systems engineering (MBSE) proposes a design information transferring methodology based on digital models, which extends the V model to double Vs model. The development of big data technology and data-driven machine learning provides an opportunity to converge data lifecycle, model lifecycle and product development lifecycle together. The paper reviews product development models from V to double Vs, and presents a model and data driven triple Vs model for product development in the big data era.",10.1109/ICICAS48597.2019.00185,Q. Li; H. Wei; C. Yu; S. Wang
Research on Big Data Security and Privacy Risk Governance,2021,2,Topic_2_data_privacy_security,0.3312553678187512,"In the era of Big Data, opportunities and challenges are mixed. The data transfer is increasingly frequent and speedy, and the data lifecycle is also extended, bringing more challenges to security and privacy risk governance. Currently, the common measures of risk governance covering the entire data life cycle are the data-related staff management, equipment security management, data encryption codes, data content identification and de-identification processing, etc. With the trend of data globalization, regulations fragmentation and governance technologization, “International standards”, a measure of governance combining technology and regulation, has the potential to become the best practice. However, “voluntary compliance” of international standards derogates the effectiveness of risk governance through this measure. In order to strengthen the enforcement of the international standards, the paper proposes a governance approach which is “the framework regulated by international standards, and regulations and technologies specifically implemented by national legislation.” It aims to implement the security and privacy risk governance of Big Data effectively.",10.1109/ICBAR55169.2021.00011,X. Wang; W. Luo; X. Bai; Y. Wang
Interoperable processes and infrastructure for the digital transformation of the quality infrastructure,2021,3,Topic_3_industry_manufacturing_chain,0.5566218763191356,"Many organisations are currently facing the challenge of transforming complex processes into digital equivalents whilst maintaining the openness and opportunity for future development. An important step in this transformation is process analysis to first identify the sub-processes (so-called process modules) involved and the associated data required for subsequent implementation. Together with the interconnection and combination of heterogeneous data sources which play an important role in the course of the digital transformation, reliable metadata also plays a decisive role for the consistent usability of data. Therefore, a goal of an effective digital transformation must include a standardised, machine-interpretable information management strategy that is applicable throughout the entire data lifecycle. By partioning the processes into digital building blocks and choosing suitable digital interfaces, this system of interoperable processes is constantly expandable across different application areas. Ideally, this is carried out with consideration to standards and guidelines that have far-reaching scope and general validity. This paper shows how individual processing steps can be combined into a harmonious overall digital process through the suitable selection of IT infrastructure and knowledge of the necessary data formats. This is demonstrated using the example of the digital transformation of metrological services at the German National Metrology Institute PTB. Furthermore, an outline is provided to demonstrate how this approach translates to an interoperable digital quality infrastructure.",10.1109/MetroInd4.0IoT51437.2021.9488563,A. Keidel; S. Eichstädt
"Extracting, Transforming, and Analyzing Olympic Data Using Azure Services: A Case Study",2024,1,Topic_1_data_big_big data,0.7639325241024317,"A field of investigation and the foundation of cloud computing are employed to administer a common resource pool. Analytics techniques are required in educational mining because of the massive amount of data needed to obtain relevant knowledge. As digital data repositories grow and dispersion, sophisticated data analysis techniques are required to quickly extract relevant knowledge from them. A cloud platform created as a service pipeline for creating and implementing distributed data analytics applications. In this scenario, we make use of learning models, data sets, analysis instruments, and data mining techniques that can be deployed as stand-alone services that can run on clouds. In this study the central focus lies in establishing a robust data pipeline utilizing Azure services, specifically targeting the Tokyo Olympic dataset obtained from Kaggle. The initial phase involves the extraction of relevant information from the Kaggle dataset using Azure Data Factory, a cloud-based integration service. Following this, Azure Databricks, featuring Apache Spark functionality, is employed for the transformation of the Tokyo Olympic data. The primary objective remains to enable advanced analytics, focusing on the Tokyo Olympic dataset. The utilization of Azure Synapse Analytics empowers organizations to perform intricate SQL queries on the Tokyo Olympic data, revealing latent trends and correlations. The outcomes can be visualized through SQL-based approaches in conjunction with Azure Synapse Analytics, providing a comprehensive understanding of the Tokyo Olympic dataset for well- informed decision-making.",10.1109/ACROSET62108.2024.10743877,D. Samanta; S. Roy; P. Sarkar; B. Misra
Target Architecture of Distribution Network Monitoring System towards Energy Revolution and Digitization,2020,-1,Outliers,0.20906773819184157,"In order to adapt to the energy revolution and digitization, a new design for the distribution network monitoring system is proposed to achieve data lifecycle management. Firstly, the current practices and limitations of existing monitoring systems are presented. According to the development trend, the reconstruction strategy is presented involving three feedback local-computing loops and data interaction between distributed intelligent agents and cloud-based master station. To support the huge scale of sensed data integration and transmission, the reference framework of data interaction is established, and the main monitoring information is presented. Then some technical evolution trends towards the new architecture are described in the fields of unified model, new generation of intelligent terminal, network interconnection, edge-cloud computing and security system.",10.1109/iSPEC50848.2020.9351196,J. Lin; P. Wang; J. Zhang; S. Guo; H. Sun
DLIO: A Data-Centric Benchmark for Scientific Deep Learning Applications,2021,1,Topic_1_data_big_big data,0.5330882537532661,"Deep learning has been shown as a successful method for various tasks, and its popularity results in numerous open-source deep learning software tools. Deep learning has been applied to a broad spectrum of scientific domains such as cosmology, particle physics, computer vision, fusion, and astrophysics. Scientists have performed a great deal of work to optimize the computational performance of deep learning frameworks. However, the same cannot be said for I/O performance. As deep learning algorithms rely on big-data volume and variety to effectively train neural networks accurately, I/O is a significant bottleneck on large-scale distributed deep learning training. This study aims to provide a detailed investigation of the I/O behavior of various scientific deep learning workloads running on the Theta supercomputer at Argonne Leadership Computing Facility. In this paper, we present DLIO, a novel representative benchmark suite built based on the I/O profiling of the selected workloads. DLIO can be utilized to accurately emulate the I/O behavior of modern scientific deep learning applications. Using DLIO, application developers and system software solution architects can identify potential I/O bottlenecks in their applications and guide optimizations to boost the I/O performance leading to lower training times by up to 6.7x.",10.1109/CCGrid51090.2021.00018,H. Devarajan; H. Zheng; A. Kougkas; X. -H. Sun; V. Vishwanath
Ray Tracing -driven System of Virtual Fitting Based on Trustworthy Identification of IIoT,2022,-1,Outliers,0.155044615981297,"Virtual fitting system is a promising direction for artificial intelligence (AI) application in the Industrial Internet of things (IIoT). However, existing research is beyond the overall links between IIoT and real-world experience. In this paper, the Product Lifecycle Management (PLM) data is integrated through the identification of IIoT to realize a digital twin system. The entire process involves multiple production companies with various PLM systems through identification. The system introduces dynamic ray tracing to imitate fitting effect in the way of rendering where the whole process of monitoring makes the traceability of the product simple and reliable. In addition, the system has an AI analysis model of user preference with a selection interface, which can build the virtual fitting model according to the users' wishes. The design help users have a more immersive shopping experience. Finally, consumers' desire is stimulated to prove the feasibility and effectiveness of the virtual fitting system.",10.23919/CCC55666.2022.9902193,X. Gong; Y. Wang; J. Xu; C. Chi; Z. Wang
Industry 4.0 Implementation in Sri Lankan Manufacturing Firms: A Lean Perspective,2023,3,Topic_3_industry_manufacturing_chain,0.7078879112690301,"Manufacturing industries require the highest quality and efficiency throughout their value chain, to compete with countries having a labor cost advantage. Today, manufacturing firms are in a fast-phased run to automate their processes and increase value chain integration through advanced technologies. Industry 4.0 has gained traction within this community, where its components like IoT, Big data, and Cloud computing are being used by manufacturing firms to optimize and increase the efficiency of their workplaces. Obtaining the proper outcomes from these advanced technologies has been an issue for most of its users. Very few studies were found in the literature, that propose ways to mitigate the issues faced by these companies in their Industry 4.0 journey. Lean concepts are a popular and proven methodology used by firms worldwide to decrease the complexity and increase the productivity of their processes. Based on a systematic literature review, the study identifies the current knowledge on mitigating the barriers faced by manufacturing firms in Industry 4.0 implementations. To address the knowledge gap identified in the literature review, the study proposes and statistically tests a framework, on how the manufacturing environment can be improved to obtain the expected outcomes of Industry 4.0 implementations, through a lean theoretical lens. Thus, the stakeholders of the company can contribute towards successful implementations of Industry 4.0 while organizational processes are being standardized and optimized to integrate these advanced technological shifts.",10.1109/SCSE59836.2023.10215031,L. Bandara; A. Withanaarachchi; S. Peter
LIoPY: A Legal Compliant Ontology to Preserve Privacy for the Internet of Things,2018,2,Topic_2_data_privacy_security,0.32080505399957004,"The Internet of Things (IoT) provides the opportunity to collect, process and analyze data. This opportunity helps to understand preferences and life patterns of individuals in order to offer them customized services. However, privacy has become a significant issue due to the personal nature of the knowledge derived from these data and the involved potential risks. Despite the increasing legislation pressure, few proposed solutions have dealt with the privacy requirements, such as consent and choice, purpose specification, and collection limitation. In this paper, we propose a privacy ontology in order to incorporate privacy legislation into privacy policies while considering several privacy requirements. Our proposed ontology aims both at making the smart devices more autonomous and able to infer data access rights and enforcing the privacy policy compliance at the execution level. We implemented and evaluated our privacy ontology based on a healthcare scenario.",10.1109/COMPSAC.2018.10322,F. Loukil; C. Ghedira-Guegan; K. Boukadi; A. N. Benharkat
Cloud-based Data Protection: A Framework for Authorizing Data Movement,2024,2,Topic_2_data_privacy_security,1.0,"As organizations increasingly migrate their data operations to cloud environments and harness the power of big data analytics, ensuring the security and privacy of data in motion becomes paramount. This research study outlines a comprehensive cloud authorisation framework explicitly designed to address the unique challenges posed by the dynamic nature of big data processing in cloud environments. The proposed prototype / framework integrates advanced authentication and authorisation mechanisms tailored to the characteristics of big data systems, including distributed processing, heterogeneous data sources, and diverse user access patterns. Leveraging scalable cloud infrastructure and advanced cryptographic techniques, the framework ensures granular access control and data protection throughout the data lifecycle. Designing a comprehensive Cloud Authorization Framework for Data in Motion within Big Data entails addressing numerous key components and considerations. A broad framework design is shown in this research study. It includes mechanisms for authentication and authorisation, multi-tenancy management, continuous monitoring and remediation, working with third-party providers, educating and raising awareness among users. By integrating these components, organisations can establish a robust security infrastructure capable of safeguarding sensitive data in cloud environments.",10.1109/ICOECA62351.2024.00057,M. Z. Khan; K. U. Nisa; M. T. Quasim; M. A. Khalifa; M. M. Mobarak
Data Security of New Power System: Challenges and Countermeasures,2024,2,Topic_2_data_privacy_security,0.44152564431278774,"With the rapid development of the new power system, the issue of data security has become increasingly prominent. This article first analyzes the characteristics of data in the new power system, and then explores the major challenges currently faced. Based on this, the article proposes corresponding countermeasures, establishing a risk early warning and defense mechanism for the entire data lifecycle. Finally, this article summarizes the current research results and looks forward to future research directions.",10.1109/IMCEC59810.2024.10575532,M. Li; J. Shen; F. Li
SLICES Data Management Infrastructure for Reproducible Experimental Research on Digital Technologies,2023,1,Topic_1_data_big_big data,0.6411778228329258,"This paper presents the ongoing research effort related to the design of the Data Management Infrastructure (DMI) to support experimental research on digital technologies with application to the ESFRI SLICES scientific instrument. We consider the experiment documentation and data collection across the whole continuum of access network, IoT, edge, cloud, and data processing workflow. The paper includes the requirements analysis for DMI to enable research reproducibility of complex and large-scale experimentation. We provide an analysis of data collected and processed in SLICES and explain approaches and solutions used in SLICES for experimental research reproducibility, primarily based on the plain orchestration service and supported by metadata collection tools. The proposed multi-layer DMI includes: data (storage) access, data processing, data ingest, experiment management, and virtual research environment. The paper also provides recommendations for the selection of existing standards and tools for data and metadata management, in particular those developed by EOSC and supported by the RDA community to ensure wide compatibility and integration.",10.1109/GCWkshps58843.2023.10464944,Y. Demchenko; S. Gallenmüller; S. Fdida; T. Rausch; P. Andreou; D. Saucez
An Automatically Privacy Protection Solution for Implementing the Right to Be Forgotten in Embedded System,2022,2,Topic_2_data_privacy_security,1.0,"Towards the massive amount of data generated in our daily work and life, embedded systems, with economical but powerful storage and computing resources, are inevitably becoming the most suitable platform for the Edge Computing for the Internet of Things. However, embedded system servers may also threaten individuals by storing individuals’ private data for years. This paper proposes a Resilient Tag-based Privacy Protection (RTPP) scheme for embedded systems. Specifically, to protect the privacy against the hackers and other non-users, we employ a pseudo-random number encryption technique with the chaos-based principle so that the third party cannot easily steal the private data and reduce the risk of personal privacy leakage. To protect the individuals’ interests, we propose a new approach to controlling the life cycle table of data to enable individuals themselves the flexibility to control the life cycle of private data. Unlike existing data lifetime management methods, the RTPP can support the retrieval of tags in the data life cycle table to control the corresponding privacy while automatically adding or removing tags. Our system automatically adjusted the survival period of private data in the life cycle table through the change of leaf weights, controlled the charge movement on the surface of flash memory, and finally achieved the resilient adjustment process of the life cycle of private data in the embedded system. The security proof and performance evaluation show that the proposed RTPP scheme is provable secure in the automatic privacy lifecycle tuning model for embedded systems and efficient in practice.",10.1109/ACCESS.2022.3162238,Y. Zhao; N. Si; Y. Sun; X. Gao; H. Tong; G. Yuan
Design and Implementation of a Real-time IoT Solution for Smart Meter Data Analysis in Microsoft Azure,2024,1,Topic_1_data_big_big data,0.7792425544629636,"This research presents an end-to-end IoT solution designed as a project, not a traditional lab setup. It simulates high-velocity data generation from smart meters and focuses on real-time data visualization within Azure’s cloud ecosystem. An integral part of this project is an AI-based spike and dip anomaly detection system. The project creates an IoT ecosystem that replicates smart meter telemetry data. It utilizes Microsoft Azure’s robust services to cover the entire IoT data lifecycle, from data ingestion to real-time visualization. Key project components involve Azure resource provisioning, IoT Hub setup, and the deployment of a Device Provisioning Service. The project implements a lambda architecture, filtering and processing a subset of telemetry data in real-time. This data undergoes immediate visualization, showcasing Azure’s agility in handling high-velocity streams. This paper is a valuable reference for interests in implementing IoT solutions with Microsoft Azure, particularly emphasizing real-time data processing and visualization within a lambda architecture while incorporating AI-based anomaly detection for spikes and dips.",10.1109/ICCCNT61001.2024.10725804,S. D. BhavaniPeri; A. Ravi; M. Supriya
From Creation to Exploitation: The Oracle Lifecycle,2024,-1,Outliers,0.2820936341816764,"Decentralized Finance (DeFi) systems leverage blockchain oracles to access off/on-chain data as a service. Therefore, maintaining the integrity of oracle data is essential. However, the integrity of these oracles data can be compromised through different attacks, and the effectiveness of these attacks varies depending on the specific stage of the oracle's lifecycle. This work presents a comprehensive analysis of this lifecycle, identifying potential attack types and examining the efficacy of existing defense mechanisms. We propose a generalized model encompassing data creation, submission, consensus, election, and deprecation stages. We evaluate our model against seven recent high-profile DeFi exploits totaling $187 million. We have also studied bond systems as a preventive measure against at least a subset of oracle exploits. Our findings suggest that while bond systems increase the cost of attacks, thereby fortifying oracle data integrity against adversarial manipulations, they also require careful calibration to avoid hindering honest participation.",10.1109/SANER-C62648.2024.00009,M. Eshghie; M. Jafari; C. Artho
mAgriculture among small holder farmers in Kenya: Challenges and lessons,2015,-1,Outliers,0.4195334846876936,"The number of mAgriculture apps have steadily increased over the last 10 years in Kenya. The availability of mobile technology among a large portion of the population has created an opportunity to deploy mobile-based services among small holder farmers. These platforms have been deployed in different areas of the country targeting small holder farmers practicing various forms of agriculture including crop and livestock farming. These services range from agriculture advisory information, record keeping, diagnosis services, market information and veterinary services among others. These services are availed on a range of mobile devices including basic phones, feature phones, smart phones and other hand-held computing devices. Different technologies have been used to develop these innovations such as SMS, USSD, IVR, mobile web and downloadable apps. We set out to establish the uptake and use of mAgriculture among the small holder farmers through FGDs and interviews with small farm holder farmers with a diverse and representative sample. Here we report our findings. We identified the challenges and lessons learnt at different stages of their implementation. It was discovered that only a few of these implementations have been widely adopted, as various challenges have hampered their growth beyond the pilot stage. However, some of the mAgriculture innovations have been successful and offer key lessons that other stakeholders should adopt for similar attempts.",10.1109/AFRCON.2015.7331924,A. Gichamba; P. Waiganjo; D. Orwa
Towards a Linked Data Publishing Methodology,2016,1,Topic_1_data_big_big data,0.4702306233380678,"Linked open government data (LOGD) can be a catalyst in the development of value-added services and products. The vision of many Linked Open Data (LOD) projects is to make publishing and reuse of linked data as easy as possible for the end user thanks to a thriving marketplace with data publishers, developers, and consumers along the value chain. In the large scale LOD project ""Fusepool P3"", tourism-related applications and software components were developed that support data owners and open data enthusiasts in transforming legacy data to linked data. Based on experiences from this project, we present reflections and discuss pitfalls in drawing a linked data publishing methodology. An integrated view on all phases of the publishing process has not been described so far, for the technical phases linked data life-cycles have been identified only. The methodology developed enables stakeholders to transfer the lessons learned to other use cases and application contexts. This allows for better estimation of efforts and skills for future LOD projects.",10.1109/CeDEM.2016.12,E. Klein; A. Gschwend; A. C. Neuroni
Method for process modelling and analysis with regard to the requirements of Industry 4.0: An extension of the value stream method,2017,3,Topic_3_industry_manufacturing_chain,0.8173230178967775,"Digitalization and Industry 4.0 create new opportunities and challenges in production and logistics. This also applies to methods for process modelling and analysis. The following paper discusses the possibilities, chances and limitations of the value stream method in this context. For this purpose, requirements regarding process modelling and analysis in digitalization and Industry 4.0 have been systematized by a criteria catalogue. Different extensions of the value stream method are described, discussed and evaluated in order to depict deficits in the current methods. Furthermore, an approach for the visualization of Industry 4.0 systems at the conceptual stage of Industry 4.0 projects has been developed, based on the value stream method. A case study evaluation shows possibilities of this approach.",10.1109/IECON.2017.8216677,M. Lewin; S. Voigtländer; A. Fay
The Healthcare Data Management Maturity Model (HDM3) for the assessment of healthcare data management in developing countries,2022,1,Topic_1_data_big_big data,0.2974540570367129,"Developing countries struggle with a range of healthcare data management challenges. The scope of the challenges is broad and it is difficult to determine on which components of healthcare data management to focus when endeavoring to improve healthcare data management systems in developing countries. This study develops, presents and evaluates a maturity model, the Healthcare Data Management Maturity Model (HDM3), that healthcare entities can incorporate to assist them in determining the as-is state of their healthcare data management. This identifies the components to be focused on during improvement endeavors. The conceptual HDM3 and its various components are presented in this study.",10.1109/ICE/ITMC-IAMOT55089.2022.10033243,L. Van der Merwe; I. De Kock; W. Bam
The Impact of Digital Platforms and Ecosystems in Healthcare on Value Creation—A Integrative Review and Research Agenda,2023,3,Topic_3_industry_manufacturing_chain,0.5411564882407552,"Enabled by technological innovations and the pressure to modernize healthcare systems, the digital transformation is fundamentally changing the healthcare sector. It facilitates strategic alignments that drive significant changes to the creation of value of organizations, as well as to their stakeholders in the sector. One manifestation of this development is the advancement of digital platforms and ecosystems into the sector, impacting established practices. Because of the sector’s low level of platformization, discussions in healthcare have yet to profoundly engage with the impact of these digital architectures. This study provides insights on the ways digital platforms and ecosystems are changing the nature of value creation in healthcare. Based on a literature review, four distinctive changes to the value creation of the sector are identified. Based on the results, lessons from other domains, and the specific dynamics of technical advancements at the intersection of information science and healthcare, an agenda for future research is proposed. This agenda takes social and ethical discussions into consideration and is intended to guide academic discussions to further leverage digital formats to foster collaboration, cooperation and coopetition.",10.1109/ACCESS.2023.3336983,J. Konopik
"The ExaNeSt Project: Interconnects, Storage, and Packaging for Exascale Systems",2016,1,Topic_1_data_big_big data,0.7748685111130057,"ExaNest is one of three European projects that support a ground-breaking computing architecture for exascale-class systems built upon power-efficient 64-bit ARM processors. This group of projects share an ""everything-close"" and ""share-anything"" paradigm, which trims down the power consumption -- by shortening the distance of signals for most data transfers -- as well as the cost and footprint area of the installation -- by reducing the number of devices needed to meet performance targets. In ExaNeSt, we will design and implement: (i) a physical rack prototype and its liquid-cooling subsystem providing ultra-dense compute packaging, (ii) a storage architecture with distributed (in-node) non-volatile memory (NVM) devices, (iii) a unified, low-latency interconnect, designed to efficiently uphold desired Quality-of-Service guarantees for a mix of storage with inter-processor flows, and (iv) efficient rack-level memory sharing, where each page is cacheable at only a single node. Our target is to test alternative storage and interconnect options on actual hardware, using real-world HPC applications. The ExaNeSt consortium brings together technology, skills, and knowledge across the entire value chain, from computing IP, packaging, and system deployment, all the way up to operating systems, storage, HPC, big data frameworks, and cutting-edge applications.",10.1109/DSD.2016.106,M. Katevenis; N. Chrysos; M. Marazakis; I. Mavroidis; F. Chaix; N. Kallimanis; J. Navaridas; J. Goodacre; P. Vicini; A. Biagioni; P. S. Paolucci; A. Lonardo; E. Pastorelli; F. L. Cicero; R. Ammendola; P. Hopton; P. Coates; G. Taffoni; S. Cozzini; M. Kersten; Y. Zhang; J. Sahuquillo; S. Lechago; C. Pinto; B. Lietzow; D. Everett; G. Perna
STAT Intelligence: Data Management and Monitoring Platform for BCG Economy Model,2023,1,Topic_1_data_big_big data,0.5455789279955466,"Leveraging open government data is crucial as it promotes transparency, encourages data reuse, engages citizens, and supports data-driven initiatives, especially for economic growth. Monitoring the progress of economic development and managing related datasets are among the challenging tasks faced by the government. To measure the success of economic growth, it is necessary to create a system for monitoring and assessing progress toward predefined targets using action plans and corresponding indicators. This paper presents STAT Intelligence, an innovative statistical service platform that manages, monitors, and assesses indicators in a case study of the BCG economy model. In this study, the management and analysis of datasets are based on 14 BCG indicators, which are specified in the BCG Action Plan 2021–2027. These indicators are used to assess the BCG economy’s progress at the national and provincial levels. Our platform showcases statistical data pertaining to BCG indicators on a dashboard, allowing users to monitor operational progress based on the provided data. Moreover, knowledge graphs are constructed to gather the relationships between BCG indicators and official statistic datasets, which are then represented in the form of data trees. The results aid users in understanding the relationships and facilitate easy tracking of significant datasets associated with each indicator. Consequently, our platform’s services enable users to publish datasets on the GD catalog and track vital datasets pertaining to the nation’s economic development based on the BCG economic model. This contributes to the ability of country and provincial executives to adjust their plans and key performance indicators (KPIs) and achieve a complete set of national and provincial indicators.",10.1109/iSAI-NLP60301.2023.10354544,P. Krataithong; M. Buranarach; P. Tumsangthong; T. Wutthitasarn
Short review: Application Areas of Industry 4.0 Technologies in Food Processing Sector,2018,3,Topic_3_industry_manufacturing_chain,0.3690572485665367,"The technological revolution or known as Industry 4.0, is a paradigm that envisages the use of sensors, machines, workpieces and IT system that is connected to the value chain beyond a single organization. As in other industries, the food processing sector is expected to embrace Industry 4.0 progressively. This paper presents the nine technological advancements that drive Industry 4.0 namely Big Data and analytics, autonomous robots, simulation, horizontal and vertical integration, cybersecurity, the Industrial Internet of Things (IIoT), the cloud, augmented reality and additive manufacturing. Additionally, the paper reviews their application areas in the food sector. These include intelligent manufacturing, food safety, training, marketing and other functions commonly found in the food industry. The advantages such as automated tasks, cost reduction, systematic management, compliance with standard and resource efficiency are presented. Yet, it is understood that new issues also arise especially when considering the common small- and medium-scaled food processors that are plagued with capital, skill, know-how and technological constraints. Hence, future studies are recommended on areas related to cybersecurity, modified workforce profile, user-friendly interface requirement, applicable concept for the small companies and their readiness for the technology.",10.1109/SCORED.2018.8711184,N. Z. Noor Hasnan; Y. M. Yusoff
Intelligent Robotics for Sorting Clothing and Textile Waste Using Cloud-Integrated Recycling Systems and CNN,2024,-1,Outliers,0.2619137767641524,"The textile industry has a significant problem dealing with massive quantities of textile and clothing waste. To address this, this paper introduces a new method for automating textile waste sorting using Convolutional Neural Networks (CNNs), cloud-integrated recycling systems, and intelligent robots. Using CNNs, our proposed system combines advanced robotics with sophisticated computer vision capabilities. Using characteristics like fabric type, color, and condition, robots can sort textile waste effectively. To further facilitate scalability and flexibility in dealing with high waste quantities, the system uses cloud computing infrastructure to process data in real time and make decisions. We lay forth the plan for the system and how it will work, including the robotic sorting mechanism, CNN s models for item identification, and the cloud-based integration that will make everything run smoothly. It conducts experimental trials to assess the system's performance, showing that it efficiently sorts textile waste, reduces human work, and improves recycling efficiency. The results show great potential in solving the problems of textile waste management. This might lead to a more sustainable and eco-friendly strategy in the textile sector.",10.1109/ICSTSDG61998.2024.11026397,C. S. Ranganathan; K. S; S. P. Vimal; V. Kannagi; M. Muthulekshmi; M. Meganathan
Understanding the Student Dropout in Distance Learning,2019,-1,Outliers,0.1688110232751042,"This Research to Practice Full Paper introduces an approach for the early identification of students at risk of dropping out of their distance learning courses. Students dropout in university courses have long been a major issue leading to social, academic and financial impacts. Predictive modelling analysis has been used as a tool to identify at an early stage probable cases of dropout. In distance learning, this type of analysis is made possible mostly because of the increasing adoption rate of Virtual Learning Environments (VLEs) where data on the student academic activities can be continuously recorded. This paper discusses the design and validation of a Learning Analytics system for early identification of students at risk of dropping out. The case study presented relies on data collected from two postgraduate courses as part of Brazils Open University. The methodology for the system design and validation comprises the steps to build an intelligent data pipeline. Jupyter Notebooks have been d used as the data science analysis environment in order to create data pipelines and have their performance evaluated. Results obtained from the validation of models built out of eight machine learning techniques show an average accuracy of 84% among the set of ML techniques tested. The highest accuracy is delivered by the Extra Trees classifier with 88%. Logistic Regression performed the worst performance with an accuracy of 79%.",10.1109/FIE43999.2019.9028433,M. M. d. Oliveira; R. Barwaldt; M. R. Pias; D. B. Espíndola
Datawarehouse design for educational data mining,2016,-1,Outliers,0.38473802130859247,"Business intelligence (BI) builds upon a set of tools and applications that enable the analysis of vast amounts of information (Big Data). Educational institutions handle large volumes of Big Data every year. There is a strong need for the use of BI in these institutions to improve their processes and support decision making. The core technology in a BI project is a datawarehouse (DW). This paper describes the design considerations for the implementation of the DW in an educational scenario. The DW will be used in a knowledge discovery process to handle the information for the analysis of key performance indicators using educational data mining (EDM) techniques. The DW along with an enterprise architecture (EA) repository are the key technological assets of a knowledge management framework (KMF). This framework was designed to put order in the creation, capture, transfer and digitalization of knowledge. This guide and the framework are two of the outcomes of a research project in a private university. Furthermore, a case study suggests how to choose the best methodology in higher institutions. In the case study the steps for the DW design are presented. This study can be useful for academics and practitioners that plan to design a DW to analyze information using EDM techniques.",10.1109/ITHET.2016.7760754,O. Moscoso-Zea; Andres-Sampedro; S. Luján-Mora
On the Impact of ML use cases on Industrial Data Pipelines,2021,1,Topic_1_data_big_big data,0.3790193971593829,"The impact of the Artificial Intelligence revolution is undoubtedly substantial in our society, life, firms, and employment. With data being a critical element, organizations are working towards obtaining high-quality data to train their AI models. Although data, data management, and data pipelines are part of industrial practice even before the introduction of ML models, the significance of data increased further with the advent of ML models, which force data pipeline developers to go beyond the traditional focus on data quality. The objective of this study is to analyze the impact of ML use cases on data pipelines. We assume that the data pipelines that serve ML models are given more importance compared to the conventional data pipelines. We report on a study that we conducted by observing software teams at three companies as they develop both conventional(Non-ML) data pipelines and data pipelines that serve ML-based applications. We study six data pipelines from three companies and categorize them based on their criticality and purpose. Further, we identify the determinants that can be used to compare the development and maintenance of these data pipelines. Finally, we map these factors in a two-dimensional space to illustrate their importance on a scale of low, moderate, and high.",10.1109/APSEC53868.2021.00053,M. Aiswarya Raj; J. Bosch; H. H. Olsson; A. Jansson
Industrial IoT business workshop on smart connected application development for operational technology (OT) system integrator,2017,3,Topic_3_industry_manufacturing_chain,0.8858608892455545,"Today, many of manufacturing system integrators are pursuing new business solution on information technology (IT) embedding Internet of Things (IoT) functionalities. However, how about conventional type of system integrators? Although their customers already require new innovative IoT solutions, such integrators tend to still provide traditional operational technology (OT) solutions that the customers are less interested in. This is a cause of lack of knowledge of IT technology that OT engineers have never experienced. This paper introduces a preliminary study for a method development on pragmatic workshop combining with business model definition and IoT technology training for such OT system integrators. This workshop method particularly focuses on convergence of OT and IT; defining a new business value chain and experiencing of commercial IoT technology as hands-on session. This paper is mainly discussed a real case study as a preliminary trial in an industrial city in Far East region.",10.1109/IEEM.2017.8289864,S. Goto; O. Yoshie; S. Fujimura
"Comprehensive Analysis: Monitoring Apache Kafka with Grafana, JMX Exporter, and Prometheus",2024,1,Topic_1_data_big_big data,0.8849305114174314,"In the big data era, monitoring and analyzing large amounts of heterogeneous data efficiently has become a major challenge. Monitoring and virtualization tools can significantly influence the performance and behavior of real-time open-source frameworks. The paper provides an analysis of Kafka’s performance by measuring producer and consumer latency and throughput using an integrated monitoring architecture including JMX Exporter, Prometheus, and Grafana. Confirming the experimental results, producer latency can be significantly decreased by up to 30% by increasing the number of partitions, whereas consumer delays may slightly increase because of partition overhead. The throughput investigation indicates that Kafka can achieve balanced producer and consumer throughput by processing a 4.6 MB dataset more effectively with more partitions. The paper offers significant results for improving real-time data processing by demonstrating the scalability and dependability of Kafka's performance when monitored using the proposed architecture.",10.1109/ICAIT65209.2024.10754944,T. Aung; H. T. Zaw; A. H. Maw; M. T. Mon
Moderating Role of Artificial Intelligence Tools to Foster Circularity in Businesses,2025,3,Topic_3_industry_manufacturing_chain,0.5573813301210658,"This paper examines how an array of disruptive digital technologies can be used in various stages of a business circular supply chain. With increased awareness of the circular economy in various sectors, challenges have arisen that need to be addressed with innovative means. The purpose of this study is to provide a deep knowledge of which digital technologies can foster a seismic shift from linear business models to circular models by closing product and material loops. Big Data, Artificial Intelligence Predictive analysis and material passports are among technologies touted as beneficial to optimize resources and reduce damage to the environment. The study is based on a qualitative methodology with two stages. The first phase examines literature regarding green production, consumption, and recycling methods in accordance with circular economy principles, with special focus placed upon robotic manufacturing and big data for more efficient production and decision-making. The second phase investigates essential enablers as well as barriers in introducing circular economy, with special focus upon those digital technologies which are likely to facilitate the circular value chain. The research concludes that AI technologies positively impact enabling circular business models.",10.1109/ICETI4T63625.2025.11132183,J. Shah; S. Loonkar; K. Desai; S. Patil
Pipeline Manager: A Flexible Semi-automatic Dataflow Analysis Framework,2021,-1,Outliers,0.3041975162619103,"Industrial big data analysis has received a bunch of attentions in recent decades. There are several famous machine learning or deep learning frameworks used in different scenarios. However, we lack a stable and easy-to-operate pipeline framework. In this paper, the purpose is to propose an algorithm pipeline integration framework to help industrial AI systems deal with loads, scheduling and automatic operations.",10.1109/SNPD51163.2021.9704972,C. -H. Chen; H. -C. Hong; Y. -S. Hong; H. Y. Wang; S. -S. Yu
Time series discord detection in medical data using a parallel relational database,2015,1,Topic_1_data_big_big data,0.5894292241883313,"Recent advances in sensor technology have made continuous real-time health monitoring available in both hospital and non-hospital settings. Since data collected from high frequency medical sensors includes a huge amount of data, storing and processing continuous medical data is an emerging big data area. Especially detecting anomaly in real time is important for patients' emergency detection and prevention. A time series discord indicates a subsequence that has the maximum difference to the rest of the time series subsequences, meaning that it has abnormal or unusual data trends. In this study, we implemented two versions of time series discord detection algorithms on a high performance parallel database management system (DBMS) and applied them to 240 Hz waveform data collected from 9,723 patients. The initial brute force version of the discord detection algorithm takes each possible subsequence and calculates a distance to the nearest non-self match to find the biggest discords in time series. For the heuristic version of the algorithm, a combination of an array and a trie structure was applied to order time series data for enhancing time efficiency. The study results showed efficient data loading, decoding and discord searches in a large amount of data, benefiting from the time series discord detection algorithm and the architectural characteristics of the parallel DBMS including data compression, data pipe-lining, and task scheduling.",10.1109/BIBM.2015.7359885,D. M. -k. Woodbridge; A. T. Wilson; M. D. Rintoul; R. H. Goldstein
Fast memory and storage architectures for the big data era,2015,1,Topic_1_data_big_big data,0.7810064899498365,"The rate of data growth is expected to propel. People and things continue to generate data, some of which are then transmitted, stored and analyzed. Stored data pose a great value to those who have access to them - but only if the data can be accessed and analyzed in a timely manner. For many interactive applications, traditional storage systems built with rotating magnetic media (hard drives) fall short because of their slow data access rate and high energy consumption. Clearly, solid-state memory and storage systems will play the central role in demanding big data analytics applications. Relevant questions are many, such as: How quickly can you detect fraud in on-line transactions? How much memory is needed to hold critical data for fast decision making? How much stored data can one process in a given time to offer relevant in-situ business intelligence? Emerging memory- and storage-centric system architecture innovations will enable cost-effective high-performance data analytics. In this presentation, recent advances in solid-state memory and storage system technologies and R&D challenges in the context of big data processing will be presented. Certain broader driving forces and trends in the memory industry will be outlined as well.",10.1109/ASSCC.2015.7387515,S. Cho
Using Active Data to Provide Smart Data Surveillance to E-Science Users,2015,1,Topic_1_data_big_big data,0.6862078054206312,"Modern scientific experiments often involve multiple storage and computing platforms, software tools, and analysis scripts. The resulting heterogeneous environments make data management operations challenging, the significant number of events and the absence of data integration makes it difficult to track data provenance, manage sophisticated analysis processes, and recover from unexpected situations. Current approaches often require costly human intervention and are inherently error prone. The difficulties inherent in managing and manipulating such large and highly distributed datasets also limits automated sharing and collaboration. We study a real world e-Science application involving terabytes of data, using three different analysis and storage platforms, and a number of applications and analysis processes. We demonstrate that using a specialized data life cycle and programming model -- Active Data -- we can easily implement global progress monitoring, and sharing, recover from unexpected events, and automate a range of tasks.",10.1109/PDP.2015.76,A. Simonet; K. Chard; G. Fedak; I. Foster
Big data security issues and challenges,2016,1,Topic_1_data_big_big data,1.0,"We have entered in data deluge already. Data Deluge means data generated by IoT devices and humans simultaneously. The data deluge is a Big threat for technologist but beneficial for end users. Now the coming problem is the security of this data. Big Data is too big, too fast and too diverse that does not compile with traditional data base system. Traditional data base systems are very good to analyze structured data but these systems are not enough to analyze unstructured data. In this paper we discourse the possible challenges and security issues related to Big Data characteristics and possible solutions.",10.1109/CCAA.2016.7813690,N. Chaudhari; S. Srivastava
A proposal to improve the system life cycle support of composites structures mapping zonal testing data on LSA Databases,2016,3,Topic_3_industry_manufacturing_chain,0.4100083731395364,"In the Life Cycle Management context of complex systems there are a few “families” of products which, due to the aleatory behavior of the development and production processes, present difficulties in the management of the remaining usable life and spare parts approach. One of such “family” is represented by composite materials and products. This article proposes a methodology to improve their support through the mapping of the zonal/functional characteristics of each item on a LSA Database (ASD S3000L will be used as the example).",10.1109/MetroAeroSpace.2016.7573203,E. De Francesco; R. De Francesco; F. Leccese; M. Cagnetti
Evaluating NoSQL Document Oriented Data Model,2016,1,Topic_1_data_big_big data,0.5696220532532841,"Exabytes of data are created everyday based on the generated user information over Internet. Social networks, mobile devices, emails, blogs, videos, banking transactions and others, are now establishing a new digital channel between the brands and their audiences. Powerful tools are needed to store and explore this daily expanding data, in order to submit an easy and reliable processing of user information. Traditional modeling tools face their limits in this challenge, as the information keeps growing in volume and variety. That can be handled only by non-relational data models. From 10 years, document oriented data model is shaking up the standard relation data model. We will present in this paper an evaluation of this model using a set of predefined criteria.",10.1109/W-FiCloud.2016.26,H. Hashem; D. Ranc
An efficient data leakage prevention framework for semiconductor industry,2016,2,Topic_2_data_privacy_security,0.46570511100058076,"When IC production enters into the nanometer generation, more and more semiconductor design and manufacture companies have taken a lot of effort in information security area to prevent company information security. But data leakage is a problem still far from been solved, despite its long history to the early days of computer system. Therefore, protection for these sensitive and confidential data gains great attention from foundry's top management, administrators and IT managers. This paper attempts to analysis data protection requirements in IC foundry and to integrate current DLP (Data leakage prevention) approaches to build an efficient data leakage prevention framework to protect IC field sensitive data safety.",10.1109/IEEM.2016.7798201,S. Zhu; E. Guo; M. Lu; A. Yue
Resource-centric Dynamic Access Control in Cloud,2016,2,Topic_2_data_privacy_security,1.0,"More and more people prefer to obtain information from cloud. Different users tend to access different resources they interested at anytime across different networks by a variety of equipments. As same as the traditional management of file, the lifecycle theory is still suitable the electronic data in cloud computing. Firstly, we analyze the motivation of access control in cloud, and summarize the security requirements for the data management in the whole lifecycle. Second, we propose a resource-centric dynamic adaptive access control model (RCDA), which is extended from the action-based access control (ABAC). RCDA is able to describe other access control models through the way of customization. Furthermore, we give the scheme for implementation of RCDA. Finally, we make the comparisons between the RCDA and other existing models, and the results indicate that RCDA is able to satisfy the security requirements for the whole lifecycle of data by adaptively adjusting the access control policies.",10.1109/TrustCom.2016.0299,M. Su; A. Fu; Y. Yu; G. Shi
Electric vehicle business model innovation study from the perspective of the energy internet,2017,3,Topic_3_industry_manufacturing_chain,0.27245051599239045,"The use of electric vehicles (EVs) has become an important part of efforts to limit vehicle emissions and improve environmental conditions by limiting air pollution. Many manufacturers of vehicles are exploring different ways to produce efficient EVs. As the EV industry expands and charging facilities are developed, enterprises that gain a sufficiently large market share will need the ability to analyze user data. A business model will be needed that optimizes the use of advertising, marketing, and big data to provide the most efficient EV business model. From the perspective of the Energy Internet, this paper analyzes the features of business model innovation in the value network of electric vehicles (EVs), discusses business models based on different value dominator, and provide a deep insight of EV business model from the vehicle-battery separation and vehicle operation pattern side. At last, this paper proposes ten types of EV business models in three categories, and provides advice focusing on business model innovation at different developmental stages of the EV industry.",10.1109/EI2.2017.8245632,Y. Chu; Q. Xiao; L. Yang
Performance enhanced security for enterprise cloud application,2016,2,Topic_2_data_privacy_security,0.7206659860410656,"Enterprise Applications are big business applications. They are complex, distributed, scalable, component-based, and large and mission critical. Enterprise applications in cloud are designed to satisfy hundreds of such Enterprise customer needs, but support the same business needs. Application should be capable of supporting multi-user, multi-developer, multi-machine, multi-component that can manipulate massive data and uses parallel-processing methods for processing it. Most enterprise application has sensitive data that requires compliance to security regulations. The data also needs to me masked, in other words encrypted before moving to cloud [1]. Most of the providers today protect data in two ways. One way is to upload the cloud data and then encrypt and other way is to encrypt and then upload data. In first method the keys are maintained by the cloud provider, e.g. Dropbox, Google Drive, Microsoft Sky Drive. The proposed method in this paper is based on the second method. The Encryption mechanism and keys are maintained by the customer. The approaches in previous works will be suitable for point-of-view online back-up a write once and read many times kind of scenario. In case of Enterprise Applications where lot of transactional data is involved, data transfer rate between application and database in cloud should be really faster to have anytime anywhere seamless experience. The proposed method adds algorithms and logics to the existing HCPOD model for fine grained, high performance cloud data access and storage.",10.1109/ICCCI.2016.7479985,M. Vanitha; C. Kavitha
An Automated Cloud-Based Big Data Analytics Platform for Customer Insights,2017,1,Topic_1_data_big_big data,0.793386310746454,"Product reviews have a significant influence on strategic decisions for both businesses and customers on what to produce or buy. However, with the availability of large amounts of online information, manual analysis of reviews is costly and time consuming, as well as being subjective and prone to error. In this work, we present an automated scalable cloud-based system to harness big customer reviews on products for gaining customer insights through data pipeline from data acquisition, analysis to visualisation in an efficient way. The experimental evaluation has shown that the proposed system achieves good performance in terms of accuracy and computing time.",10.1109/iThings-GreenCom-CPSCom-SmartData.2017.48,L. Han; M. S. Haleem; T. Sobeih; Y. Liu; A. Soroka; L. Han
Data VV&C of artificial Beijing system,2016,-1,Outliers,0.39442473322849,"As a most important part of modeling and simulation, data influences the reliability and credibility of model and system directly. First, this paper researches on the related concepts of data VV&C (verification, validation and certification) and analyzes the artificial Beijing system deeply. Then, According to the building process of human model in artificial Beijing, it expatiates on the process and key steps of input and output data VV&C, so as to provide a referable case for data VV&C and provide certain reference for the development and evaluation process of model and system as well.",10.1109/ICMA.2016.7558573,Y. Luo; H. Duan
An Architecture for Model Monitoring System with Automated Data Validation and Failure Handling,2025,1,Topic_1_data_big_big data,1.0,"This paper illustrates the architecture and implementation of a state-of-the-art, Cloud Native Model Monitoring System built to ensure model integrity, validity, and operational resilience in large, geographically dispersed systems. The system is built completely on AWS, with core services such as Amazon S3, Lambda, EMR, SNS, and SOS as well as big data systems like Apache PySpark integrated to provide robust, automated data validation and real-time anomaly detection. Unlike existing monitoring solutions that focused on static checks, this work presents a hybrid approach that combines both dynamic and static validation techniques to proactively detect and mitigate risks in real-time. A hybrid approach with dynamic and static checks is coupled with data quality assessment at aggregate and record levels. The system uses an event-driven architecture to provide low latency and scalable data validation and it is faulttolerant to anomalies with predictive maintenance and dead letter queues. This work shows how modern serverless technologies and distributed processing frameworks can be orchestrated to solve critical data quality challenges in continuous and reliable monitoring of data pipelines distributed across complex, diverse business environments.",10.1109/ICCSAI64074.2025.11064092,S. B. R. Karri; V. K. Devalla; R. K. Bojja; M. S. Pandey
Nonparametric regression-based failure rate model for electric power equipment using lifecycle data,2016,0,Topic_0_prediction_degradation_rul,1.0,"In order to analyze the fault trends more accurately, a failure rate model appropriate for general electric power equipment is established based on a nonparametric regression method, improved from stratified proportional hazards model (PHM), which can make maximum use of equipment lifecycle data as the covariates, including manufacturer, service age, location, maintainer, health index, etc. All of covariates are represented in the hierarchy process of equipment health condition, which is beneficial for processing and classifying the lifecycle data into multitype recurrent events quantitatively. On this occasion, more inspecting events can be utilized in a complete cycle to predict potential risk and assess equipment health condition. Then, stratified nonparametric PHM is employed to build the multitype recurrent events-specific failure model appropriate for competing risk problem toward interval censored. Lastly, the example in terms of transformers demonstrates the modeling procedure. Results show the well asymptotic property and goodness-of-fit tested by both of graphical and analytical methods. Compared with existing failure models, such as age-based or CBF model, this improved nonparametric regression model can mine lifecycle data acquisition from asset management system, depict the failure trend accurately considering both individual and group features, and lay the foundation for health prognosis, maintenance optimization, and asset management in power grid.",10.1109/TDC.2016.7519850,Jian Qiu; Huifang Wang; Dongyang Lin; Benteng He
Implementing an edge-fog-cloud architecture for stream data management,2017,1,Topic_1_data_big_big data,1.0,"The Internet of Moving Things (IoMT) requires support for a data life cycle process ranging from sorting, cleaning and monitoring data streams to more complex tasks such as querying, aggregation, and analytics. Current solutions for stream data management in IoMT have been focused on partial aspects of a data life cycle process, with special emphasis on sensor networks. This paper aims to address this problem by developing streaming data life cycle process that incorporates an edge/fog/cloud architecture that is needed for handling heterogeneous, streaming and geographically-dispersed IoMT devices. We propose a 3-tier architecture to support an instant intra-layer communication that establishes a stream data flow in real-time to respond to immediate data life cycle tasks in the system. Communication and process are thus the defining factors in the design of our stream data management solution for IoMT. We describe and evaluate our prototype implementation using real-time transit data feeds. Preliminary results are showing the advantages of running data life cycle tasks for reducing the volume of data streams that are redundant and should not be transported to the cloud.",10.1109/FWC.2017.8368538,L. Hernandez; H. Cao; M. Wachowicz
Big data security and privacy issues — A survey,2017,1,Topic_1_data_big_big data,1.0,"Nowadays, many people get connected with each other in one virtual world known as “Cyber Society” instead of physically connected. The interaction of people with cyber society components, such as social media, search engines, blogs, websites - with their services, causes generation of enormous amount of data termed as, “Big Data”. With adaption of Big Data in banking, finance, retail industry, health care, smart city, social media and IT sectors, it has started gaining importance along with many research challenges such as heterogeneity, data life cycle management, data processing, scalability, security and privacy, and data visualization. Many security and privacy issues emerged with Big Data that are not likely to be solved by conventional security solutions. Hence, this article is aimed to present overall perspective snapshot of security and privacy issues of Big Data.",10.1109/IPACT.2017.8245064,N. Joshi; B. Kadhiwala
EUDAT - A Pan-European Perspective on Data Management,2017,1,Topic_1_data_big_big data,0.5079498222989661,"Data management planning - thinking in advance about what will happen to data produced during the research process - is increasingly required by national research funding agencies, and data management guidelines for Horizon 2020 research projects were released by the EU in December 2013 (Guidelines on Data Management in Horizon 2020). Similar guidelines have been issued by the US Department of Energy (Statement on Digital Data Management), Australia (ANDS Data Management Plans) and across many other countries. The EUDAT project exists in part to disseminate and promote best practice in data management for twenty-first century research, and to provide support for communities in adopting basic principles such as data registration, metadata creation and data movement. As part of its mission to help researchers and research communities manage and preserve their data, EUDAT works with the world-recognised Digital Curation Centre on a version of their widely-used DMPonline tool which will capture the H2020 guidelines in a data management planning tool tailored to the emerging needs of European research. EUDAT's is building a Collaborative Data Infrastructure (CDI) as a pan-European solution to the challenge of data proliferation and associated management in Europe's scientific and research communities. The CDI will allow researchers to share data within and between communities and enable them to carry out their research effectively. Our mission is to provide a solution that will be affordable, trustworthy, robust, persistent, open and easy to use.",10.1109/NSSMIC.2017.8533053,S. de Witt; D. Lecarpentier; M. van de Sanden; J. Reetz
A journey on privacy protection strategies in big data,2017,2,Topic_2_data_privacy_security,1.0,"In this modern world providing security for the data is the great challenging task. Especially handling of big data is a great issue because of its volume and variety of data structure. There are various strategies for storing the big data in an efficient way. But the consideration of privacy look up is very important. Privacy preservation varies from different stage of big data life cycle. Due to multi tenancy and massive computation issues, it is become a demanding task. While considering the Framework security, data security, integrity constraints management protecting big data privacy is plays an important role. This paper surveys the privacy requirements, obstacles and the techniques to handle privacy protection strategies in big data.",10.1109/ICCONS.2017.8250688,D. Viji; K. Saravanan; D. Hemavathi
SDN-based Architecture for Big Data Network,2017,1,Topic_1_data_big_big data,1.0,"Big data needs underlying network for data transmission in the practice, big data network is an essential content of big data research. In this paper, a big data network framework for the demands in the life cycle of big data is proposed, and the main challenges of big data network using traditional technologies are analyzed. Software defined networking (SDN) separates the control layer from the data layer, and has the advantages of logical concentration, scalability and programmability. According to the characteristics of each subnet in big data network framework, we design the SDN-based architectures of data access network, content transmission network, data center network and backbone network respectively, and elaborate their implementation methods, main functions and collaborative operation.",10.1109/CyberC.2017.102,Y. Xu; Z. Sun; Z. Sun
Software Defined Cyberinfrastructure for Data Management,2017,1,Topic_1_data_big_big data,0.6326065298710385,"Scientific research is data-centric, relying on the acquisition, management, movement, analysis, and sharing of data. Proficiently managing the end-to-end lifecycle of scientific data is non-trivial and comprises many time consuming and mundane tasks. While individual tasks are not prohibitive, when done repeatedly and frequently they represent a significant strain on researchers. We posit that a better approach is to automate these tasks through a Software Defined Cyberinfrastructure. We have developed R IPPLE to provide such capabilities by automating research data management activities via a programmable and event-based cyber-environment. Users specify high-level management policies, such as data movement and metadata extraction, using intuitive If-Trigger-Then-Action rules. These rules are then autonomously, and reliably, executed and managed by Ripple.",10.1109/eScience.2017.69,R. Chard; K. Chard; S. Tuecke; I. Foster
A novel product life-cycle management architecture of construction machinery,2017,3,Topic_3_industry_manufacturing_chain,0.6105085192441898,"This paper illustrates the product life-cycle management architecture of construction machinery under the background of the rapid development of new technologies such as big data, cloud computing and internet of things. The significance and definition of product life-cycle management is proposed before analyzing the connection among three different stages in detail. Then a more vivid system diagram called product life-cycle management house is proposed and described specifically as the basis of building product life-cycle management system. The operating mechanism is also stated and four specific applications of product life-cycle management are presented. Finally an integrated life-cycle management system of bucket wheel is given to show the effectiveness of this architecture.",10.1109/ICMA.2017.8015935,H. Shen; Q. Zhou; J. Zhao; X. Liu
Data management in PLN TJBB : Initial business case,2017,1,Topic_1_data_big_big data,0.35101138957226974,"In asset management, the availability of appropriate information is key in generating strategic, tactical and operational business decisions. Especially in the planning of asset management, it is required centralization and integration of data to ensure the control of the output (information) in the decision-making. PLN TJBB currently develop data management consist of assets data and asset management data. Their life cycle begins from acquire, storage, update, utilize, access, archive and disposal of data. Data management aims to standardize the management of data collection for all department in organizations. Nowadays, the data are still handling with different standard and management. Control data management is important to ensure that data which is being managed are the correct and useful one, and these are being used as the basis for decision making. It is means that we are not collect data without knowing the value obtained from the data store. Approach to data management development are by assess data in order to determine the current conditions and improve data management to fulfill the needs in the future with consideration of overall business process. It is also considering the organizational behavior include: employee discipline, competence level in utilizing the tools of data collection, commitments to the existing data, and so on. This paper will discuss phase in data management, consideration that being used, the obstacles, the process of formulation, unique organizational behavior and available resources.",10.1109/ICHVEPS.2017.8225926,S. Naswil; N. U. A. Wardani; C. P. Prabaswara
"Live demonstration: Enhancing biomedical research precision, productivity and reproducibility via autonomous data acquisition and robust data curation",2017,1,Topic_1_data_big_big data,0.6678493851271841,"We present the eGor digital laboratory assistant platform that improves the experience of characterizing materials, devices and processes. Conceived to address challenges in biosensor and biomedical system development, eGor is a highly flexible platform for 1) automation of data acquisition with precise timing control, 2) production of results objects that are rich with information defining the measurement setup and provenance of instruments and datasets, and 3) curation of results objects and processed child datasets throughout the data life cycle. eGor packs three tools into a user-friendly browser interface: Designer to manage digital inventory of instruments and digitally capture measurement project scheme details including instrument layout and test procedures; Executer to monitor real-time measurements, search and run project schemes, and schedule future automated project runs; Analyzer to search, view and annotate results objects containing a digital description of the project scheme and the results data it generated, filter and process datasets, and trace provenance across all instruments, schemes, and datasets. These eGor services interface with a measurement workbench through the Instrument Manager tool that runs on local workbenches to collection data from and manage communication with physical instruments. Together, these eGor tools enable biomedical research with improved accuracy and precision through timing-controlled automation and with greater productivity through intuitive user-friendly interfaces capable of scheduling and running measurements without user presence. Moreover, eGor can have groundbreaking impact on research reproducibility by generating meta-rich results objects that permit exact repetition of measurements and collaborative sharing of both data and detailed project schemes.",10.1109/BIOCAS.2017.8325087,Y. Gtat; A. J. Mason
On QoE-awareness through virtualized probes in 5G networks,2016,1,Topic_1_data_big_big data,1.0,"The advent of 5G Networks introduces significant challenges in almost every link of the network value chain. The demand for seamless connectivity, extremely low latency, high-speed data transfer and energy efficiency along with the exponential increase of interconnected devices will shape an ecosystem with such complexity that enforces the replacement of almost every current standard. It is therefore necessary to re-address all aspects of networking with Quality of Experience (QoE) amongst them. This paper aims to provide an overview of some exciting new technologies 5G networks are based upon and present a novel architectural component that will solve the thorny issue of QoE-awareness facilitated by the advanced virtualization and data management capabilities this novel user-centric networking paradigm supports.",10.1109/CAMAD.2016.7790351,C. Tselios; G. Tsolis
On the Design of Medical Data Ecosystem for Improving Healthcare Research and Commercial Incentive,2021,-1,Outliers,0.18522115852151452,"This paper outlines a visionary approach for building a distributed federated medical data lake and ecosystem with commercial incentive across hospitals and personal health data generated from wearable medical devices at home. This article creates the ownership enforcement guideline as the basis for building the blockchain based ecosystem platform by reviewing various aspects of studies of data ownership rights. This blockchain based platform can enforce the data ownership, patient privacy, and provide a controlled and secure access to the medical data. It can also perform owner centric medical data exchange and effectively aggregate various related medical data sets from various distributed data sets from various hospitals. Moreover, this blockchain platform unlocks the academic and business value of the medical data by modeling medical data as Non-Fungible Token (NFT) which can provide incentives for all data driven value chain entities to make a medical data ecosystem possible.",10.1109/CogMI52975.2021.00024,Z. -Y. Shae; J. J. P. Tsai
Wiping Techniques and Anti-Forensics Methods,2018,2,Topic_2_data_privacy_security,1.0,This paper presents a theoretical background of main research activity focused on the evaluation of wiping/erasure standards which are mostly implemented in specific software products developed and programming for data wiping. The information saved in storage devices often consists of metadata and trace data. Especially but not only these kinds of data are very important in the process of forensic analysis because they sometimes contain information about interconnection on another file. Most people saving their sensitive information on their local storage devices and later they want to secure erase these files but usually there is a problem with this operation. Secure file destruction is one of many Anti-forensics methods. The outcome of this paper is to define the future research activities focused on the establishment of the suitable digital environment. This environment will be prepared for testing and evaluating selected wiping standards and appropriate eraser software.,10.1109/SISY.2018.8524756,M. Ölvecký; D. Gabriška
Research Data Management Policy And Institutional Framework,2018,1,Topic_1_data_big_big data,0.45397444366086975,"Data created from research is a valuable resource and usually requires much time and money to be produced. Research data is the data that is collected, observed, or created for purposes of analysis to produce original research results. Research data has significant value in responsible research, which refers to the ability to justify conclusions on the basis of the data acquired and generated through research and that is furnished to other researchers for scrutiny and/or verification. This data can be used and reused for future scientific and educational purposes. Research data management is becoming increasingly important and it is beneficial for the institutions as well as for the researchers. It helps institutions to increase impact and visibility of their research and simultaneously helps researchers to keep their data secure, improve the opportunities for collaboration for further research, more research publications and increased citations. This paper discusses some of the key issues related to research data management, roles and responsibilities of the institutions as well as the research scholar, cost and infrastructure, data sharing policies, legal and ethical issues, copyright issues, etc. An attempt has been made to depict and present suitable policies with a policy framework. Work flow chart with adoption of best practices in RDM is given, that will boost the research process in an organisation. The main objective of this paper is to provide suggestions and recommendations for successful research data management based on some of the best practices and principles of RDM adopted by the institutions worldwide.",10.1109/ETTLIS.2018.8485259,N. K. Singh; H. Monu; N. Dhingra
Towards an End-to-End Architecture for Run-Time Data Protection in the Cloud,2018,2,Topic_2_data_privacy_security,1.0,"Protecting sensitive data is a key concern for the adoption of cloud solutions. Protecting data in the cloud is made particularly challenging by the dynamic changes that cloud systems may undergo at run-time, as well as the complex interactions among multiple software and hardware components, services, and stakeholders. Conformance to data protection requirements in such a dynamic environment cannot any longer be ensured during design time; e.g. due to the dynamic changes imposed by replication and migration of components. It requires run-time data protection mechanisms. This paper proposes combining multiple existing data protection approaches and extending them to run-time, ultimately delivering an end-to-end architecture for run-time data protection in the cloud. We validate the practical applicability of our approach by a commercial case study.",10.1109/SEAA.2018.00088,N. Gol Mohammadi; Z. Á. Mann; A. Metzger; M. Heisel; J. Greig
Towards a Cloud-Based Controller for Data-Driven Service Orchestration in Smart Manufacturing,2018,3,Topic_3_industry_manufacturing_chain,0.8794458020138484,"The orchestration of smart manufacturing service operations and processes arises as a challenging step in the realization of the Industry 4.0 vision. This paper presents the work in progress towards the specifications of a controlling environment for data-driven orchestration of software services in future smart manufacturing scenarios. The paper discusses the role and significance of multi-aspect data in the management of manufacturing operations and proposes a reference architecture for controlling the orchestration of the respective data services, following the work that has been conducted in the context of the EU-funded project DISRUPT.",10.1109/ES.2018.00022,V. Tountopoulos; E. Kavakli; R. Sakellariou
Data Management Architecture A Need in Smart Grids Domains,2018,1,Topic_1_data_big_big data,0.46929655402859927,"A system architecture is a high level design that enables to represent a (complex) system. As architectures are abstract entities, they must be expressed through their architecture descriptions. These descriptions are documents that arise as the result of activities related to the architecture development. In this work, we propose a practical system architecture definition based on some related concepts as architecture framework, reference model, standards and reference architecture. We also identify the features that an architecture must have as a guide for its development. Then, we analyze the most relevant Smart Grid architectures proposals, looking for whether they include or not a Data Management perspective. Data management is a high-level function that seeks to manage appropriately every aspect associated to data and information assets in an organization. Data is an important asset in smart grid domain. We conclude that there is an important gap between the Smart Grid Data Management requirements and the guides in the reviewed architectures.",10.1109/ICSGCE.2018.8556826,M. C. B. Botero; O. G. D. Velasco
Achieving broadband-based smart city services in connected communities,2016,3,Topic_3_industry_manufacturing_chain,1.0,"New opportunities exist to leverage investments in 'broadband to achieve community-centered socio-economic impact both in the context of regional strategies and national priorities of e.g., advanced manufacturing, transportation, healthcare. However, success of realizing smart services depends on numerous socio-economic-technical interactions between the Community, City and Contextual (CCC) factors. Studies of these interactions have not previously been pursued as a systematic field of study. This paper presents a novel CCC service design theory to guide a prescriptive framework to achieve delivery of broadband-based smart city services. Our framework entitled Enterprise Services Delivery Architecture (ESDA) is applied to case studies in Dublin and Columbus (cities in Ohio, USA) that are Intelligent Community Forum award winners. ESDA aligns community service needs to organization and information services delivery to identify gaps around which innovation can co-evolve with diverse stakeholders. Through the lens of the ESDA, the cases demonstrate: 1) strategies by which a designed service value chain can be resourced innovatively to deliver the benefits of broadband; 2) ways to enable cities to become platforms within which physical or virtual communities can be empowered despite complexities confronted; and 3) a standard way in which good practices can be curated globally, ranging from a city-scale, and beyond to rural and regional scales.",10.1109/ISC2.2016.7580814,J. Ramanathan; P. Calyam
Study on Partial Stratified Resampling for Particle Filter Based Prognosis on Li-Ion Batteries,2018,0,Topic_0_prediction_degradation_rul,1.0,"Accurate online prognosis of engineering systems plays a vital role in prognosis and health management (PHM) technologies to ensure safety, prevent damage and economic loss. The particle filter (PF) algorithm has proved to be an effective method for prognostics. However, the PF algorithm suffers from serious particle degeneracy and particle impoverishment problems. Most of the studies in the literature focus on solving the particle degeneracy problem but at a heavy computational cost. In this study, we aim to explore the time efficient Partial Stratified Resampling algorithm which can be used for online state estimation problems and compare it with conventional Stratified Resampling algorithms. The accuracy and precision of the algorithms are validated using the lithium-ion battery data sets from CALCE® research group.",10.1109/PHM-Chongqing.2018.00207,K. Pugalenthi; N. Raghavan
Predictive Technologies and Methodologies for Human Operator Assessment in Industry 5.0: A Conceptual Framework,2024,3,Topic_3_industry_manufacturing_chain,1.0,"The advent of Industry 5.0 represents a paradigm shift towards a more human-centric approach in manufacturing, focusing on integrating human operators with advanced technological systems. Despite significant progress in predictive maintenance for machinery, there is a notable gap in predictive assessment technologies to safeguard human operators. This paper introduces a novel conceptual framework designed to fill this gap by leveraging predictive technologies and methodologies to proactively monitor human operators in Industry 5.0 paradigm settings. Our framework emphasizes the importance of human well-being and safety by integrating data collection, advanced analytics, and targeted intervention techniques. Through a literature review of related works and a detailed exposition of our framework, we highlight its potential to enhance operational efficiency, environmental sustainability, and, importantly, the overall welfare of the workforce. This research underlines the critical need for a balanced focus on both technological advancement and the well-being of human operators, proposing a preemptive approach that aligns with the pillars of Industry 5.0. We discuss the implications of our findings for future research, particularly the need for ethical data collection practices, real-time data processing techniques, and personalized interventions. The proposed framework categorizes conceptual approaches and introduces recent innovations in predictive assessment technologies, outlining the way for more sustainable, efficient, and human-centric industrial environments.",10.1109/ETFA61755.2024.10710870,M. Martinelli; S. I. Lopes; M. Migliardi
Application of Data Assets,2018,1,Topic_1_data_big_big data,0.34671094420580106,"This paper aims to addresses problems of effective data assets in data centers and proposes solutions oriented around business demands. An algorithm is proposed to describe a means of constructing data asset and evaluation indicators such as asset conversion rate, productization rate by defining data products, data account and data asset. The life cycle management of data assets are then discussed by applying a data asset management platform based on a provincial power enterprise. Finally, discuss the future work based on data product achievement, data service and data assets.",10.1109/IMCEC.2018.8469446,X. Liu; Y. Jiang; L. Pan
Mobile Technology Model to Collection Information of Self-Assisted Clinical History,2018,2,Topic_2_data_privacy_security,0.6226864055284845,"In this paper, we propose a mobile technology model to collection information of self-assisted clinical history. The model allows the collection of patient information through a mobile application with the aim of facilitating care in health centers and clinics of Peru. It should be noted that the patient is autonomous to record their health history obtained until the date of their consultation. This technology model is implemented under a Cloud Computing platform that allows integration with the various devices for the process of capturing medical information. It also considers the flow of data processing through the services acquired, the privacy and security of data for the use of sensitive data that makes up the patient's clinical history and, finally, the management and treatment of data to ensure its availability. The transfer of patient health information is done through a security code generated in the application, in such a way that the sensitive data of the patient is visible to the doctor who requests this information.",10.1109/INTERCON.2018.8526434,J. Serrato-López; R. P. Pacheco-Rojas; J. Armas-Aguirre
Blockchain Technologies and Their Applications in Data Science and Cyber Security,2020,-1,Outliers,0.28123332639160814,Blockchain technologies have been very effective in processing distributed transactions securely. They have many applications including in handling bitcoin cryptocurrencies and smart contracts. More recently the use of blockchain has been explored for data science applications. This paper examines blockchain technologies and discusses their applications in data science and cyber security.,10.1109/SmartBlock52591.2020.00008,B. Thuraisingham
Big Data: Current Challenges and Future Scope,2020,1,Topic_1_data_big_big data,1.0,"Big Data encompasses huge amounts of raw material which influence multitude of research fields as well as different industries performance such as business, marketing, social network analysis, educational systems, healthcare, IoT, meteorology, fraud detection. It aimed to uncover hidden trends and has prompted a development from a model-driven perspective to a data-driven approach. Among numerous properties of Big Data, datasets of Big Data are identified primary as 3Vs attributes which have high variety, velocity and volume. These provide an invaluable insight and assist in making precise decisions. Analyzing this information and outlining the outcome into helpful data is the method for extricating an incentive from these enormous volumes of datasets. Nevertheless, Big Data containing unique features that cannot be handled and processed using the conventional methods. This has presented a significant challenge to the industry. This research paper presents a general outline of the characteristics of Big Data as well as expounds on the present challenges and limitations in this area. It further discusses the future scope in particular the future direction for Big Data research.",10.1109/ISCAIE47305.2020.9108826,A. Ashabi; S. B. Sahibuddin; M. S. Haghighi
Big Data Processing and Application Research,2020,1,Topic_1_data_big_big data,1.0,"Nowadays, big data has become a constantly extended and widely mentioned term. It can excavate, describe and utilize a large amount of structured, unstructured and semi-structured data to obtain more information. With the rapid increase of data, big data has become more and more diverse, and the big data technology has emerged consequently. This paper reviews the literature of big data and the related technologies, such as Hadoop and Map Reduce. And it discusses the life cycle of big data, that is, big data acquisition, preprocessing, storage and analysis. Then it expounds the representative application of big data. Finally, based on the above study, this paper summarizes the development of big data.",10.1109/AIAM50918.2020.00031,P. Gao; Z. Han; F. Wan
Intelligent Maintenance of Complex Equipment Based on Blockchain and Digital Twin Technologies,2020,-1,Outliers,0.2571151518192444,"With the development of modern equipment system and science and technology, equipment is more and more toward the direction of digital and intelligent development. Complex equipment often undertakes important tasks, once the failure occurs, the result will be particularly serious. Based on this, the main work of this paper is as follows. (1) This paper builds the whole life cycle data chain of complex equipment from design, parts production, equipment transportation and installation based on blockchain technology so that the data is effectively used, and the data privacy is protected. (2) The blockchain technology is applied to the construction of digital twin, and the mapping from physical entity to virtual space is realized. It is more effective and accurate to diagnose the state and forecast the future trend of complex equipment. (3) A new intelligent maintenance framework is proposed, which provides new ideas and solutions for the intelligent maintenance of complex equipment.",10.1109/IEEM45057.2020.9309898,Q. Chen; Z. Zhu; S. Si; Z. Cai
ColPri: Towards a Collaborative Privacy Knowledge Management Ontology for the Internet of Things,2020,2,Topic_2_data_privacy_security,0.3427773686071071,"User privacy preferences management is a nontrivial task. In the context of the Internet of Things (IoT), where a huge amount of data is generated, transferred and stored via various local and cloud architectures, privacy protection becomes complex and hard to manage. Indeed, privacy management is a time-consuming activity that requires a lot of knowledge which most of IoT system users often lack or are not keen on acquiring due to its complexity. The knowledge dimension has often been neglected, by both researchers and industry. In this article, we focus on the privacy protection knowledge management aspect. We produce a first version of ColPri, an ontology that sets the basis for a collaborative extensible privacy protection knowledge management system that is able to collaboratively produce diagnosis of IoT stakeholders privacy policies. This paper aims to investigate collaborative privacy knowledge management in the IoT and how non-technical users could benefit from it to easily configure their privacy policies. It allows an open exchange of privacy-related knowledge. We propose ColPri, a collaborative privacy ontology after specifying design requirements that guided our choices during the ontology creation process. This ontology lays out the use of a privacy community to create and develop privacy-related information within a user-centric privacy architecture. Then, we show how to use this ontology through a use case scenario. Finally, we describe future research based on this work.",10.1109/FMEC49853.2020.9144927,A. Toumia; S. Szoniecky; I. Saleh
Research and Application of Data Security Protection Technology Based on Security Label,2020,2,Topic_2_data_privacy_security,0.41568904763490844,"With the rapid growth of power grid scale and data volume, the traditional storage of relational databases, data integration and integration analysis methods can’t meet the needs of business applications. This paper teases the data of power grid business system, and studies data security label management technology, unstructured data label technology and structured data label technology. A flowchart of data authorization control process based on data label technology is proposed, which combines data with security control information such as classification, density and sharing scope through data label technology. In the process of data sharing, publishing and using, fine-grained authorization and control can be carried out based on label attributes. A unified data security protection technology method based on security label is proposed, which can realize the data security policy based on data classification and grade to define the whole life cycle of data, and further realize that the data of specific type and level can be used, stored or transferred by personnel of specific level. The method proposed in this paper can be widely used in many business scenarios, such as data access control, data kinship analysis, data behavior tracking, watermarking tracing of leaked data, etc.",10.1109/ICISCE50968.2020.00222,D. Liu; Y. Chang; X. Liu; L. Ma; H. Zhang; R. Wang; H. Yu; B. Su
Exploration on Big Data Education for Computer majors in Applied Colleges and Universities,2020,1,Topic_1_data_big_big data,1.0,"In the teaching process of computer major in applied colleges and universities, the full application of big data education can improve the overall education level of computer major. In the teaching process of computer major, big data thinking and big data application ability should be listed as the key teaching content. Therefore, it is necessary to study and analyze the big data education of computer major in applied colleges and universities, to discuss the main components and main points of learning of big data ability education, and to promote the innovation of teaching methods and teaching schemes of computer major in applied colleges and universities. Only in this way can we cultivate students' big data thinking and improve students' big data application ability.",10.1109/ICMCCE51767.2020.00405,K. Xia; Y. Li
Statistical Learning: the impact of feature extraction process on the dispersion map of composite data points,2020,1,Topic_1_data_big_big data,0.4607352527830144,"In this paper, we present the impact of feature extraction process on the dispersion map of composite data points. We analyze this impact in distance space using distance graphs(scatter plots). The dispersion maps of composite data points in different feature spaces are depicted and compared using scatter plots to show differences. Finally, we implement the combined feature spaces as an extension to data life cycle to test its effectiveness on dispersion map of composite data points.",10.1109/ICDS50568.2020.9268753,M. Daoud
Research on Data Governance Methods and Systems for Large Hub Airports,2025,1,Topic_1_data_big_big data,0.3997028830989739,"As a new type of production factor in the digital era, data breaks the qualitative state of traditional production factors and has been highly valued by the state and the industry in recent years. With the continuous advancement of intelligent civil aviation construction, data issues have become the key bottleneck restricting the intelligent construction of airports. Data governance is not only the core grip for airports to give full play to the value of data elements, but also can provide important support for the integration and consolidation of the functions of various information systems and the collaborative control of various business modules in airports. This paper focuses on the data governance path of the Capital Airport, and describes the data governance system framework and information platform architecture of typical hub airports from top to bottom, which can provide a reference for other units in the industry to carry out data governance work with reference significance.",10.1109/ICAACE65325.2025.11018979,Y. Li; Y. Liu; T. Yuan; Y. Xu; N. Lei; J. Ning
Research on data Traceability Method Based on blockchain Technology,2020,-1,Outliers,0.30405033894486483,"Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.",10.1109/ICBASE51474.2020.00017,L. Wei; W. Dawei; W. Lixia
Remaining Useful Life Prediction Based on a Bi-directional LSTM Neural Network,2020,0,Topic_0_prediction_degradation_rul,1.0,"Electric motors have been widely used in the fields of national economic construction, scientific research, medical treatment and national defense. The health of motors plays key role in ensuring the safety of these fields, however, the online health monitoring of motors is not well studied. On the other hand, the combination of health science and artificial intelligence technology is playing an increasingly important role in replacing the traditional health monitoring of machines and has been proved its ability in serial data processing and other aspects. In this paper, a bi-directional cyclic neural network based algorithm is proposed for the intelligent remaining useful life (RUL) prediction of motors. Compared with the traditional one-way neural network, bi-directional cyclic neural network can predict the current state based on the past and future information at the same time, which obtains higher accuracy. This paper is organized in two stages: first, a health index is developed to fit the life cycle data of motors; Secondly, a bi-directional cyclic neural network based model is trained based on the health index for the online RUL prediction of motors. The simulation results show the effectiveness of the proposed method.",10.1109/ICCA51439.2020.9264453,Z. Pan; Z. Xu; C. Chi; H. Wang
"Data Science, COVID-19 Pandemic, Privacy and Civil Liberties",2020,2,Topic_2_data_privacy_security,0.6534404650177648,"The world has seen pandemics, terrorism, hurricanes and other natural and man-made disasters. Each time such an event occurs we discuss technologies that can solve the problem and their impact on our privacy and civil liberties. Such discussions occurred after the 9/11 terrorist attacks and is happening now during the COVID-19 pandemic, the worst human crisis we have faced in a century. This paper discusses the applications of data science to detect and possibly prevent such pandemics and its impact on our privacy and civil liberties.",10.1109/BigData50022.2020.9377966,B. Thuraisingham
Automatic Error Correction Technology for the Same Field in the Same Kind of Power Equipment Account Data,2020,3,Topic_3_industry_manufacturing_chain,0.21219073253460402,"Account data of electrical power system is the link of all businesses in the whole life cycle of equipment. It is of great significance to improve the data quality of power equipment account data for improving the information level of power enterprises. In the past, there was only the error correction technology to check whether it was empty and whether it contained garbled code. The error correction technology for same field of the same kind of power equipment account data is proposed in this paper. Combined with the characteristics of production business, the possible similar power equipment can be found through the function location type and other fields of power equipment account data. Based on the principle of search scoring, the horizontal comparison is used to search and score in turn. Finally, the potential spare parts and existing data quality are identified according to the scores. And judge whether it is necessary to carry out inspection maintenance.",10.1109/IICSPI51290.2020.9332426,B. Zhao; H. Zhang; Y. Luo
Evaluation scheme of OBE learning output certification based on Blockchain,2020,-1,Outliers,0.3155975919698125,"The content of OBE learning output certification and evaluation work is complex and numerous. It is difficult for the traditional teaching model to record and analyze the full life cycle of data in all aspects, and to achieve effective and credible quality supervision. To this end, we propose a solution that uses a partially decentralized alliance chain, introduces the Certification Engineering Education Committee(CEEC) as a centralized trusted authority, and expands trusted nodes through the PoA consensus mechanism. The life cycle of the data is recorded by the authorization contract, the learning output certification evaluation contract and the evaluation result contract which the data is stored in the Merkle tree. Finally, we build an engineering education learning output chain to conduct simulation experiments on resource consumption and transaction throughput. Experimental results show that this solution can effectively solve the above problems.",10.1109/ICVRIS51417.2020.00139,P. Hong; X. Hu
Full Lifecycle Management of Optical Access Networks with a Hierarchical Model-Based Design,2025,1,Topic_1_data_big_big data,1.0,"Optical Access Networks (OANs) as a digital infrastructure that provides reliable and high bandwidth connections to households and businesses are becoming increasingly important for today’s digital economy. The use of OANs in mobile X-haul and distributed edge computing imposes even more stringent requirements on the planning, design, deployment, and operation of OANs. In this paper, we discuss the opportunities and challenges of full life cycle management (LCM) of OANs. We propose a hierarchical model-based design methodology with which management tasks at different lifecycle stages of an OAN can be efficiently developed.",10.1109/ICTON67126.2025.11125235,Y. Huang; W. Sun
Control Loop Intellectualization in Human-Machine Systems,2025,3,Topic_3_industry_manufacturing_chain,0.7669349528881622,"This paper examines the components of the intelligent control loop from the point of view of various parameters. In the context of the AI data life cycle, each stage of the cycle was analyzed based on the criteria of functions, requirements and limitations of the human and machine components of the intelligent control loop. In conclusion, it is concluded that the intelligent (machine) component within the considered AI data life cycle “actively” plays its role at the stages of model building and operation of the AI system. In turn, the activity of the human operator (human component) is always performed, regardless of the specific stage. Understanding the amount of necessary work, the tasks that must be performed to create and operate AI systems at each stage will improve the quality of AI work due to precise compliance with requirements, achieve efficient use of resources (time, human, technical) in design and development.",10.1109/REEPE63962.2025.10970795,B. S. Goryachkin; K. P. Grishin
Data Curation as a Stepwise Service to Data Sustainability: The Grey Area Between Small-Scale Applications and Large-Scale Data Repositories,2025,1,Topic_1_data_big_big data,0.5902982194096473,"In developing a system architecture, data curation should be viewed as a crucial task for ensuring data sustainability, particularly managing the diverse scales of data organization - each with varying size, complexity, and resource requirements. The approach at hand considers data curation as a staged service aimed at bridging the intermediate zone between small-scale data management and the requirements of large-scale repositories. Within this scope, datasets often lack the formal structure and resources of major repositories but still require careful curation to meet the FAIR principles: Findability, Accessibility, Interoperability, and Reusability. The proposed framework offers an incremental approach within the data curation workflow to address these needs. Key to this workflow is a specialized software component - the Data Transfer Facilitator (DTF). The DTF is designed to streamline the transition of data from smaller systems into standardized, large-scale repositories, mitigating common transfer challenges. The architectural design should support adaptable archiving practices and sustainable data management strategies. An incremental approach allows smaller datasets to integrate with larger data infrastructures over time, adapt to evolving standards, and maximize the long-term utility of data resources across multiple disciplines. A prototype is used to demonstrate the form and function of a DTF for a project. Various prototypes are used to demonstrate the DTF's practical implementation.",10.1109/SysCon64521.2025.11014656,H. Peukert; T. Asselborn; R. Möller; S. Melzer
Physics-Informed Machine Learning for Predicting Remaining Useful Life of Aircraft Engines: A Robust Framework for Risk-Aware Maintenance,2025,0,Topic_0_prediction_degradation_rul,1.0,"Aircraft components degrade over time, directly impacting reliability and performance. This project aims to develop a machine learning framework for predicting the Remaining Useful Life (RUL) of aircraft engines based on comprehensive life cycle data, to optimize maintenance strategies and mitigate risks associated with component failure. A diverse set of regression and classification models-including K-Nearest Neighbors (KNN), Naïve Bayes, Random Forest, and Support Vector Machine (SVM)-are implemented and evaluated on NASA’s C-MAPSS dataset to estimate engine lifetimes. The key focus is to predict the accurate RUL for turbofan engines, particularly under High-Pressure Compressor (HPC) failure scenarios, and specifically targeting low RUL values to prevent critical risks. Physics-informed machine learning approach incorporates enhanced predictions by improving the robustness of maintenance planning.",10.1109/MPSecICETA64837.2025.11118401,A. Srivatsa; V. I. B. Goudar; T. Naveen; R. M. Peri; D. K. Banik; M. Rajesh Kumar
SqPal - Text to SQL GenAI Tool for PayPal,2025,1,Topic_1_data_big_big data,0.5424886358183407,"The advent of Large Language Models (LLMs) has transformed traditional practices in product data science. In this paper, we explore the complete lifecycle of GenAI tools within product data science teams, using the PayPal digital wallet data science team as an example. Specifically, we focus on the GenAI-powered text-to-SQL model we developed to support data scientists. This tool significantly reduces the time spent on ad-hoc data retrieval tasks-critical for business operations but often resource-intensive. We will delve into our modeling approach and demonstrate the tool's fast and secure implementation. Additionally, we will discuss our user data collection and feedback processes, and how periodic measurement of the tool performance using a unique question bank for PayPal data has ensured the tool's continuous improvement. Finally, we will address key challenges in adopting GenAI tools in large organizations, including gaps in data catalogs and the inherent complexities of data structures and lifecycles.",10.1109/CAIN66642.2025.00034,D. Liyanage; M. Moha; S. Suresh
A multi-instance LSTM network for failure detection of hard disk drives,2020,0,Topic_0_prediction_degradation_rul,1.0,"Hard disk (HDD) failure is the most important reliability issue in the data center. Therefore, the prediction of hard disk failure has become the focus of attention of major data centers. However, most current research work does not notice the fact that the data on the hard disk is mostly unlabeled data. Since the degradation period in HDD is very short, the mixture of health data and erroneous data can cause serious data imbalance. This makes fault prediction a difficult task. In response to the above problems, a multi-instance long-term sequence classification method based on long-short-term memory (LSTM) network is proposed. By dividing the longterm sequence data packet into multiple instances, the relationship between the instance and the sample label is studied to predict HDD failure. Through the analysis of the hard disk data of a communication company and the Backblaze data center, this method can obtain better results than other methods.",10.1109/INDIN45582.2020.9442240,
Research on the Analysis of Software Development Processes Based on GJB5000B and DO-178C,2025,-1,Outliers,0.12875712376370493,"GJB5000B and DO-178C represent software development standards from different domains. The former is applied to military products while DO-178C is predominantly applied in aviation field. Both standards aim to guide software development processes throughout the software life cycle, mitigating the risk of errors through process control. Analyzing and studying the similarities and differences between the two standards enhances a deeper understanding of the software development process, thereby further improving development efficiency and software quality.",10.1109/AEMCSE65292.2025.11042388,J. Luo
Key Technologies and Applications of Smart Maintenance for Metro Rails Based on Lifecycle Management,2025,1,Topic_1_data_big_big data,0.4261640897191471,"China's metro system ranks among the top in the world in terms of both its total operational mileage and the scale of planned construction. As the metro system enters a phase where construction and operation are given equal priority, there is an increased demand for improved management capabilities in metro operation and maintenance. This paper investigates the business requirements for lifecycle management of metro rails, exploring key technologies such as coding standards for individual rails, attribute data standards, the implementation of electronic tags, and service life evaluation models. A lifecycle management information system for metro rails has been designed and developed, which supports rail degradation prediction and optimizes maintenance and replacement decisions. This paper focuses on the system's technical architecture and core functionalities. The system has been successfully implemented on the actual lines of Tianjin Metro, effectively meeting operational requirements and yielding positive application outcomes.",10.1109/ICIT63637.2025.10965326,L. Bai; Q. Li; S. Wu; J. Du; M. Ding; C. Chi
Power Load Forecasting Assessment Method Based on Data Lineage Analysis,2025,-1,Outliers,0.2006169675015295,"Electric load forecasting utilizes historical data of economic, social, meteorological, and other influencing factors that drive load development. It applies scientific tools and methods to conduct quantitative or qualitative analysis on the patterns of historical load data variations, thereby predicting future load conditions. However, due to the randomness and uncertainty of load fluctuations, different prediction subjects exhibit distinct characteristics, while various forecasting methods each have their own advantages and limitations. This necessitates adopting evaluation methods to assess forecasting accuracy.Data lineage emphasizes tracking the entire lifecycle of data, including its sources, processing logic, transmission paths, and assessment of influencing factors. This approach proves particularly suitable for evaluating load forecasting outcomes as it enables comprehensive tracing of data generation and transformation processes. By establishing clear causal relationships between raw data and prediction results, data lineage technology helps identify key factors affecting prediction deviations and optimizes model reliability.This paper proposes a method to evaluate power load forecasting results by analyzing actual load data and forecasted data, supporting power system decision-makers in making more rational decisions.",10.1109/ITAIC64559.2025.11163278,C. Feng; L. Tao; R. Ye; L. Xie; Y. Wang; Q. Feng; L. Yu; Z. Zhang
Electric Semantic Short Packet Communication: A Green ISAC Perspective,2025,-1,Outliers,0.1395613658860905,"Millisecond-precision measurement of smart grid presents elevated demands for 6G sensing and communication capabilities. How to ensure timely, reliable, and green delivery of critical information remains a core challenge. In this paper, we address this issue by studying electric semantic short packet communication from a green integrated sensing and communication (ISAC) perspective. First, a novel information timeliness metric named peak age of incorrect semantics (PAoIS) is developed. It describes the entire lifetime of sensing, compression, encoding, transmission, and decoding. Then, a collaborative problem is formulated to jointly minimize PAoIS and ISAC energy consumption by optimizing sensing frequency and semantic compression ratio. An electric multimodal driven green collaborative optimization algorithm is proposed. It enables dynamical adjustment of sampling ratio of electric multimodal experience samples, enhancing optimization performance under sparse modes. Simulation results verify the effectiveness of the proposed algorithm.",10.1109/ICC52391.2025.11161246,H. Liao; W. Che; Z. Zhou; X. Wang; A. Ali; M. Guizani
Remaining Life Prediction of Complex Devices Based on Mamba,2025,0,Topic_0_prediction_degradation_rul,1.0,"Complex industrial equipment is expensive, so accurate prediction of the remaining useful life(RUL) of complex industrial equipment has important industrial application value. In order to solve the problems of low accuracy of existing prediction algorithms for complex industrial equipment and insufficient fitting of equipment in the decline period, a segmented prediction method of the remaining useful life of complex equipment based on Mamba network is proposed. First, data preprocessing technology is applied to normalize the equipment data from Supervisory Control And Data Acquisition System (SCADA) to eliminate the effect of data magnitude differences; then the Mamba network frame is used to construct the model’s main and end models; the main model performs preliminary life prediction based on preprocessed SCADA data and makes realtime decision-making for model switching; and when the equipment is judged to be in the decay period, the end model is switched to focus on the end life prediction of the equipment to ensure the robust operation of the equipment through accurate prediction; finally, the CMAPSS dataset is used as the experimental data for the test, and the RMSE is reduced to 10.97, which strongly verifies the effectiveness of the proposed prediction algorithm.",10.1109/AEMCSE65292.2025.11042523,S. Lin; L. Zhou; Y. Ke; Z. Wu
Real-Time State of Health Prediction of Lithium Ion Batteries Based on Deep Neural Networks,2025,0,Topic_0_prediction_degradation_rul,1.0,"Data-driven methods have become essential for effective diagnostics and prognostics of lithium-ion batteries throughout their lifespan, from manufacturing to end-of-service. Reliable prediction of battery State of Health (SOH) remains challenging, mainly due to the nonlinear and accelerated capacity degradation arising from complex internal electrochemical processes. In this work, we comprehensively evaluate advanced deep learning models for accurate SOH estimation using multiple health indicators. The proposed framework systematically addresses each step, including data preprocessing, feature engineering, model training, and performance analysis. Results demonstrate that the examined models achieve an average prediction error below 0.69%, highlighting their robustness and potential to enhance real-time battery health management.",10.1109/ICCSC66714.2025.11134871,E. O. Hind; E. H. Ibtissam; M. Tawfik
Towards Proactive Fairness Monitoring of ML Development Pipelines,2025,1,Topic_1_data_big_big data,0.35776683750720195,"Fairness in machine learning (ML) algorithms is a multifaceted issue that necessitates input from technical, social, and legal perspectives. Ensuring effective bias mitigation requires input from multiple stakeholders throughout the development process. However, in practice, development teams, particularly those in small and medium-sized enterprises (SMEs), often lack the resources and expertise to thoroughly analyse and address fairness concerns within their product development workflows. To address this gap, it is crucial to create practical guides or blueprints for product development pipelines. These resources should enable each stakeholder to understand the issues of fairness, and where challenges can arise, as well as provide measurable techniques to address these issues. In this short paper, we explore the viability of a proactive and continuous fairness monitoring flow to seamlessly integrate bias identification and mitigation strategies in an existing product development pipeline. Further, we present open questions related to the implementation and use of this tool, inviting further discussion and research. We particularly focus on curating a set of best practices to establish an effective fairness monitoring flow, aimed to facilitate alignment in the bias mitigation strategies across different teams within the development process.",10.1109/CITRExCompanion65208.2025.10981495,A. Sabuncuoglu; C. Maple
Impact of Industry 4.0 on Organizational Structures,2018,3,Topic_3_industry_manufacturing_chain,0.6802837108335467,"In today's economy, companies have to deal with increasing digital transformation, also known as Industry 4.0. Industry 4.0 is a broad term that encompasses various perspectives and corporate functions. Basically, the term Industry 4.0 describes the networking and the continuous real-time communication of man, machine, products, processes and environment. As a result, products can be managed along the entire value chain and the increasingly individualized customer requirements can be met. Thus, in relation to Industry 4.0, not only the technological side is considered, but also the process organization. The aim of the present paper is to compare German and French companies regarding their organizational structure due to the developments through Industry 4.0. For this purpose, a pilot study was carried out. Representatives of German and French companies were therefore asked about the implementation level of Industry 4.0 in their companies. This survey especially focuses on the dimension of Process Organization, including corporate strategy, work organization and human resource (HR) development. First of all, the literature concerning Industry 4.0 and the dimension of Process Organization will be analyzed. Further, the research approach and methodology will be introduced followed by the field research and data collection. The results will be analyzed in relation to the differences of the two countries. Finally, the results will be summarized together with an outlook on further possible research fields.",10.1109/ICE.2018.8436284,K. Fettig; T. Gačić; A. Köskal; A. Kühn; F. Stuber
"ENVRI-FAIR - Interoperable Environmental FAIR Data and Services for Society, Innovation and Research",2019,1,Topic_1_data_big_big data,0.5566885850653699,"ENVRI-FAIR is a recently launched project of the European Union's Horizon 2020 program (EU H2020), connecting the cluster of European Environmental Research Infrastructures (ENVRI) to the European Open Science Cloud (EOSC). The overarching goal of ENVRI-FAIR is that all participating research infrastructures (RIs) will provide a set of interoperable FAIR data services that enhance the efficiency and productivity of researchers, support innovation, enable data-and knowledge-based decisions and connect the ENVRI cluster to the EOSC. This goal will be reached by: (1) defining community policies and standards across all stages of the data life cycle, aligned with the wider European policies and with international developments; (2) creating for all participating RIs sustainable, transparent and auditable data services for each stage of the data life cycle, following the FAIR principles; (3) implementing prototypes for testing pre-production services at each RI, leading to a catalogue of prepared services; (4) exposing the complete set of thematic data services and tools of the ENVRI cluster to the EOSC catalogue of services.",10.1109/eScience.2019.00038,A. Petzold; A. Asmi; A. Vermeulen; G. Pappalardo; D. Bailo; D. Schaap; H. M. Glaves; U. Bundke; Z. Zhao
Big Data Analytics Technique in Cyber Security: A Review,2019,1,Topic_1_data_big_big data,1.0,"As the new cyber threats are emerging, ""CYBER SECURITY"" becomes a torrid research topic among the researchers to develop a secure and safer social environment where a huge amount of data comes into consideration and management. Big data comes from various sources like Banking, Industries, Hospitals, Social media, Finance and IT sectors etc. In handling this huge amount of data various challenges are also introduced like data life cycle, security, privacy, data processing, scalability and data visualization. Big data analytics resembles the analysis and mining of large amount of data represented in zettabytes. Thus, by proper handling and implementation we can use it at various levels.",10.1109/ICCMC.2019.8819634,N. Srivastava; U. Chandra Jaiswal
Research on Haier COSMOPlat Promoting Industry Upstream and Downstream Collaboration and Cross-border Integration,2020,3,Topic_3_industry_manufacturing_chain,0.7861895765195169,"Under the fourth industrial revolution, how the traditional manufacturing industry moves from the low end of the value chain to the mid-to-high end, and from the traditional standardization and mass production to personalization and small batch customization is the key to the transformation and upgrading of the industry. This paper focuses on how Haier COSMOPlat can facilitate industrial upstream and downstream collaboration and cross-border integration. Through an explanatory single case study method, this paper analyzes how Haier COSMOPlat provides a model and opportunity for transformation and upgrading for industrial manufacturing, as well as how Haier builds a global and industry-wide industrial mass customization platform, successfully achieves cross-domain and cross-industry collaboration, and enables companies to transform and upgrade. This paper summarizes that COSMOPlat not only promotes the upstream and downstream collaboration of the industrial chain, but also promotes the cross-border integration, with a view to providing a new path for the transformation and upgrading of China's traditional manufacturing industry.",10.1109/ICIEA49774.2020.9102092,Y. Xie; Y. Li
Big Data Governance for Building A Smart Cities,2019,1,Topic_1_data_big_big data,0.559774494846833,"There have been many studies conducted related to Smart City, IT Governance and Big Data. In this study aims to find out how the relationship between the three and how to form a framework to explain it. The methodology used is qualitative by looking for some literature on smart city framework, IT Governance framework, and a Big Data framework. From these results, an overall picture of the relationship between the three is concluded, where Big Data has a role in IT Governance. and also the relationship of IT Governance to the realization of Smart City. And the final results of this study produce a framework to explain the relationship between Smart City, IT Governance and Big Data.",10.1109/ICISS48059.2019.8969818,N. Zulkarnain; R. Kosala; B. Ranti; S. H. Supangkat
A Method for Performance Degradation Assessment of Wind Turbine Bearings Based on Hidden Markov Model and Fuzzy C-means Model,2019,0,Topic_0_prediction_degradation_rul,0.8827377965868596,"Bearings used in the wind turbine generators (WTGs) will subject to different degrees of damage during operation, including all kinds of vibration and shock. In this paper, a vibration-based performance degradation assessment method for high-speed shaft wind turbine bearings is proposed using fusion of Hidden Markov Model (HMM) and Fuzzy C-means Model (FCM). The wavelet packet decomposition is used to extract the energy of the wavelet packet nodes of the whole life cycle vibration signal. The autoregressive model (AR) extracts the coefficients and residual of the wavelet packet nodes, and takes the two features as the combined features. The FCM is established using the normal and failure samples and the HMM is established using the normal samples. The two degradation indicators which was obtained by imputing the under test data to FCM and HMM model are input to the FCM model as the input characteristic. Then the performance degradation curve is obtained. Finally, Mahalanobis distance (MD) and FCM models are combined to compare and illustrate. The method combines the advantages of spatial statistical distance model and probabilistic statistical model. Then the WTG bearing's experimental data are used and the experimental results of AR model combined with FCM model are compared to verify the conclusions of this paper. The experimental analysis shows that the method is consistent with the performance degradation trend of rolling bearings and has certain adaptability.",10.1109/PHM-Qingdao46334.2019.8942900,J. Zhou; C. Zhang; F. Wang
Tobacco Industry Data Security Protection System,2019,2,Topic_2_data_privacy_security,0.43278894280338004,"With the rapid development of information technology, large data technology has been fully applied in the tobacco industry, which improves the efficiency of analysis, processing, decision-making, marketing and distribution. However, the current data security is facing a serious threat, which is related to the safety of business secrets. In this paper, through the analysis of security situation and challenge of tobacco industry data, and the study of data processing mechanism used in tobacco industry, a data security protection system covering the data life cycle is put forward.",10.1109/CCOMS.2019.8821674,Y. Jiang; Y. Xu; Q. Xu; L. Fang; C. Lin
Internet of things in Smart City Santiago,2020,3,Topic_3_industry_manufacturing_chain,0.8902238709582805,"Smart City Santiago seeks to solve the main problems faced by Santiago daily, thus achieving that people improve their quality of life. This model is based on the Smart City Málaga model, Spain. In the study, the main challenges facing this type of companies are presented, in term of IoT for the construction of a Smart City. The research focuses on the components that define the IoT of the referential model, such as: sensors, networks and value chain management. The methodology that is applied is exploratory, through a case study that identifies the main challenges presented by Smart City Santiago companies that use IoT in the implementation and development of a Smart City model. This is carried out with interviews with twelve companies participating in the Smart City Santiago project, which allows determining the most important challenges within each component. The results of this research contribute to companies that have an interest, collaborate or intervene in the implementation and development of the model of a Smart City.",10.1109/SACVLC50805.2020.9129894,S. Gutiérrez; J. Serey; D. Fuentealba; I. Soto; I. Jiron; R. Carrasco; G. Gatica
MDV: A Multi-Factors Data Valuation Method,2019,1,Topic_1_data_big_big data,0.6297487309168017,"Data valuation plays a key role in information lifecycle management (ILM). There are two kinds of methods to assess data's value: policy-based method and non-policy-based method. A policy-based method depends on the administrators' principles and the relations between applications in upper layer which makes data value assessment comprehensive. UT (usage over time) which is a typical non-policy-based method, uses visit time to assess data's value. Such ways make data valuation simple and objective. However, as the appearance of big data, the existing algorithms cannot be appropriate to assess data value quickly and accuracy. In this paper, a new one called MDV based on UT method is proposed. MDV takes more factors into consideration, including data's size, age and usages. Such synthetic idea makes the method more suitable for the features of big data. We build an experiment platform based on Hadoop and test the algorithm. The preliminary results show that MDV could achieve the valuation goal.",10.1109/BIGCOM.2019.00016,X. Ma; X. Zhang
Certification Aspects of Model Based Development for Airborne Software,2019,-1,Outliers,0.1308230412529272,"Software in the aerospace industry becomes the integral components of modern day avionic systems and plays a key role in conducting successful missions. Further to that, safety critical systems rely on software components where malfunctioning may result in catastrophic consequences. For example, a failure in the software of flight computers or autopilot systems of a commercial aircraft may end in series of damages or even mass of causalities. Increasing competition in the aviation industry forces companies to develop more capable systems in less time with less cost. In competitive market conditions, companies are in the position of having to use new technologies in their product development life cycle processes. One of these technologies used in the aviation industry is the Model Based Development (MBD) technology. The MBD technology has been widely used in the development of airborne software because of the time and cost advantages that it provides to companies. However, although the MBD technology promises to increase productivity, it also introduces a number of new significant issues that need to be considered when it is used for developing safety critical systems. In this study, two of those issues are explained in an example. The first issue relates to the integrity of model when model is transferred from an unqualified tool to a qualified tool and the second issue relates to the comparison between model coverage analysis and structural coverage analysis. These two issues are also among the most questioned topics by the certification authority for the airborne software if MBD technology is used as they may have safety impacts on flight safety and may affect type certification process in terms of software.",10.1109/INFOCT.2019.8711129,T. Saraç
Analysis on Hotspots of International Scientific Data Management and Sharing Based on Informetrics Atlas,2019,1,Topic_1_data_big_big data,0.46229979135685756,"This research aims to track hotspots of international scientific data management and sharing. The method of informetric analysis is applied in this paper. CitespaceV, an information visualization software, is utilized to quantitatively analyze the relevant subject literature. By analyzing knowledge atlas and studying key articles, the research draws the following result: current research hotspots include theory-oriented research based on open science, service-oriented practice of university libraries, application-oriented expansion of emerging areas, integrated discipline research methods and technology-oriented research.",10.1109/ITME.2019.00142,N. Dong
How South Africa as Emerging Economy are Leveraging Industry 5.0,2025,3,Topic_3_industry_manufacturing_chain,0.8745520919961758,"This study aimed at measuring the impact of industry 5.0 on South African economy. This is the first of kinds due to the fact that it focused on analysing how South Africa as an emerging economy are leveraging the advent of industry 5.0. to date, Manufacturing is the largest contributor to the South African economy, generating employment and prosperity throughout the continent. From 2014 to 2024, the industrial sector consistently represented approximately 27% of South Africa's GDP, with manufacturing contributing roughly 11% of value to the economy. The South African industry is robust, yet it encounters persistent obstacles. It is intensely competitive, yet functions within an increasingly intricate globalised economy. It is a robust exporter, however it is vulnerable to a rapidly evolving geopolitical environment. It is efficient and economical, however susceptible to disruptions in extended value chains. For the sector to sustain profitability in South Africa, it must perpetually adapt to address these evolving issues. This continuous adaptation is achievable just through persistent innovation. Through innovation, South African industry may enhance its efficiency throughout various segments of the value chain, augment the adaptability of its production processes to meet the rapidly evolving expectations of global consumers, and maintain its status as a global benchmark for quality. Innovation will largely stem from the application of increasingly sophisticated digital technologies.",10.1109/SmartNets65254.2025.11106863,N. Y. Mulongo
Combating Dirty Data using Data Virtualization,2019,1,Topic_1_data_big_big data,0.552003245998183,"Data is the new Gold of the modern age. Organizational decisions, artificial intelligence, machine learning, time critical processes, analytics, etc. are driven by highly profiled data. Data gives an organization the ability to act in a corresponding situation and hence, keep up with the market. Financial returns are directly associated with the quality of data being used. Today's world has experienced a data explosion with the advent of data sources like Hadoop, data lakes, clusters and many more. Even more, the day to day devices that we use collect data to present meaningful statistics to us. There is an exponential increase in data volumes which are characterized by heterogeneous data formats and disparate data sources which in turn is complicating the task of providing uniform and profiled data to the business users for analysis. Warehousing is also not able to cope up with the on-demand real-time data access. Holistic view of the entire data is extremely difficult to achieve and takes a considerable amount of time which is slowing down an organization's decision making capabilities. The complexity of the current systems has increased due to the interconnected nature of widely distributed systems which share data. Dirty data i.e. data which is inaccurate, inconsistent, unclean or incomplete is spread all across. As all the data is unrequired garbage data, there is no point in developing your business intelligence over this data. Data Clarity used along with Data Virtualization provides an exceptional solution to these problems by handling the complex data workloads with respect to heterogeneous data and disparate data sources and provides a holistic view of the data to the business organization.",10.1109/I2CT45611.2019.9033690,O. V. Sawant
Design & Development of Misinformation Analysis System for Government Prevention of Public Health Crises,2023,1,Topic_1_data_big_big data,0.6318806059095314,This paper summarizes the design and development of a novel big data pipeline and ecosystem for identifying and analyzing misleading information related to a particular health topic of set of related topics. The objective of this study is to bridge the gap between the epidemiological capacity of ever-increasing amount of social media data pertaining to health topics and the public health officials and policymakers who currently do not have access to it without needing a technical background. The described system has been developed using a fluoride misinformation use case and is being adapted for other prevailing health topics like vaccine hesitancy.,10.1109/WiMob58348.2023.10187823,I. Z. Hussain; J. Kaur; M. Lotto; Z. A. Butt; P. P. Morita
Digital Twin Technology for Integrated Energy System and Its Application,2021,-1,Outliers,0.15759409342972974,"Drived by advanced information technology, the level of digitization and informatization of integrated energy system has been continuously improved. However, there are still many issues in the development and utilization of integrated energy at present. For instance, how to give full play to its technical value and its application in the market still need to be explored. In this paper, the research status of the digital twin technology of the integrated energy system is summarized from the aspects of the integrated energy system and digital twin technology. The basic technical framework of the digital twin technology of the integrated energy system is discussed, and the related technical characteristics are described. Based on this, the application of the digital twin technology of the integrated energy system is further analyzed.",10.1109/DTPI52967.2021.9540160,H. Li; T. Zhang; Y. Huang
Data augmentation for fault prediction of aircraft engine with generative adversarial networks,2021,0,Topic_0_prediction_degradation_rul,1.0,"Fault prediction is to predict the remaining useful life(RUL) of the equipment by constructing models using historical data. However, the run-to-failure data is difficult to obtain, and it is impossible to build an accurate prediction model. To address this problem, an innovative forecasting method based on data augmentation technology is proposed in this paper. First, a generative adversarial network (GAN) is used to study the distribution of the original dataset, and generate a new train set. Then, the original dataset is fused with the new one to train a convolutional neural network and long short-term memory network (CNN-LSTM) prediction model. Finally, the experiment is conducted on the original C-MAPSS and fusion dataset, which results show that the proposed feature extraction method can effectively predict the RUL compared with the existed methods.",10.1109/SAFEPROCESS52771.2021.9693711,P. Lang; K. Peng; J. Cui; J. Yang; Y. Guo
Emerging Technical Debt in Digital Twin Systems,2021,-1,Outliers,0.15704198241791326,"Nowadays, the data silo problem in the industrial systems is a major problem hindering scalable design of data-driven use cases across the value stream of products. Although the notion of digital twins is gaining attention as a solution to address the data silo problem, we observe that various kinds of technical debt may occur in the design of digital twin solutions. Such technical debt can eventually reduce the agility of companies in addressing the changing customers' demands. This paper defines a maturity model for digital twin solutions, and outlines roles and products that emerge in digital twin solutions. These are then used as the basis to identify various kinds of technical debt that we observe emerging in the realization of digital twin solutions in practice. The increasing demand for digital twins in industrial use cases on one hand, and extensive studies on its simulation and analysis aspects on the other hand, have caused practitioners and researchers to overlook the software engineering aspects of digital twins. We believe that this paper helps close this gap by identifying various topics that need to be addressed by the practitioners and researchers to design scalable and maintainable digital twin solutions.",10.1109/ETFA45728.2021.9613538,S. Malakuti
Information Fusion and XGBoost Algorithm Used for Bearing Remaining Useful Life Prediction,2021,0,Topic_0_prediction_degradation_rul,1.0,"To overcome the shortcomings of traditional bearing remaining useful life(RUL) prediction methods, which mainly focus on prediction accuracy improvement, the present work proposed a novel prediction method based on information fusion and XGBoost(eXtreme Gradient Boosting) algorithm. This method divides the full life data of bearings into the performance normal data and the performance rapid degradation data based on the singular value decomposition normalized correlation coefficient, and uses the performance rapid degradation data for feature extraction, then uses the information fusion of distance correlation coefficients and XGBoost feature importance evaluation value to realize feature selection, final establishes a bearing rul prediction model based on the XGBoost algorithm. Results of experiments that carried out using the FEMTO-ST research organization dataset show that it can more accurately predict the RUL of the bearing, and the prediction accuracy is higher than other method such as LightGBM algorithm.",10.1109/CAC53003.2021.9727685,C. Guo; Z. Xu; Q. Yao
State Assessment of Rolling Bearings Based on the Multiscale Bubble Entropy,2021,0,Topic_0_prediction_degradation_rul,0.8413653803779083,"An improved bubble entropy called multiscale bubble entropy (MBE) is proposed based on the multiscale processing, and then the application of MBE in bearing state assessment is investigated. Firstly, the MBE features are extracted from the collected vibration signals of the bearing with the whole life, and then dimension reduction is performed with principal component analysis (PCA). Secondly, a performance degradation indicator (PDI) based on the first smoothed principal component is constructed to represent the bearing condition monitoring. In the following, the fault type of bearings is identified with the principal components of features from different fault types and support vector machine with directed acyclic graph (DAG-SVM). Two groups of experimental data are investigated to illustrate the availability of the proposed feature in bearing condition monitoring and fault diagnosis. The results show that the trend of PDI has good monotonicity to represent the condition monitoring of the bearing, while the accuracy of fault classification is high and stable.",10.1109/ECIE52353.2021.00045,J. Zhang; C. Wang; P. Gui; M. Wang; T. Zou
Pragmatic Online Privacy: the SftE Approach,2021,2,Topic_2_data_privacy_security,0.9286298951367276,"This position paper presents and proposes new requirements for Privacy and Data Protection. We first raise misalignments of current Privacy regulations and argue that current regulatory approaches do not benefit individuals as much as expected to the point that it primarily shields large organisations from ethical management of personal data. From this assessment, we propose the Start-from-the-End (SftE, pronounced “soft”) approach to online Privacy. It puts the focus on the later stages of the lifecycle of Personal Data (such as the Right to Erasure), while removing focus from the points of collection of personal data. The ultimate goal is to reclaim straightforward enforcement and re-empower individuals in a way that is meant to be feasible and practical.",10.1109/EuroSPW54576.2021.00035,V. Jesus
Data Management 4.0 for Financial Service Authority (FSA): A Case Study in Indonesia,2021,1,Topic_1_data_big_big data,0.3594804911067635,"Financial technology-based institutions have grown tremendously in the last five years. Due to its strong growth, Indonesian Financial Service Authority (FSA) such as OJK, has many difficulties to supervise their operations. Studies show many foreign financial technology-based institutions operate in Indonesia and earn many benefits from Indonesian customers. They may conduct illegal operation and many of them operate beyond the knowledge of OJK as a supervision body. For this reason, the article examines the importance of developing data management service that enables to equip FSA to easily supervise all financial institutions. so that all activities in the financial services sector are where Organized regularly, fairly, transparently, and accountably, Able to realize a financial system that grows in a sustainable and stable manner and Able to protect the interests of consumers and society.",10.1109/ICIMTech53080.2021.9535002,N. Legowo; G. Wang; S. A. Hammam; Wirianto; A. Gunawan; H. Girsang
Towards identifying and linking data silos along the software life cycle,2021,1,Topic_1_data_big_big data,0.34081066927710213,"Software is of increasing importance in all industries and it’s efficient creation an important factor in the success of corporations. Using the data generated during the entire software life cycle to create understanding and derive actionable insights, decisions can be made on a factual basis. One of the key requirements to making these objective decisions possible is the collection, composition, and communication of data to the correct stakeholders. To further these goals, the most complete collection of data artifacts available in the software life cycle is presented. These are abstracted to be independent of programming language, development process and toolset. As value is derived from the connection of entities, a set of possible connections is introduced as well as challenges and solutions in their creation discussed. The theoretical observations, and results are verified in the context of a large development organization with more than a thousand developers working from multiple global locations. Our results show that the combination of multiple data sources and their systematic composition are paramount to deriving value from life cycle data in large corporations.",10.1109/ICCSE51940.2021.9569317,B. Martens; P. Pethő; T. Holm; J. Franke
Pathways to Data: From Plans to Datasets,2021,1,Topic_1_data_big_big data,0.4384061617973319,"What is the relationship between Data Management Plans (DMPs), DMP guidance documents, and the reality of end-of-project data preservation and access? In this short paper we report on some preliminary findings of a 3-year investigation into the impact of DMPs on federally funded science in the United States. We investigated a small sample of publicly accessible DMPs (N=14) published using DMPTool. We found that while DMPs followed the National Science Foundation's guidelines, the pathways to the resulting research data are often obscure, vague, or not obvious. We define two “data pathways” as the search tactics and strategies deployed in order to find datasets.",10.1109/JCDL52503.2021.00077,A. Bennett; W. Sutherland; Y. Tian; M. Finn; A. Acker
Key technologies of intelligent interpretation cloud platform for coal mine logging,2021,-1,Outliers,0.3856679933928513,"In order to meet the needs of intelligent mining of coal mine for detecting data and realize intelligent interpretation of coal mine logging, an online intelligent interpretation cloud platform for single well of coal mine logging based on web is designed. Based on Browser-Server (B/S) architecture, man-machine interactive display is realized on the front-end browser, data processing and interpretation are achieved on the back-end cloud server; data management is carried out by using SQL database to facilitate data interaction with other platforms; coal and rock identification and interpretation of intelligent logging is realized by using python; data encryption is designed by using block chain technology to realize data life cycle. The data transmission protocol adopts the general protocol technology, the communication between the front end and the back end adopts the Https protocol, and the data transmission adopts the JSON format, which facilitates the data docking with other platforms. The platform is not restricted by terminal form. As long as there is a browser in the network, it can be operated, used by users. The platform has no version restrictions and is convenient for developers to maintain. The platform construction, front-end design, back-end design, database design and data encryption design method formed in this paper provide new ideas and solutions for the development of intelligent logging interpretation.",10.1109/ICMSP53480.2021.9513387,B. Jiang
Research on Product Life Cycle Data Traceability Based on Multi-Blockchain,2021,-1,Outliers,0.28238137937860536,"Realizing process data traceability of industrial products is an important way to solve the problem of product quality analysis and achieve optimal design. At present, research on product data traceability mainly relies on centralized service provider storage, and product data rarely covers the whole life cycle, resulting in the opacity and incomplete of the product traceability process data. In this paper, blockchain technology is introduced into the traceability management of product data, and a product life cycle data traceability method based on multi-blockchain is proposed. The data sharing and communication mechanisms among blockchains are analyzed. Moreover, the block structure of product traceability data, the upload process of block data and the tracing process of multi-chain product data are elaborated in detail. Finally, based on the open source ethereum architecture, the system application platform is developed to verify the feasibility and rationality of the proposed method.",10.1109/ISRIMT53730.2021.9596963,T. Ding; G. Yan; Z. Zhou; Y. Lei
A Novel Machine RUL Prediction Method in Small Sample based on Fully Convolutional Variational Auto-Encoding Network,2021,0,Topic_0_prediction_degradation_rul,1.0,"The life cycle data of machines are difficult to collected in practice. Therefore, the data-driven remaining useful life (RUL) prognostic face following problems: (1) under-fitting problem in small sample learning, how to (2) extract effective performance degradation features and (3) determine their failure threshold. Here a novel machine RUL prediction method based on deep learning is proposed. Firstly, a fully convolutional variational auto-encoding network is constructed to extract the features to solve problem (1) and (2). Secondly, the features are labeled by the ratio of RUL and total life of machines to overcome problem (3), and input a fully convolutional neural network to predict RUL of machine. Finally, a weighted average method is designed to improve the prediction accuracy. The proposed method is validated on an experimental dataset, and the results show that the proposed method is effective by comparing with the prediction results of SVR, traditional CNN and VAE.",10.23919/ICAC50006.2021.9594066,Y. Zou; K. Shi; Y. Liu; Zhangjidong; Y. Liu; G. Ding
MTS-HMM for Rolling Bearing Health State Assessment,2021,0,Topic_0_prediction_degradation_rul,0.7757283982694309,"Health state assessment is a key technology for system Prognostic and Health Management (PHM) and an important basis for remaining useful life prediction and maintenance decision-making. Rolling bearings are key components of rotating machinery equipment and also one of the most vulnerable components. It has important theoretical and practical significance to evaluate the health state of rolling bearings. In this paper, the empirical mode decomposition (EMD) method is used to extract the vibration signal characteristics of rolling bearings, the dimension of the features is reduced by Mahalanobis-Taguchi system (MTS), and a Hidden Markov Model (HMM) is combined with Mahalanobis distance (MD) to complete health state assessment of rolling bearings. The experimental bearing life cycle data set provided by the Intelligent Maintenance Center of the University of Cincinnati is selected to verify the effectiveness of the proposed method. The experimental results show that the method can detect an early failure and has good sensitivity.",10.1109/ICTC51749.2021.9441645,Q. Yao; L. Cheng; X. Dong; W. Bian
A Privacy Protection Mechanism For Health Big Data Based On Xml,2021,2,Topic_2_data_privacy_security,0.4908367685745362,"With the deepening application of big data technology in the field of health care, the potential risks such as personal privacy and security that may be brought by the collection, analysis and sharing of health data cannot be ignored. How to ensure the safety of health big data and conduct reasonable and compliant analysis and utilization of health big data is an urgent problem to be solved at present. Based on the characteristics of health big data, this paper focuses on the privacy connotation of health big data, puts forward the privacy protection framework of health big data around the privacy protection needs of various stakeholders in the life cycle of health big data, and combs the privacy protection technology system currently available in the field of health care, In order to provide support for each application link of health big data, a set of health data desensitization method based on XML is studied and designed. This method can dynamically add data desensitization strategy, meet the different needs of hospitals for medical record privacy data protection under different application scenarios, and promote the standardized and orderly development of health big data.",10.1109/ICCWAMTIP53232.2021.9674143,Y. Yimei; Y. Yujun; Z. Wang; X. Hongbo; L. Wei
Research on Data Model Construction Method of Enterprise OA System Based on Data Central-Platform,2021,1,Topic_1_data_big_big data,0.5270745133124962,"In recent years, with the rapid development of Internet and AI technology, it has become a consensus that data has become a new asset. How to effectively gather, manage, process and analysis data for enterprise decision-making and business innovation has become the joint direction of academia and industry. From this reason, data central-platform came into being. It provides a different solution for data capitalization and value, and achieves great results. The construction of data model is one of the key factors affecting the effectiveness of data central-platform. In this paper, combined with the characteristics of enterprise OA system, we explore the method of data model construction based on data central-platform, servicing the application of data sharing and value mining.",10.1109/IMCEC51613.2021.9482004,R. Wan; P. Gao; Y. Chen; Y. Liang
Software planning,2021,-1,Outliers,0.13944691043182111,"Software development is an extremely complex, complicated process, and due to the extreme complexity of the products, testing cannot be complete at any level. The required quality depends on the rules and standards of the development process and compliance to them.One of the most important steps of this process is planning. Planning is also based on a standard and set of rules in all safety-critical cases.The article deals with the most important plans and their significance, considering the aspects of safety-critical development.",10.1109/CANDO-EPE54223.2021.9667863,G. Schuster
The Design and Implementation of Visual Simulation Software for WSNs Communication Link,2021,1,Topic_1_data_big_big data,1.0,"On the basis of analyzing the distribution model and communication protocol structure of WSNs communication mechanism, considering the real-time communication processing and human-computer interaction of Wireless sensor network, a visual simulation software for WSNs communication link based on $\mathbf{MATLAB}/\mathbf{VC++}$ has been designed, this simulation software has mainly designed the function, user interface and software flowcharts. The test results show that the software has strong real-time and human-computer interaction, and can provide technical support and reference for the optimization of communication link in WSNs through the configuration of different modes and parameters.",10.1109/ICNISC54316.2021.00140,X. Zhang; Z. Fan; W. Xu; L. Zhuo
Research and application of supervision mechanism for traction battery recycling based on big data technology,2021,3,Topic_3_industry_manufacturing_chain,0.38278134446557643,"With the rapid development of new energy vehicle industry, the recycling of automotive traction battery has become the focus of resource sustainable development. In order to build the whole life cycle digital management system, this paper innovatively designs data management system of the traceability of the traction battery, achieving the efficient management of traction battery life cycle data and providing data support for industrial policy research.",10.1109/ICISCAE52414.2021.9590683,C. Ping; J. Xiaotong; L. Longhui
Security in Big Data Health Care System,2021,2,Topic_2_data_privacy_security,0.6344997183858719,"There has been a fundamental shift in the way firms in every industry manage, examine, and utilize their data. Health care is one of the most promising industries in which the use of big data may make a positive impact. Healthcare technology is being improved at a fast rate as an outcome of growing information and innovative innovation. In healthcare, there are different articles of big data. Digital medical data, biometric data, medical image processing, biosensor data, physician data, patient information, and administrative data are examples of these types. Many combined technologies are being deployed to modify healthcare systems in the COVID-19 pandemic. The security of medical data is required for the management of an integrated healthcare solution. In this paper, we found that many researchers face significant hurdles in encrypting sensitive patient information to prevent misuse or leakage. Our aim is to provide a focus on security issues in healthcare system and try to give a solution.",10.1109/ICIIP53038.2021.9702663,R. Verma; R. Bhatt
Building a Secured Data Intelligence Platform,2021,2,Topic_2_data_privacy_security,0.6391660306755504,"The Salesforce Unified Intelligence Platform (UIP) team is building a shared, central, internal data intelligence platform. Designed to drive business insights, UIP helps improve user experience, product quality, and IT operations such as in [1]. At Salesforce, Trust is our number one company value and building in robust security is a key component of our platform development. In this paper, we'll share our experience and learnings relating to security design, covering topics such as data classification, data encryption, network security, authentication, data access, multi-tenancy, data environments, and third-party software.",10.1109/CloudIntelligence52565.2021.00017,C. Yang
Research on data security sharing mechanism and application based on blockchain and privacy computing,2022,-1,Outliers,0.19445552272150474,"The rapid growth and massive accumulation of various types of data have driven all walks of life to achieve high-quality development, and have had a major and profound impact on social governance and people’s lives. However, there are many problems in the traditional data transaction and sharing, such as the loss of data ownership and the leakage of original data information. How to ensure the invisibility of data and the determination of data ownership while sharing data is an urgent problem. Based on this, combining the advantages of emerging technologies such as blockchain and privacy computing, this paper proposes a data security sharing architecture and mechanism based on blockchain and privacy computing to ensure the effective protection of personal information in the process of data sharing, to realize the safe circulation and sharing of data.",10.1109/IAECST57965.2022.10062202,B. Ma; C. Chi; J. Tian; Y. Zhang
RLWE-based Privacy-Preserving Data Sharing Scheme for Internet of Medical Things System,2022,2,Topic_2_data_privacy_security,0.6125122707148128,"Internet of Medical Things (IoMT) system is a collection of devices and software which are applied in medical care domains. In a IoMT system, patients and doctors conduct remote diagnosis and treatment through a health data sharing scheme. These data sharing schemes faces privacy challenges if the medical information gets leaked. However, the existing privacy-preserving data sharing schemes usually use classical encryption/signature algorithm, which will turn to be not secure under the development of quantum computer. In this paper, we propose a privacy-preserving data sharing scheme using lattice-based cryptography, the ring learning with errors (RLWE) cryptography. Our scheme integrates both quantum-resistant encryption and digital signature algorithm based on RLWE, in order to realize the resistance to quantum attack for the whole life cycle of data. In addition, the performance evaluation shows that the proposed scheme gets 75.7% higher efficiency than the existing schemes.",10.1109/CECIT58139.2022.00082,J. Wang; L. Liu
Towards Continuous Plant Bioimpedance Fitting and Parameter Estimation,2021,-1,Outliers,0.3496191463060174,"The push to advance artificial intelligence, internet of things, and big data analysis all pave the way to automated and systematic optimization in precision agriculture and smart farming applications. These advancements lead to many benefits, including the optimization of primary production, prevention of spoilage via supply chain management, and detection of crop failure risk. Noninvasive impedance sensors serve as a promising candidate for monitoring plant health wirelessly and play a major role in this optimization problem. In this study, we developed a software pipeline to support impedance sensing applications and, as a proof of concept, applied this to track longitudinal consistent bioimpedance data from the V4 leaf midrib in maize plants. The script uses the single-shell equivalent circuit model to represent the extracellular fluid, cellular membrane, and intracellular fluid as a simplified resistive-capacitive circuit, where these elements’ parameters are estimated with complex nonlinear least squares. The double-shell model extends the single-shell model to account for the effects of the relatively large plant cell vacuole. Limit cases for impedance are utilized for specific parameters as an alternative method of estimation. We investigated a complex analysis-based modification to the objective function and model optimization for the data pipeline automation. Various weighing functions are applied and checked against one another. Additionally, a custom graphical user interface was developed to assist with parameter initialization for correcting potential convergence issues and understating the influence of each parameter on the dataset. We demonstrated that the analysis of an example longitudinal dataset was able to reveal a time series for parameter fitting.",10.1109/SENSORS47087.2021.9639492,D. Martin; J. Reynolds; M. Daniele; E. Lobaton; A. Bozkurt
NASA'S Advanced Information Systems Technology (AIST): Combining New Observing Strategies and Analytics Frameworks to Build Earth System Digital Twins,2022,-1,Outliers,0.24439282390120082,"NASA's Advanced Information Systems Technology (AIST) Program is one of several Technology programs managed by the Earth Science Technology Office (ESTO) in the Earth Science Division (ESD). The AIST Program focuses on advanced information systems and novel computer science technologies that will be needed by NASA Earth Science in the next 5 to 10 years. The three main thrusts of the AIST Program deal with New Observing Strategies, Analytic Collaborative Frameworks and Earth System Digital Twins. This paper describes these three thrusts and how they work together to create future Earth Science information systems.",10.1109/IGARSS46834.2022.9883640,J. L. Moigne
Analytics and Blockchain for Data Integrity in the Pharmaceuticals Industry,2022,-1,Outliers,0.29704628050304044,"The data quantity explosion that we witnessed during the last two decades has lead industrial organizations to exploit this sheer amount of data for tasks that previously would seem impossible. However, larger data volumes, draw together a series of drawbacks that affect data quality and integrity and this becomes more evident in government supervised industrial settings. In many of such cases, public authorities have defined sets of principles (e.g., ALCOA+) regarding data management that industrial organizations must abide to, and either deliberate or not violation of these standards most often comes with severe legal consequences. As a matter of fact, in an effort to follow as much as humanly possible to such principles, pharmaceuticals industries invest heavily in resources to maintain high quality standards in their data urging for automated methods for calculating, monitoring and predicting compliance.Also, in complex manufacturing and production lines, data analytics provide means for real-time and continuous monitoring of large numbers of sensor variables and categorical or numerical values where higher order conclusions can be derived and taken into account when business process optimizations are considered. In this work we present an easy-to-use integrated platform for real-time raw sensor data monitoring and pre-processing in pharmaceuticals production lines combining blockchain storage for data integrity and deep-learning capabilities for data analytics. Additionally, the platform is able to calculate, monitor and predict compliance to the ALCOA+ set of principles, reducing substantially the time and effort needed to maintain and calculate such complex parameters manually.",10.1109/CTISC54888.2022.9849776,I. Kavasidis; E. Lallas; V. C. Gerogiannis; A. Karageorgos
Blockchain Based Trustworthy Digital Twin in the Internet of Things,2022,-1,Outliers,0.2579739630983663,"The combination of digital twin technology and the application of IoT(Internet of things) has been widely used. It manages the whole life cycle of IoT data by establishing a digital twin system of physical system. However, extracting effective insights from the collected data requires an infrastructure that can ensure the credibility of the data. We propose a data security sharing architecture based on dual Blockchain to solve the storage and security problems when data is invoked by third-party services. The architecture is applied to the Internet of things system based on digital twin to solve the data security transmission between physical system, digital twin system and IOT application system.",10.1109/ICIPNP57450.2022.00040,Q. Zheng; J. Wang; Y. Shen; P. Ding; M. Cheriet
Challenges to Adoption of Digital Agriculture in India,2022,3,Topic_3_industry_manufacturing_chain,0.3147991712984655,"There is a paradigm shift due to digital technology disruption in agricultural value chain. In the current context, digital agriculture is indispensable for sustainable agriculture, farmers’ livelihood and inclusive growth. Application of AI, Machine Learning, Deep Learning, IoT enabled sensors, drones and cobots are some key technologies which have vital roles in modern practices of agriculture. However, Adoption of digital agriculture in India is at a nascent stage. It is necessary to address multiple challenges to adoption of digital technology to achieve sustainable developments goals in India. The aim of the study is to decipher the challenges of digital technology adoption in India and to apply Interpretive Structural Modelling which ranks and finds out the inter dependencies among the key challenges. ISM modelling as a qualitative modelling tool develops a hierarchical structure among the key challenges. The research work limits the study in India. The study is first of its kind which applies ISM as a qualitative modelling tool to challenges to digital agriculture in Indian context.",10.1109/ICMIAM56779.2022.10147002,J. Hota; V. K. Verma
Data Governance Maturity Assessment: A Case Study Directorate General of Corrections,2022,1,Topic_1_data_big_big data,0.3409901532443339,"Data becomes a high-value asset or a new type of wealth. Valid data is one of the keys to government organizations carrying out public services. The administration of the government by using information technology is a challenge for the Indonesian government. The electronic-based government system policy aligns with one Indonesian data set to accelerate the implementation of information technology-based public services and improve data management problems. The Directorate General of Corrections (GDC) is one of the implementing elements of the Ministry of Law and Human Rights (MHLR), responsible for formulating and implementing policies and technical standardization in the correctional sector. Correctional institutions as one of the pillars of law enforcement in Indonesia. Efforts to change the process of handling criminal cases based on information technology have been carried out by the government, which helps accelerate law enforcement officers' tasks in handling quality cases. An integrated system for handling criminal cases based on information technology performs a data exchange mechanism between law enforcement officers to fulfill the values of justice, legal certainty, and expediency. The report on implementing the national strategy for preventing corruption in 2021, strengthening the system for handling criminal cases in an integrated manner based on information technology, got 27.8% results. To improve data quality in data exchange, good data management is needed. Therefore, this study aims to measure the maturity of the data governance program at DGC and provide corrective measures to increase the maturity of the data governance program. The Stanford data governance maturity measurement tool in this study was used to measure the maturity of the data governance program. The results obtained are the maturity level of the data governance program with an average of 1.93 (initial), with the foundation component having an average value of 2. On the other hand, the project component has an average value of 1.82. Recommendations for increasing the maturity level of data governance at level three consist of thirty recommendations.",10.1109/ICISS55894.2022.9915243,I. Mirza Harwanto; A. Nizar Hidayanto
Assessment of the quality of user awareness of GDPR in healthcare IOT,2022,2,Topic_2_data_privacy_security,1.0,"This analysis of study results examines the impact of the GDPR on connected medical technologies. Building on these findings, elements are identified that lead to lawful and responsible processing of personal data. Key observations reveal that effective privacy compliance suffers from ever shortening innovation cycles, frequent introductions of hardware- and software and the lack of privacy by design integration in the end-to-end data processing lifecycle. Further, the GDPR awareness of users has a mutual effect on organizational accountability. A symbiosis of psychological, legal and technological parameters provides the framework for GDPR compliance.",10.1109/BIA52594.2022.9831287,K. Ider
Research on Data Exchange Schema for Railway Infrastructure,2022,1,Topic_1_data_big_big data,0.4576532218539433,"Building Information Modeling (BIM), as an effective means to improve the efficiency of building design and construction, is gradually receiving widespread attention. However, in the railway industry, BIM is still in its infancy in terms of infrastructure data transmission, and has not yet formed a general data exchange mode that can fully meet the full life-cycle data transmission needs of railway infrastructure. According to the requirement of data sharing in the full life-cycle of railway infrastructure, this paper proposes a railway infrastructure data exchange model. By referring to the data modeling methods of Industry Foundation Classes (IFC) and Railway Markup Language (RailML), this paper proposes a metadata definition method of railway infrastructure and a method of automatically generating data exchange schema based on the data exchange model. Application results show that the proposed data exchange schema can provide an efficient and convenient way for data sharing in the full life-cycle of railway infrastructure.",10.1109/IAEAC54830.2022.9930074,S. Gu; B. Liu; X. Lv; H. Li; R. Wang
RUL Prediction Based on Improved LSTM Network Structure,2022,0,Topic_0_prediction_degradation_rul,1.0,"Using the sparse idea of Highway network to design a sparse denoising LSTM network that suppresses redundant neurons to achieve more accurate residual life (RUL) prediction. Different from the idea that the traditional Highway network is sparse in time direction, this paper transforms the traditional LSTM network by designing sparse gates, and suppresses those neurons that have contributed little to the next layer in the previous layer, and highlights those nerves that contribute more. The role of the element, thereby achieving the goal of sparseness and "" denoising "" at the same time. When the time series is long, the prediction accuracy of RUL prediction using the sparse denoising LSTM network (Sparse Denoising LSTM, SD-LSTM) is high, and the sparse gate structure can also reduce the computational complexity to a certain extent.",10.1109/CCDC55256.2022.10033753,P. Hu; Z. Li; D. Tian; J. Zhang
An Approach of Rolling Bearing Remaining Useful Life Prediction based on Adaptive Ensemble Model,2022,0,Topic_0_prediction_degradation_rul,1.0,"Rolling bearing is an important component of rotating machinery equipment. Predictive maintenance can reduce unplanned maintenance expenditure and effectively improve the reliability of the equipment. The bearings produced in one batch may present completely different degradation trends, which increases the difficulty of tracking the degradation trend and predicting the remaining useful life (RUL) of bearings. Therefore, a RUL prediction method based on adaptive ensemble model is provided in this paper. First, an adaptive features integration technology is designed to construct a comprehensive health indicator with feature set in the time-frequency domain, which enhances the performance of health indicators to characterize the health status of bearings. Second, a dynamic ensemble model is further developed to predict RUL, which enhances the ability of the model to track different degradation trends and adaptively adjust the model parameters according to specific trend. A bearing life cycle dataset is applied to demonstrate the superiority the proposed approach.",10.1109/ICSMD57530.2022.10058425,W. Li; D. Liu
Minimization of Information Losses in Data Centers as one of the Priority Areas of Information Security Technologies,2022,-1,Outliers,0.27460412356289604,"This article describes the minimization of information losses in data centers as one of the priority areas of information security technologies. It is determined that the model of DLP strategy includes the following stages: data management, DLP management and information security support, which includes system lifecycle data (SDLC). The data management phase consists of developing data center standards and policies, identification models, risk assessments, classification development, architecture construction and quality assessment methodology. Thus, after identifying the types of data to be protected, the place of this information in the organization's IT infrastructure should be analyzed, dividing the data into those stored in structured repositories (e.g relational databases controlled by the IT organization) and unstructured end-user repositories on network resources and workstations. Carrying out this classification is a basic measure of preparation for monitoring the data center network.",10.1109/PICST57299.2022.10238649,Y. Shestak; S. Toliupa; A. Torchylo; O. J. Onyigwang
Artificial Intelligence Vis-à-Vis Data Systems,2022,1,Topic_1_data_big_big data,0.44883483811300584,"NASA Earth Science Data Systems (ESDS) program manages a full lifecycle of data collected by all Earth science missions. ESDS also develops capabilities optimized to support rigorous science investigations. As technology landscapes evolve, ESDS has also evolved to transform its internal services and enhance external user centric services. This paper describes how ESDS is (i) adopting artificial intelligence (AI) technology to improve core services and (ii) enabling its users to advance AI driven research and build applications.",10.1109/IGARSS46834.2022.9883626,M. Maskey; R. Ramachandran; I. Gurung; M. Ramasubramanian; A. Koul
Privacy security protection based on data life cycle,2022,2,Topic_2_data_privacy_security,1.0,"Large capacity, fast-paced, diversified and high-value data are becoming a hotbed of data processing and research. Privacy security protection based on data life cycle is a method to protect privacy. It is used to protect the confidentiality, integrity and availability of personal data and prevent unauthorized access or use. The main advantage of using this method is that it can fully control all aspects related to the information system and its users. With the opening of the cloud, attackers use the cloud to recalculate and analyze big data that may infringe on others' privacy. Privacy protection based on data life cycle is a means of privacy protection based on the whole process of data production, collection, storage and use. This approach involves all stages from the creation of personal information by individuals (e.g. by filling out forms online or at work) to destruction after use for the intended purpose (e.g. deleting records). Privacy security based on the data life cycle ensures that any personal information collected is used only for the purpose of initial collection and destroyed as soon as possible.",10.23919/WAC55640.2022.9934483,H. Zhang; S. Cheng; Q. Cai; X. Jiang
Research on Logistics Big Data Asset Management and Data Mining Based on Particle Swarm Optimization,2022,3,Topic_3_industry_manufacturing_chain,0.5571059222111366,"The increasing progress of mobile Internet provides more opportunities for asset management technology, such as the rise of Internet of things, cloud computing and big data technology. In the field of logistics distribution, with the increasing transaction volume generated by mobile devices, the huge amount of data generated continuously also needs more and more efficient and practical data processing methods. In the field of logistics, logistics big data asset management has also become a new technical means to optimize logistics management and improve distribution efficiency. With the increasing progress of logistics informatization, the scale of logistics data increases geometrically. At present, many logistics information platforms have huge databases, but the data in them are lack of effective analysis and mining, which hides the law and value behind the data. Therefore, it is very necessary to establish a data mining (DM) system based on logistics information platform. According to the characteristics of large amount of cloud data and complex structure, this paper plans and implements a system that can manage the whole life cycle of data from collection to processing, then to cleaning, and finally to production, and capitalize these disordered data.",10.1109/AIARS57204.2022.00094,S. Guo
Rolling Bearing Health State Assessment Based on K-Means and Ensemble HMM,2022,0,Topic_0_prediction_degradation_rul,1.0,"The division and identification of rolling bearing health states are the basis for Condition-based Maintenance, which effectively guarantee the safe and stable operation of the equipment. In order to accurately divide the normal and failure states, analyze failure occurrence time and identify the current state, the K-Means clustering method is used to cluster the data of the full life cycle, and the ensemble Hidden Markov Model (HMM) method for pattern recognition of online data. The experimental bearing life cycle data set provided by the Institute of Design Science and Basic Component at Xi’an Jiaotong University (XJTU) and the Changxing Sumyoung Technology Co. Ltd. (SY) is selected to verify the effectiveness of the proposed method. The results show that the data consist of different states can get a good clustering effect and each state data can also be accurately identified.",10.1109/ICTC55111.2022.9778234,X. Cai; L. Cheng; Q. Yao
Digital transformation risks and countermeasures of small and medium-sized cross-border E-commerce enterprises,2022,3,Topic_3_industry_manufacturing_chain,0.9482564980305405,"The existing trade platforms for small and mediumsized cross-border e-commerce enterprises can no longer meet the growing demand of business volume, which need to implement digital transformation urgently. By investigating the current situation of digital transformation of small and medium-sized cross-border e-commerce enterprises, this paper identifies and analyzes the potential risks at different stages of the whole life cycle of digital transformation data from the perspective of digital economy, and puts forward risk response suggestions for digital transformation of small and medium-sized cross-border ecommerce enterprises in order to guide the smooth implementation of digital transformation of small and mediumsized cross-border e-commerce enterprises.",10.1049/icp.2022.2922,H. Chen; A. Liu
Research on the Construction of Data Governance System in Vocational colleges under the Perspective of Big Data,2022,1,Topic_1_data_big_big data,0.38412779565149296,"Data governance in higher education is a new stage in the current development of education informatization. To strengthen the effective governance of education data is an important means for vocational colleges to comprehensively improve their core competitiveness and realize the modernization of internal governance ability. In this paper, we start from the problems of data management in vocational colleges and builds a ""trinity"" of data governance system covering data management system, data standard system, data technology system and data application services. It will ensure the smooth flow of the basic elements of the smart campus system, improve the quality and efficiency of the core business of the school and provide reference for the data governance construction of other vocational colleges.",,B. Wang
Leveraging Experience Telemetry: Use Cases,2022,-1,Outliers,0.2891314241107567,"Experience Telemetry is a branch of Telemetry that collects non-operational telemetry data from IT infrastructure in order to facilitate the work of roles such as architects, product managers, customer success and user experience planners, in any organization. These roles have interests geared more towards the business rather than operational utility of any product or service. Such data is consumed by various management applications to provide analytics and insights for the above mentioned users. The novelty in this paper lies in the demonstration and qualitative evaluation of how Experience Telemetry concepts can be applied in solving two specific lifecycle management and operations problems, thus advancing the current state-of-practice.",10.1109/NoF55974.2022.9942501,S. Kumar; M. Beverley; D. Engi; S. Grimee; N. K. Nainar; M. Palmero; G. Salgueiro; Y. Viniotis
"From silos to open, federated and enriched Data Lakes for smart building data management",2023,1,Topic_1_data_big_big data,0.4373705496845878,"Current building data is treated as silos from the different building domains. However, this provokes the lack of cross-domain data mixture to provide added-value services, mainly due to lack of interoperability. Data quality is also an issue when collecting data from buildings. The proposed data lake aims to solve these challenges by considering the whole data life-cycle to ensure minimum data quality requirements, providing high-quality services to make better-informed decisions. Heterogeneous building-related data is thus combined to enrich the information, being able to address multiple stakeholders in the smart building context. The data lake is being deployed in the DigiBUILD project, where data from 10 pilots with different purposes are collected to demonstrate the capability and benefits of its application.",10.1109/MetroLivEnv56897.2023.10164046,J. L. Hernández; S. Martín; V. Marinakis; I. de Miguel
Research On Privacy Protection Scheme For Educational Data Based On Blockchain,2023,-1,Outliers,0.18875529613149677,"This study aims to explore the issue of educational data privacy, based on the “privacy computing + blockchain” technology system, and analyze its operating mechanism from the perspective of the entire life cycle. With the advancement of educational digitization, privacy protection is facing huge challenges. This paper analyzes the various educational data subjects and their relationships. In order to cope with this challenge, the article starts from the technical direction of privacy computing and introduces the technical process of “privacy computing + blockchain”. The constructed technical system integrates the security of blockchain and the protection of privacy computing, with special emphasis on data security and trusted computing, thus promoting the harmonious and longterm development of the educational data society.",10.1109/CCSB60789.2023.10398775,X. Gao
Model for Verifying the Reliability of Candidate Data Based on Blockchain Technology,2023,-1,Outliers,0.31383244428352336,"Big data is one of the most prominent technologies today and is applied in different areas of society. Big data is all information, collected from different sources and continuously updated over time. Therefore, big data has many challenges that make it difficult to apply in practice. One of the challenges that many researchers care about and find solutions to overcome is to ensure the reliability of the data. Meanwhile, blockchain technology emerges with a transparent, immutable, and secure data storage solution. Therefore, we focus on researching and developing a solution to verify the reliability of data based on blockchain technology. We analyze candidate data provided from multiple sources and find solutions to verify the reliability of this data to meet the urgent needs of employers looking for high quality human resources. Currently, on the internet, many systems provide an environment for information exchange between candidates and employers. However, it is difficult to verify the reliability of this information. Therefore, recruiters spend a lot of time checking the reliability of data provided by candidates. In this paper, we propose a model for verifying the reliability of candidate data based on blockchain technology (VRCD-BT). The system built according to this model can become an effective bridge between candidates and employers. Highly reliable candidate data creates many opportunities for candidates and employers to easily collaborate and grow. Thereby, contributing to the development of a sustainable economy operated by high-quality human resources.",10.1109/ICCAE56788.2023.10111120,T. Nguyen; K. T. VO; T. -T. Ta; T. -A. Nguyen-Hoang; N. -T. Dinh
Cloud Data Security using Balanced Genetic Algorithm,2023,2,Topic_2_data_privacy_security,0.82041724988953,"In computer science, data plays a vital role in accessing the core architecture of any product. Nowadays, cloud computing has emerged as a new paradigm to store data and services. It provides a computational architecture where users can send their data and performs any operation on it. with the growing demands of the cloud, it has become a major area of concern as it contains sensitive data. Data protection and maintaining the integrity of data are the key issues in the cloud due to its capability to store information at different geo-locations. The data security model of the cloud needs special attention as new threats and vulnerabilities are getting introduced day by day. The available standard encryption algorithms that provide data security, have their own limitations and must be addressed at any cost. We are proposing a methodology called the Balanced Genetic Algorithm (BGA) to address the underlined problem statement of data security in the cloud. The idea is to balance the existing genetic algorithm to provide scalability, enhance security, and decouple the data life cycle from the core encryption terminology. With our experimental result, we have shown that the proposed idea has better performance than the current standard encryption techniques.",10.1109/ICEES57979.2023.10110211,R. R. Prasad; A. Kumari
Avionics Linux,2023,-1,Outliers,0.21427843344043085,"Linux is a well-known and widely used operating system, even in some safety-critical domains. Though rare, Linux has been used in aerospace, certified to DO-178C Software Level D. For broader use in aerospace, Linux must address at least four challenges: it must be fast, deterministic, embedded, and assured. This paper will briefly review the first three: the need for high performance, the need for real-time determinism for worst-case timing, and the need to target specialized embedded computer processing hardware. Then, the challenges related to assurance will be examined in more detail, along with three candidate approaches for the assurance of existing software: field service history, overarching properties, and reverse engineering. Each of these methods is investigated, concluding that reverse engineering is likely the most viable solution.",10.1109/DASC58513.2023.10311247,S. H. VanderLeest
Outlining the Open Digital Twin Platform,2023,-1,Outliers,0.23912362999031328,"Complex simulations and machine-learning models increase in application in research, industry, and governance. However, applying these systems with reasonable accuracy and efficiency requires large-scale efforts of data collection, data transformation, data analysis, and data visualization. At the same time, maintaining the required infrastructure, software, and personnel skyrockets making these tools unavailable to many potential users. The paradigm of the digital twin offers a novel perspective on how to manage the data efficiently and make these systems available more steadily at a lower cost. We introduce the first prototype of the Open Digital Twin Platform (ODTP) that is designed to be openly available to all interested parties to enable a common framework and baseline for digital twin based research. ODTP uses containerization, loose coupling, and micro-services to provide dynamically composable digital twins. ODTP also provides tools for licensing resolution, privacy and access control, and reproducibility. In its first iteration presented here, ODTP implements a common mobility research pipeline of the eqasim pipeline for MATSim. These kind of programs are usually difficult to assemble and use, thus leading to dangerous versions of ""never change a running system"". ODTP converts them into an easy-to-use version making it possible to initiate mobility simulations with one click. ODTP enables the quick adding of relevant data sources and analytical pipelines related to any topic and make them easily usable, accessible and shareable to research, industry, and governance. Thus, ODTP expands the FAIR principle from data to the complete data life cycle.",10.1109/SWC57546.2023.10448743,J. Grübel; C. V. Rios; C. Zuo; S. Ossey; R. M. Franken; M. Balac; Y. Xin; K. W. Axhausen; M. Raubal; O. Riba-Grognuz
Exploring The Use of Blockchain Technology in Cybersecurity and Data Science,2023,-1,Outliers,0.28364354393388386,"Blockchain technology has demonstrated to be extremely efficient at processing distributed transactions in a secured manner. It consists of a wide range of applications. B. To handle smart contacts and Bitcoin cash. Blockchain systems may allow automatic data exchange and thinking about, resulting in increased effectiveness and lower costs. This is made possible by the adoption of contract technology and decentralized ledgers. Additionally, blockchain technologies can enhance integrity and security of data, enabling more precise and reliable analysis of information. Ethereum along with information science integration may make it easier to create decentralized apps, opening up novel industries and income sources. However, there are still issues with adaptability, interconnection, and complexity in the combination of blockchain technology and data science. For this type of technology to achieve all of its potential advantages, it is imperative that studies and advancements in this field persist. Blockchain applications for data science are currently being investigated. This essay examines how blockchain technology is used in cybersecurity and data science.",10.1109/ICACITE57410.2023.10183132,R. Rahmat; M. S. Abbas; M. Nordin; Y. Yunus; S. Muhammad; A. F. Ismail
Research on Intelligent Perception and Supervision for Data Circulation Security Based on Block-Chain,2023,2,Topic_2_data_privacy_security,0.4331357990476051,"The data of the government and enterprises, as the production factors are facing risks and problems of security violations, such as data leakage, data abuse and data tampering during quick circulation. This paper studies the security supervision architecture of data circulation (exchange, sharing, transaction) from the perspective of the whole life cycle, proposes and constructs the security supervision metadata model, which is used to represent the changes of users, behavior, data lineage, etc. during the whole life cycle of data; For massive data, based on the metadata model of security supervision, innovates the key technologies such as data security monitoring, tracing and ownership authentication; Per the verification need, a set of security supervision prototype showing security situation, tracing performance, ownership construction/authentication and low-level visual explorer is developed.",10.1109/ICCCBDA56900.2023.10154854,H. Yang; Y. Cao
The Application of Data Governance in Universities,2023,1,Topic_1_data_big_big data,0.4513278320681425,"With the wide application of information systems, the universities have accumulated a large number of data resources. Due to the lack of effective data governance, such problems as the difficulty in data sharing, low data quality, lack of security, poor utilization ability, etc. are more and more prominent. In order to solve these problems, this paper designs a university data governance platform, which realizes the whole life cycle management of data, and thus can improve the data management ability, eliminate data islands, and fully exploit the potential value of data. This paper expounds the architecture, the resource development process, the main functions and the typical applications of the data governance platform.",10.1109/ICAIBD57115.2023.10206371,H. Jiang; W. Yan
Data Mapping and Management Method for Knowledge Mining in the Full Lifecycle of Equipment,2023,3,Topic_3_industry_manufacturing_chain,0.2817883409534531,"The full lifecycle data of complex equipment contains a wealth of knowledge, which can effectively support equipment design, manufacturing, operation and maintenance activities through knowledge mining and services. Complex equipment is characterized by intricate physical structures, extended lifecycles, and interdependent stages. Establishing mapping relationships between multi-source and heterogeneous data throughout the entire lifecycle represent a prerequisite for knowledge mining, and they are pivotal in improving the quality of knowledge extraction. To address this problem, this study proposes a data mapping method for the full lifecycle of equipment based on meta-structure trees and encoding. First, using the equipment’s smallest design units, manufacturing units, and maintenance units as leaf nodes, an equipment metastructure tree is established and uniformly encoded. Next, a data association mapping mechanism based on the meta-structure tree is proposed for each lifecycle stage. Finally, a data management method based on fact constellation is proposed to achieve the associated mapping of complex equipment full lifecycle data and provide support for deep data mining.",10.1109/SWC57546.2023.10448723,Q. Zheng; G. Ding; S. Wang; J. Zhong; P. Teng
"The Lifecycle of EEG Data: Acquisition, Analysis, and Future Prospects",2023,-1,Outliers,0.4240508794293501,"The landscape and ecosystem surrounding modern EEG is rapidly evolving, in tandem with the wider technological advancements seen in the healthcare and broader technology industries. Expanding computational and storage capabilities are enabling researchers to study EEG datasets at formerly unprecedented scale and levels of efficiency. For many patients and their families, the recommendation and referral for a clinical EEG assessment is just the beginning of what may well prove to be a lifetime journey of clinical coordination or disease state management. As any parent or patient will attest – this population deserves the very best that the human collective has to offer, and the industry solution providers must now lean-in with conviction to enable the academic researchers who are pushing those limits. In this presentation, we’ll begin by briefly examining the lifecycle of EEG data, from its acquisition and storage to its eventual analysis and presentation in a variety of clinical use cases. Specifically, we will take a diagrammatical look at the journey of the data itself, including how this data is represented at rest and inflight. We will follow up by addressing several nuances of clinical EEG data collection as well as its subsequent analyses. From here we will focus on a handful of touch points throughout the data flow diagram where opportunities for clinical presentation arise. These opportunities range in complexity from simple on-board impedance checking interfaces and computer-based representations of the raw data, to complex visualizations of data trends and even cross-sectional comparisons to data segments collected from other patients, as seen most recently in a collection of academic research initiatives. A concrete goal of this presentation is to provide a modern understanding of where EEG data flows, how it is represented, and what opportunities arise for both immediate research and new forms of clinical presentation. With the changing tides of the clinical EEG landscape in mind, we will specifically showcase how academic researchers can begin leveraging and contributing to both real-time and completed study SDKs in collaboration with members of the Research and Advanced Development team at Natus Neuro, to enable next-generation breakthroughs in clinical research.",10.1109/SPMB59478.2023.10372605,J. Salazar
Reference model for personal data protection in the Peruvian microfinance sector,2019,2,Topic_2_data_privacy_security,0.7763520021730252,"This study proposes a reference model that provides 16 control groups based on legal framework, international standards, project frameworks and best practices of security information to protect personal data in the microfinance sector from a legal, organizational and technical approach. This model would also align the control to each phase of the personal data life cycle in order to guarantee a consistent protection. The validation was conducted in a Peruvian savings and credit cooperative, where information was collected, the level of protection of their personal data was evaluated and controls were proposed to raise their indicator based on the reference model, whose implementation increased the level of personal data protection by 98%.",10.1109/INTERCON.2019.8853835,C. Alejo; A. Navarro-Ruiz; D. Mauricio
Research on Full Link Monitoring and Fault Link Automatic Repair Based on Data Center,2023,2,Topic_2_data_privacy_security,0.1604105122438546,"In this paper, the data center is taken as the object of study, the whole link monitoring mechanism with the data center as the core is studied, the whole system structure of the data link monitoring is constructed, the link monitoring flow is explained, and the link automatic repair technology is used, the classification of link failure is realized, and corresponding maintenance methods are adopted for different faults. This paper takes the whole-link Monitoring System of data center station as the research object, breaks through the single monitoring means, establishes the whole-link monitoring capability, and promotes the formation of the whole-link monitoring and the automatic repair method of applaud link covering the whole life cycle of data.",10.1109/CSAT61646.2023.00138,J. Sun; C. Hu; R. Lu; S. Qi; Z. Yao; X. Ding; C. Liu; H. Hou
Application of Analytical Computer Science and Technology in Data Management,2023,-1,Outliers,0.28835062595901806,"The application of analytical computer science and technology in data management is a part of analytical computer science and technology. It mainly involves data management, data mining, information retrieval, text mining, etc. This course will help you understand how to use computers to solve problems related to data analysis. You will learn about the different techniques you can use when analyzing large amounts of data, such as clustering, regression analysis, classification, etc. The main purpose of this course is to let students understand various tools available in the market for analyzing large amounts of data from various sources (such as web logs or databases).",10.1109/INCET57972.2023.10170544,M. Wang
Research and Application of Material Data for Digital Manufacturing,2023,3,Topic_3_industry_manufacturing_chain,0.4968710477315184,"The organization mode of “headquarters and multi-factories” has been widely developed in manufacturing enterprises. Although the multi-production base model meets the needs of rapid business expansion, it puts forward higher requirements for enterprise management. The differences in cost accounting methods, planning methods, and operation modes of the factories under the headquarters lead to different production management organizations, purchasing organizations, and sales organizations. Because the basic materials are managed by the headquarters, each factory can only select the required materials from the headquarters material warehouse. Therefore, there is a phenomenon that the same item corresponds to multiple business views. In this paper, the full attribute management mode of material data is taken as the research object, and a full attribute management solution with material code and factory code as the unique identification is proposed. It provides the theoretical basis and practical guidance for the application and promotion of material data life cycle management in multi-factory mode.",10.1109/ICIPCA59209.2023.10257994,G. Wang; G. Zhang; J. Wang; J. Chen
Research and Application of Data Security Control System of Electric Power ERP System,2023,2,Topic_2_data_privacy_security,0.4318856259684164,"At present, the electric power industry is accelerating the exploration of electric power data authorization operations, and the support of electric power data security control system is particularly important in this process. However, most of the head enterprises are using the ERP system, and there are generally insufficient technical means of data security control, lack of systematic data security protection capabilities, information system data security operations and data security risks are increasing. Therefore, this paper takes the ERP data of electric power as an example to study the construction of electric power data security control system to support data circulation, based on data asset sorting and data flow monitoring, and the security control of circulating data in a hierarchical and classified manner. Combined with access control, data encryption, data desensitization and other technologies, it manages, protects, desensitizes, audits and analyzes different user roles and data access behaviors. Based on the concept of “visible, manageable and controllable”, it realizes the security protection of the whole life cycle of data.",10.1109/PEPSC58749.2023.10395677,Q. Gao; N. Jia; H. Meng; F. Yi; H. Li; H. Li; Y. Li
Research on National Cultural Security Assessment Method in Foreign Language Teaching Based on Improved DSMM Model,2023,2,Topic_2_data_privacy_security,0.44351918802320717,"In order to further ensure the security of national cultural data in foreign language teaching and education, a national cultural data security assessment method based on DSMM model and improved analytic hierarchy process is proposed. Among them, the DSMM model is used to construct the evaluation dimension of national cultural data, and then the weight of the index is determined by the improved analytic hierarchy process, and finally the random forest algorithm is adopted for safety assessment testing. The results show that compared with other weight allocation methods, the analytic hierarchy process based on entropy weight is more in line with the current mainstream trend in the weight allocation of national cultural data security indexes. At the same time, in the prediction results of each security assessment algorithm, the weight distribution of the designed security assessment method is very close to the manual allocation result, indicating that the weight allocation meets the actual requirements. Therefore, the constructed national cultural data security assessment method based on DSMN is feasible.",10.1109/ACAIT60137.2023.10528399,A. Xu
Efficient Data Query for Large-scale Manufacturing Product Lifecycle based on Blockchain,2023,3,Topic_3_industry_manufacturing_chain,0.39348081901974613,"The data query for large-scale manufacturing product lifecycle provides the data basis for finding out the processes and causes of the quality problems. At present, lifecycle data query methods are inefficient due to the complex search process. Therefore, this paper designs an efficient data query method for large-scale manufacturing product lifecycle. The new method uses blockchain technology, improves the original transaction structure, and sets K-V(Key-Value) database tables. Firstly, add pre-index and post-index to the transaction data structure. Secondly, store the correspondence between the pre/post-index and the hash value of transactions through K-V database tables. Thirdly, a new efficient lifecycle data query algorithm is designed using the pre&post-index. Finally, a comparative experiment is carried out to validate the efficiency of the efficient lifecycle data query method. The results show that the new method is more efficient than current methods.",10.1109/IAI59504.2023.10327513,Y. Hu; D. Jia; M. Huang; Z. Gao; S. Xu; G. He; S. Liu
Construct and Analysis of Metal Cutting Tool Remanufacturing Service Model,2023,3,Topic_3_industry_manufacturing_chain,0.8242639055073878,"Manufacturing is the pillar industry of the national economy, and remanufacturing is an important direction for the manufacturing industry to move towards greening, but the current research lacks a macro analysis of the tool service model. Given this problem, this paper first analyzes the current situation of metal cutting tool remanufacturing service mode. Then, the cause and effect analysis of the remanufacturing service model was carried out through Vensim PLE software. Finally, based on the above analysis, the remanufacturing service model is qualitatively analyzed and improvement strategies are proposed, which can promote the sustainable development of the tool remanufacturing market and increase the corresponding service revenue.",10.1109/ICCNEA60107.2023.00071,Z. Zhu; Y. Cao; L. Fu; C. Dong
CQUPT-FL: Cross-Domain Sharing and Security Awareness and Early Warning Platform of Health Science Big Data,2023,2,Topic_2_data_privacy_security,0.881961047058347,"Federated learning (FL) is a rapidly growing research area in machine learning, but it is problematic. It has been questioned whether or not existing FL libraries are practical in the area of medical privacy. To address these issues, we developed the CQUPT-FL system. The system focuses on resolving the conflict between data integrity and medical data privacy protection in cross-domain and cross-institution collaborative analysis. CQUPT-FL supports distributed computing and stand-alone simulation computing methods. To deal with the problems of heterogeneity, data domain diversity, and effective data scarcity, we adopted key technologies such as multi-party secure computing and holistic information representation and studied user identification, privacy protection, and heterogeneous user alignment to achieve sustainable Cross-domain and cross-platform data fusion of letters. The goal of introducing the CQUPT-FL system is to improve the level of data privacy protection and enhance the data privacy protection mechanism, solve the machine learning dilemma in the field of medical privacy, and provide a reliable solution for cross-domain collaborative analysis.",10.1109/MedAI59581.2023.00041,Y. Xiao; Q. Zhang; W. Zhao; X. Li; J. Peng; H. Mo; H. Zhu; F. Tang
Technological Disruption in Manufacturing - Redefining Buyer-Supplier Relationships and Business Models in the Industry 4.0 Era,2024,3,Topic_3_industry_manufacturing_chain,0.6913660895814547,"The impact of Industry 4.0-related technology shifts on buyer-supplier dynamics (BSD) is the subject of this research. As evidence, it draws on 17 expert interviews with manufacturing company managers in India. In order to infer meaning from the empirical data and discover overarching themes, patterns, and categories, a qualitative content analysis is carried out. The paper concludes that moving different value creation activities to platforms, digitised and automated procedures will form the basis of most future transactions in manufacturing supply chains. A more passionate character emerges in BSD. A company's supplier base may be streamlined by identifying and working with its most valuable strategic suppliers. Additional research may build on these findings by comparing and contrasting BSD across other sectors or stages of the value chain, and then drawing comparisons and highlighting commonalities. Management theory and business practise may both benefit from the findings presented in this article. As for BSD, they aid businesses in realising Industry 4.0's full promise by establishing and maintaining competitive advantages over the long run. Among the first to do so, this research uses Industry 4.0 to conduct an empirical investigation of BSD.",10.1109/ICTMOD63116.2024.10878171,D. Kumar; R. K. Singh; R. Mishra
Mathematical Models Applications in Digital Supply Chain Network,2024,3,Topic_3_industry_manufacturing_chain,0.6860775729152907,"Nowadays supply chains are distinct than those of a few decades ago, and they're now evolving in a highly competitive environment. More clearly, supply chains can no longer be placed, only, in such a way that they can buy, make, move, or sell the right items in the adequate quantities in the right places rapidly. As a result, supply chains must become intelligent to successfully cope with the growing challenges. Many functional supply chain applications based on mathematical quantitave models have been used, however there have been few research on mathematical programming models applicability in the digital supply chain processes. For this reason, this paper aims to investigate the possible benefits of mathematical programming solutions in digital supply chains since we are aware of their potential benefits in the traditional one. We categorized the quantitative mathematical models used in digital supply chain management based on reviewing research articles published in this topic. Furthermore, we provided an integrated model illustrating the use of mathematical programming methods in the digital supply chain. The conceptualized framework aids in showing an overview of usable mathematical models in the digital supply chain. Because it outlines the important advantage and benefit of applying mathematical models integrated with novel technologies in the value chain.",10.1109/ICTMOD63116.2024.10878207,K. Zekhnini; A. C. Benabdellah; A. Lahmar
"Comprehensive Survey on Data science, Lifecycle, Tools and its Research Issues",2022,1,Topic_1_data_big_big data,0.5261387125136209,"The amount of data produced each minute has increased dramatically since the internet's inception. There must be a method in place to manage this information. Thousands of terabytes are produced each year, and they must be safely stored and conveniently accessible; this necessitates a significant amount of research and development. This new worry has spawned a subject known as ""data science"" and individuals known as ""data scientists."" Data scientists are working hard nowadays to come up with new and novel ways to handle and store massive volumes of data using cutting-edge methods. Another key challenge nowadays is the optimal use of available energy resources; data science may greatly assist in predicting resource demand and eliminating waste. This paper discusses data science, its life cycle, and research difficulties in the subject. The main focus is on data science tools and its lifecycle.",10.1109/COM-IT-CON54601.2022.9850751,S. Jain; Kushagra
Digital Transformation in Power Transformer O&M: Critical Technologies for Multi-Source Heterogeneous Data Lifecycle Processing,2025,3,Topic_3_industry_manufacturing_chain,0.23282348305564682,"Multi-source heterogeneous data processing for power transformers serves as a critical driver for the digital transformation of operation and maintenance (O&M). This paper systematically elaborates key technologies for full lifecycle data processing. Firstly, it defines the characteristics of multi-source heterogeneous data in power transformers. Subsequently, preprocessing methods are proposed, including text and audio data cleansing, as well as structured data normalization. Furthermore, an analytical framework is established that integrates structured data analysis and text mining techniques. The paper discusses data fusion technologies and demonstrates the effectiveness. Finally, future trends are prospected, highlighting the application of big data processing and large language models as pivotal research directions for advancing intelligent lifecycle management.",10.1109/CEEPE64987.2025.11034178,A. Wen; B. Wen; J. Zhu; K. Song; X. Wang; Y. Tan
Health Status Analysis Method for Power Distribution Network Operators Based on Intelligent Wearable Device data and Logistic Regression Algorithm,2024,0,Topic_0_prediction_degradation_rul,1.0,"Power industry has already put forward the requirement of minimal or even no power outages for the maintenance and repair of the power system. The improvement of relevant requirements puts forward new requirements for the personal care of on-site maintenance operators in the power industry. As power operations are usually carried out in complex and dangerous environments, such as high-altitude work, proximity to high-voltage equipment, etc., exploring more scientific and stable ways to ensure the safety of operators during maintenance operations has important practical significance and research value. Current reports on power system security issues, are mostly limited to the safety of the system itself or the operation safety of power system enterprises, and rarely involve the safety care of operators. With the emergence of intelligent wearable devices, real-time monitoring of operators’ mental, physical, and other states has become possible. In this paper, a health status fusion evaluation algorithm based on swarm intelligence optimization and logistic regression based on various physiological parameters extracted by intelligent wearable devices has been proposed, which effectively classifies and identifies the health status of workers reflected by multivariate nonlinear physiological data.",10.1109/ICIBA62489.2024.10868445,Y. Fu; Y. Liu; Y. Cheng; L. Xia; J. Liu
"Enabling Dynamic Data Governance in Science: Design, Implementation, and Future Directions of the Modern Data Governance Framework",2024,1,Topic_1_data_big_big data,0.3991041663125921,"As scientific data volumes exponentially grow, dynamic, flexible and open approaches to data governance are needed. In this paper, we describe our efforts to build an open, scientific Modern Data Governance Framework (mDGF) that streamlines and makes actionable data governance requirements for projects and data providers. We present the goals and design of the mDGF. We also share our envisioned usage for the mDGF and planned future work.",10.1109/IGARSS53475.2024.10640434,K. Bugbee; R. Ramachandran; A. Kaulfus; J. Le Roux; G. Peng; D. Smith; I. Gurung; A. Acharya; J. Christman
Construction of the full life cycle data management model for complex aerospace products based on digital twins,2024,-1,Outliers,0.15398438641337892,"Complex aerospace products such as satellites, missiles, and rockets have features like high complexity, strong dynamics, and frequent design changes during the design and manufacturing processes. Thus, data management throughout full life cycle of complex aerospace products is a challenge. However, the emergence of the concept of digital twins offers a new management model for full life cycle data management of complex aerospace products. To achieve data management at all stage from design to manufacturing of complex aerospace products, this paper proposes full life cycle data management method for complex aerospace products based on digital twins. Firstly, the evolution process of full life cycle data of complex aerospace products is analyzed from the perspective of the concept of digital twins. Based on this, a full life cycle data management model for complex aerospace products based on digital twins is constructed, and key technologies such as the workflow-based business process management system for the design-process-manufacturing of complex aerospace products and the construction of the digital twin model for the production quality management of complex aerospace products are broken through. On this basic, full life cycle data management system for complex aerospace products based on digital twins is developed, verifying the effectiveness of this method. Finally, the work of this paper is summarized, and the future research works is discussed.",10.1109/AIIM64537.2024.10934453,W. Yu; J. Wu; Y. Yu; K. Huang; Y. Shen; F. Xu; P. Li; S. Peng
Unveiling Synergies: Integrating IEEE 1451 and ISO 15926 for Advanced Interoperability in Smart Manufacturing and Transportation,2024,3,Topic_3_industry_manufacturing_chain,0.9198107817480256,"In smart manufacturing and transportation systems, seamless interoperability is crucial for efficient data exchange and informed decision-making. This paper explores integrating IEEE 1451 and ISO 15926 data models to enhance interoperability in these sectors. We align sensor and actuator models to bolster interoperability and assess leveraging these standards within the Arrowhead flexible Production Value Network (fPVN). We propose innovative solutions, including an ontology mapping tool for seamless data integration, a wrapper service for real-time data translation, and a lightweight semantic reasoner at the fog computing layer. These solutions bridge the gap between theoretical constructs and practical applications, overcoming interoperability barriers, enhancing decision-making, and refining operational effectiveness. Our study shows significant potential for improving system cohesiveness, efficiency, and robustness tailored to Industry 5.0. This research provides a foundation for future implementations in smart manufacturing and transportation, paving the way for advanced, responsive, and sustainable industrial ecosystems.",10.1109/ICECS61496.2024.10848585,V. Tavakkoli; K. Mohsenzadegan; W. V. Kambale; K. Kyamakya
The Application of AI for Electronic LCA Data and Assessment Toward the Circular Economy,2024,3,Topic_3_industry_manufacturing_chain,0.36432420754322387,"LCA is viewed as one of the important techniques to assess the environmental impacts of products, especially during the whole of the life cycle towards a circular economy approach. In the electronics sector, where products undergo rapid technological advancements and complex supply chains, LCA plays a crucial role in promoting sustainability and guiding decision-making processes. On the one hand, the environmental sustainability of electronics like ICT, Consumer Electronics, micro-electronics and Network Equipment is affected by the huge missing data because of the complexity, variety, large number of components, and otherwise different suppliers around the world. A more accurate, reliable, and standard LCA data management is necessary to decrease risks in the Supply Chain, increase transparency and provide sustainable and circular Business Models for Electronic Products and ICT Services. Meanwhile, artificial intelligence (AI), and particularly the advances in machine learning (ML) and deep learning (DL) have led to disruptive innovations in data modeling and other fields. To address this issue in the electronic industry, the integration of big data and artificial intelligence (AI) can transform precision assessment from data inventory to automated LCA modeling, and innovative methodologies for a cost-efficient data configuration. We provide a comprehensive overview of advances in the application of big data and AI technologies for LCA of electronics. We discuss key challenges in missing data and utilization for electronics and micro-electronics, offering strategic solutions. This novel AI-data approach helps organizations easily provide the missing data by integrating complex and disparate data sources across multiple distributed sources to give insights. Meanwhile, a dynamic big data discovery approach delivers more accurate and precious data for intelligent decisions. The intention is to provide the industry, LCA experts, and decision-makers with a more standardized, comprehensive, and reliable data ecosystem. (Abstract)",10.23919/EGG62010.2024.10631236,Z. Mehdipour
A DF-LCA Modeling and Evaluation Case Study on an Agricultural Pests and Diseases Knowledge Graph Based Q&A System,2024,3,Topic_3_industry_manufacturing_chain,0.24236874713810827,"Digitilization footprint (DF) quantifies the cost, time, and data volume of information storage, transmission, and processing in a virtual environment, and is a key indicator for evaluating virtual sustainability. Based on the modeling process and indicator system of DF-LCA, this paper models the automated construction of a knowledge graph question-answering system for agricultural pests and diseases, defines a data processing unit (DPU), and analyzes data flow to evaluate resource consumption during system construction. By monitoring DPU through Grafana, this paper obtains key indicators of the virtual environment, providing a basis for evaluating the virtual sustainability of different construction processes. Compared with the evaluation of individual modules by previous researchers, this paper models the life cycle of the entire data for this process, which not only reveals the contribution of each module of the knowledge graph question-answering system in the virtual environment, but also provides a basis for the study of knowledge graph sustainability. At the same time, it provides a case reference for the application of DF -LCA in the agricultural field. Its modeling and monitoring methods also provide a reference for DF research in other systems.",10.1109/MetroAgriFor63043.2024.10948796,X. Wang; Q. Gao; Y. Tao; F. Marinello; Q. Huang
Research on digital transformation of spacecraft development,2024,-1,Outliers,0.26061064448615523,"With the increase of the complexity, cost and development cycle of satellite development, document design intensively under baseline management pattern with PLM system has been difficult to meet the needs of efficient and real-time collaborative design. There are some problems in this pattern such as discrete data sources, discontinuous data flow, and low efficiency of inter-specialty collaborative iteration, which restrict the overall efficiency and level of satellite development. In this paper, a digital development mode based on model penetration is proposed to comprehensively optimize and sort out the collaborative business process of the whole life cycle of spacecraft development, and adopt advanced information technology to form a new and complete R & D capability system. Based on the model, digital collaborative design and simulation verification are carried out to achieve rapid verification and iteration of scheme design, greatly reduce physical tests, and improve the efficiency of R & D design. Through the digital main line, the whole life cycle data will be gathered together and linked to form a complete global and unique truth source, evolutionary traceability chain and comprehensive data view, and promote the integration of the physical form and digital form of the satellite to achieve digital delivery.",10.1109/ICIBA62489.2024.10868496,J. Hao; Wangxinxiang; Zhaopeilin
Autonomous Data Quality Monitoring with AI Agents: Integrating ML with Cloud Warehouses and Data Lakes,2025,1,Topic_1_data_big_big data,0.8693989761781287,"Since storing data in cloud warehouses and lakes has increased, it is now important to rely on automation for keeping data accurate and trustworthy. It outlines an autonomous way of monitoring data quality by using AI agents with botnet techniques and cloud architecture. Key features of the system are that it constantly scans the data, finds unusual activities, detects any changes in the structure, and responds to any data updates without someone needing to act. Missed values, duplication, inconsistency, and noticing outliers on both structured and semi-structured data is done by using both supervised and unsupervised ML models. AI agents interact by way of a network pipeline and use reinforcement learning to pick the best solutions. easily set up Cloud Genomics in any leading cloud environment, and it works well with data warehouses as well as data lakes. Experimental evidence shows that AI-driven analysis is better at detecting errors, offering faster results, and being more adaptable than hand-made and rule-based ones. In this research, data pipelines can become more flexible and easier to manage in today's business architecture.",10.1109/ICCTDC64446.2025.11157955,S. Chippagiri; K. Alang; A. Gumber; S. G. Thomas
Research on performance degradation assessment of rolling bearings,2024,0,Topic_0_prediction_degradation_rul,0.9300010156496636,"The performance degradation assessment (PDA) in rolling bearings through vibration signals is crucial for the successful implementation of preventive maintenance strategies. However, traditional indicators in existing methodologies often fail to effectively characterize the degradation trend, and feature extraction is heavily reliant on specific experience and operational parameters. Addressing these limitations, this paper introduces a novel PDA method. Initially, the selected features under normal operating conditions are selected based on the correlation coefficient method. Subsequently, the dimensionality of the feature space is then reduced using the Kernel Principal Component Analysis (KPCA) algorithm. The refined comprehensive feature set is then input into the degradation assessment model, employing the novel method Support Vector Data Description (SVDD) to integrate these features into a coherent degradation indicator (DI). To fine-tune the SVDD parameters, including the penalty parameter and kernel parameter, the Hippopotamus Optimization (HO) algorithm is incorporated into the optimization process. Experimental results validate the method in degradation detection and in accurately reflecting the overall performance degradation of the bearings, showcasing distinct advantages over other existing methods.",10.1049/icp.2024.3574,C. Hu; Y. Li; J. Mao; J. Cao
Energy Low-carbon Operational Asset Lifecycle Management Based on Improved Random Forest and RBF Neural Network,2024,3,Topic_3_industry_manufacturing_chain,0.22700215757622255,"In order to improve the ability of energy data information management, this research collects energy data information management information in all aspects, applies embedded technology, sets ARM embedded series chips in the host computer, and realizes the data information collection of electric energy meters based on STM32F103VET6 single-chip microcomputer chip. The robot collects the external environmental data information that affects the low-carbon energy operation equipment, adopts the improved random forest algorithm model to improve the data information management ability of the low-carbon energy operation assets, and constructs the RBF neural network architecture model to diagnose the input data information. The ability to manage and control the entire life cycle of energy low-carbon operating equipment has been improved. Through experiments, the accuracy rate of the method in this study is as high as 92%, which has outstanding technical significance.",10.1109/ACPEE60788.2024.10532509,X. Liu
Key Technical Points Analysis of the Civil Aviation Data Governance System,2024,1,Topic_1_data_big_big data,0.3894325796780776,"As a new type of production factor, data is a crucial foundation for the digital transformation of the civil aviation industry. In the past two years, the Civil Aviation Administration of China (CAAC) has successively issued the “7+1” series of Smart Civil Aviation Data Governance Standards, providing guidance for industry data governance and offering solutions for “how to manage and use data” in civil aviation. This article combines national development trends and the current state of industry data to analyze the civil aviation data governance system and its construction key points through the interpretation of these series of standards.",10.1109/ICICC63565.2024.10780590,T. Yuan; Y. Liu; X. Liu
Research on Carbon Emission Accounting Method of 126 kV GIS Power Equipment Based on LCA,2024,3,Topic_3_industry_manufacturing_chain,0.3849909517192169,"This study focuses on the field of power switchgear, aiming to solve the problems of complex carbon footprint accounting of products and insufficient research on related carbon emission accounting due to the long span of industrial chain. In this study, 126 kV gas-insulated switchgear (GIS) is selected as the research object, and the Life Cycle Assessment, LCA) method is adopted to establish the boundary of the whole life cycle system from raw material, production, transportation, use to final disposal, that is, the assessment range of “cradle to grave”. The specific accounting adopts the emission factor method (IPCC method) to quantitatively analyze the carbon emissions of GIS equipment in different life cycle stages. Based on the analysis results, feasible carbon reduction strategies are proposed. The results show that the carbon emissions in the raw material acquisition stage and the product use stage are the most in the whole life cycle of GIS products. Therefore, in the process of GIS manufacturing, carbon emissions should be reduced from the source by purchasing environmentally friendly materials, recycling and other measures. This study not only provides a reference for the emission reduction scheme of GIS in the whole life cycle, but also provides a reference methodology for carbon emission accounting and analysis of other types of power equipment.",10.1109/NPSIF64134.2024.10883278,J. Zhong; Y. Li; X. Fang; X. Zhao; F. Gao; Z. Li; W. Zhang
Best Practices for Managing Passive Acoustics Data at Ocean Networks Canada,2024,1,Topic_1_data_big_big data,0.4362594524843928,"Presented here are the data management best practices applied to passive acoustics data collected by hydrophones from Ocean Networks Canada (ONC) sites. Being the largest single source of data from the observatories, the data collected by hydrophones currently occupy over 700 TB in the archive, spanning about 18 years of observation and data collection. To manage this sheer volume of data, scientific data management best practices are adopted by ONC. Such practices incorporate the entire eight components of the data life cycle: plan, collect, assure, describe, preserve, discover, integrate, and analyze. ONC's data infrastructure, Oceans 3.0, facilitates such data management integration. In addition, ONC is committed to adhering to research data community standards and best practices in data management including FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. These best practices are applied to acoustic data through workflows that record all events of a hydrophone life cycle.",10.1109/OCEANS55160.2024.10754545,O. Aghaei; A. Lam; M. Wolf; L. Muzi; G. Baillie; B. Biffard; J. Dorocicz; D. Snauffer; J. Bedard; H. F. Netof
Industry 5.0: From Manufacturing Industry to Sustainable Society,2022,3,Topic_3_industry_manufacturing_chain,1.0,"Industry 4.0 brought a new revolution in industries by making them fully automated via innovative technologies, without considering human-power. Industry 4.0 aims to establish “smart manufacturing industry” by emphasizing on Information Technology (IT), Internet of Things (IOT), Cyber Physical System (CPS), Industrial Internet of Things (IIOT), Artificial Intelligence (AI), Big Data, and Robotics. This highly automated industry neglected human's intellectual and cognitive skills, causing an increase in unemployment rate and devastation of ecosystem. In this paper, we proposed a framework of emerging technologies of Industry 5.0. Here, we examined how Industry 5.0 will further extend the development of Industry 4.0 and how humans can contribute to its manufacturing process. In addition, prestigious and significant skills for workforce in manufacturing industry are also explored. We also investigated how the Covid-19 epidemic was associated to Industry 5.0 and the idea of sustainable development goals (SDGs). Finally, we highlighted some of the challenges facing the industrial sector as research direction of Industry 5.0.",10.1109/IEEM55944.2022.9989705,M. Iqbal; C. K. M. Lee; J. Z. Ren
An IoT Course Program to Foster the Adoption of IoT Driven Food and Agriculture in Sub-Saharan Africa (SSA),2022,-1,Outliers,0.41789160588956054,"The Internet of Things (IoT) technologies, together with other technological advances such as big data analytics, Artificial Intelligence (AI), automation, and unmanned electrical vehicles (e.g., self-driving vehicles and drones), are currently being used to transform the food and agriculture industry to increase yields significantly with minimal damage to the environment. These recent advances in Information and Communication Technologies and advances in biotechnology are driving the transition into the fourth agricultural revolution or agriculture 4.0. Some research studies on the adoption of IoT in the agriculture sector in Sub-Saharan Africa concluded that some of the reasons the rate of adoption of IoT in Africa is still very low are gaps in knowledge, skills, finance, and infrastructure. The knowledge and skills gaps are partly a result of an educational curriculum that is not up to date with current advances in the food and agricultural industry. To foster IoT adoption in the food and agriculture sectors and other sectors or industries in Sub-saharan Africa, we propose an IoT course program consisting of lectures and laboratory modules. The course is accompanied by textbooks, video lectures that are freely available on Edx, and remote online laboratories with the hardware resources hosted in some European countries but accessible through the Internet. These courses can also be adapted by vocational training schools, Higher Institutes, and universities in Sub-Saharan Africa or could be used by professionals and enthusiasts to improve their skills.",10.1109/ICECET55527.2022.9872825,G. S. Kuaban; M. Nowak; P. Czekalski; K. Tokarz; J. K. Tangka; K. Siggursson; A. Nikitenko; K. Berkolds; R. Sell
An Architecture and Stochastic Method for Database Container Placement in the Edge-Fog-Cloud Continuum,2019,1,Topic_1_data_big_big data,0.7237590941771287,"Databases as software components may be used to serve a variety of smart applications. Currently, the Internet of Things (IoT), Artificial Intelligence (AI) and Cloud technologies are used in the course of projects such as the Horizon 2020 EU-Korea DECENTER project in order to implement four smart applications in the domains of Smart Homes, Smart Cities, Smart Construction and Robot Logistics. In these smart applications the Big Data pipeline starts from various sensor and video streams to which AI and feature extraction methods are applied. The resulting information is stored in database containers, which have to be placed on Edge, Fog or Cloud infrastructures. The placement decision depends on complex application requirements, including Quality of Service (QoS) requirements. Information that must be considered when making placement decisions includes the expected workload, the list of candidate infrastructures, geolocation, connectivity and similar. Software engineers currently perform such decisions manually, which usually leads to QoS threshold violations. This paper aims to automate the process of making such decisions. Therefore, the goals of this paper are to: (1) develop a decision making method for database container placement; (2) formally verify each placement decision and provide probability assurances to the software engineer for high QoS; and (3) design and implement a new architecture that automates the whole process. A new optimisation method is introduced, which is based on the theory and practice of stochastic Markov Decision Processes (MDP). It uses as input monitoring data from the container runtime, the expected workload and user-related metrics in order to automatically construct a probabilistic finite automaton. The generated automaton is used for both automated decision making and placement success verification. The method is implemented in Java. It also uses the PRISM model-checking tool. Kubernetes is used in order to automate the whole process when orchestrating database containers across Edge, Fog and Cloud infrastructures. Experiments are performed for NoSQL Cassandra database containers for three representative workloads of 50000 (workload 1), 200000 (workload 2) and 500000 (workload 3) CRUD database operations. Five computing infrastructures serve as candidates for database container placement. The new MDP-based method is compared with the widely used Analytic Hierarchy Process (AHP) method. The obtained results are used to analyse container placement decisions. When using the new MDP based method there were no QoS violations in any of the placement cases, while when using the AHP based method the placement results in some QoS threshold violations in all workload cases. Due to its properties, the new MDP method is particularly suitable for implementation. The paper also describes a multi-tier distributed computing system that uses multi-level (infrastructure, container, application) monitoring metrics and Kubernetes in order to orchestrate database containers across Edge, Fog and Cloud nodes. This architecture demonstrates fully automated decision making and high QoS container operation.",10.1109/IPDPS.2019.00050,P. Kochovski; R. Sakellariou; M. Bajec; P. Drobintsev; V. Stankovski
An Overview of Current Data Lake Architecture Models,2022,1,Topic_1_data_big_big data,0.4688164246992177,"As the Data Lakes have gained a significant presence in the data world in the previous decade, several main approaches to building Data Lake architectures have been proposed. From the initial architecture towards the novel ones, omnipresent layers have been established, while at the same time new architecture layers are evolving. The evolution of the Data Lake is mirrored in the architectures, giving each layer a distinctive role in data processing and consumption. Moreover, evolving architectures tend to incorporate established approaches, such as Data Vaults, into their layers for more refined usages. In this article, several well-known architecture models will be presented and compared with the goal of identifying their advantages. Next to the architecture models, the topic of Data Governance in the terms of the Data Lake will be covered in order to expound its impact on the Data Lake modeling.",10.23919/MIPRO55190.2022.9803717,T. Hlupić; D. Oreščanin; D. Ružak; M. Baranović
Research on E-Waste Risk Assessment through the Construction of a Cloud Platform in the Context of a Big Data Environment,2024,1,Topic_1_data_big_big data,0.4782392684719863,"In the realm of big data, the volume of research data in universities is expanding rapidly. However, the lack of data collaboration results in data isolation, hindering the realization of their potential value. This study aims to enhance data interoperability and promote data reuse by developing a comprehensive metadata scheme, which facilitates standardized descriptions of scientific data metadata throughout the entire process, from data generation to citation. Additionally, by delineating the data governance process encompassing data collection, processing and handling, archival and management, as well as data utilization and sharing, a data management cloud platform is established based on data reuse patterns. This platform incorporates features for data retrieval, storage and management, and data monitoring, ensuring the seamless continuity of research management efforts.",10.1109/EEE59956.2024.10709755,S. Yang; R. Zhang; B. Zhu
Semi-supervised Graph Convolutional Neural Network Based Classification for Auto Parts Inventory Management,2022,0,Topic_0_prediction_degradation_rul,0.49509560183234624,"Auto parts inventory management is an important link of the automobiles multi-value chain, which has a certain impact on the upstream procurement, downstream production and other links. However, there are many kinds of auto parts, which will lead to improper allocation of funds and uneven allocation of resource when managed according to a unified standard. Therefore, they need to be managed by categories. At present, the industry mainly adopts the ABC methods of stores control for classification. But this method has some shortcomings, such as single classification standard and no consideration of the correlation between materials, etc. In order to solve these problems, we introduce a semi-supervised graph convolutional neural network based classification method, which models the graph of automobile multi-value chain data according to the strong correlation between auto parts, and reclassifies them. Through the experiment in Geely's multi-value chain data set, we achieve an 91.7% accuracy rate and obtain a better classification results, which provided new ideas for inventory management classification.",10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00166,F. Weng; B. Guo; X. Suo; X. Wu; Y. Hu; Q. Zheng
A Framework for Data Quality and Protection Management in Service-Based Data Pipelines,2025,1,Topic_1_data_big_big data,1.0,"We present a framework that implements a service selection process tailored for the definition of service-based data pipelines. The framework addresses the critical challenge of balancing data quality and data protection in distributed, service-based data pipelines, an issue that existing solutions overlook by treating these dimensions independently. By modeling the pipeline as a Directed Acyclic Graph (DAG), the framework extends pipelines with functional and data protection requirements. An extensive experimental evaluation measures the performance of our framework by analyzing variations in data quality across diverse datasets and configurations.",10.1109/ICWS67624.2025.00040,A. Polimeno; M. Luzzara; M. Anisetti; C. A. Ardagna; C. Ghedira-Guegan
Data Management Challenges for Deep Learning,2019,1,Topic_1_data_big_big data,0.5459530570611889,"Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.",10.1109/SEAA.2019.00030,A. Munappy; J. Bosch; H. H. Olsson; A. Arpteg; B. Brinne
Glassdoor Job Description Analytics – Analyzing Data Science Professional Roles and Skills,2021,-1,Outliers,0.15107165430688227,"With increasing data volume and adoption of technologies including machine learning and artificial intelligence across all industries, the demand for skilled Data Science professionals is continuing to increase globally. For educational institutions to teach the most up-to-date and industry-relevant skills and for businesses to hire employees with the right set of skills, it is important for them to stay tuned to the fast-changing dynamics of job landscape. In this research study, we present an NLP approach to the analysis of job listings from Glassdoor. Our solution mines insights on trending technical and soft skills in the Data Science job categories. Based on the insights, we provide recommendations to design overall data science curriculum learning outcomes (LOs). We also provide recommendations to the course designers on specific technical skills required for the topics of courses under the data science curriculum.",10.1109/EDUCON46332.2021.9453931,S. Gottipati; K. J. Shim; S. Sahoo
Scalable Data Analytics from Predevelopment to Large Scale Manufacturing,2019,3,Topic_3_industry_manufacturing_chain,0.3110731432780373,"Data analytics provides a toolset to extract insights from large amounts of data. In order to stay competitive, companies of the manufacturing domain utilize data analytics to be more efficient and to increase quality of the production and product. Current methodologies for the application of data analytics and data mining techniques focus on finding correlations within data from existing systems and historic data. Therefore, data analytics is typically applied to solve existing problems within existing manufacturing systems. Since present brownfield production lines often provide insufficient data, new hardware has to be retrofitted to acquire the required data. Hence, valuable time for problem solving is lost. This paper presents an approach to proactively implement data analytics during early predevelopment phases in order to allow scalability of the approach to large scale manufacturing systems. The approach is implemented and evaluated within the context of high voltage battery manufacturing for electric vehicles.",10.1109/APCoRISE46197.2019.9318833,H. Heimes; A. Kampker; U. Bührer; A. Steinberger; J. Eirich; S. Krotil
Digital Twin: A Comprehensive Survey of Security Threats,2022,2,Topic_2_data_privacy_security,0.23879489930345973,"Industry 4.0 is having an increasingly positive impact on the value chain by modernizing and optimizing the production and distribution processes. In this streamline, the digital twin (DT) is one of the most cutting-edge technologies of Industry 4.0, providing simulation capabilities to forecast, optimize and estimate states and configurations. In turn, these technological capabilities are encouraging industrial stakeholders to invest in the new paradigm, though an increased focus on the risks involved is really needed. More precisely, the deployment of a DT is based on the composition of technologies such as cyber-physical systems, the Industrial Internet of Things, edge computing, virtualization infrastructures, artificial intelligence and big data. However, the confluence of all these technologies and the implicit interaction with the physical counterpart of the DT in the real world generate multiple security threats that have not yet been sufficiently studied. In that context, this paper analyzes the current state of the DT paradigm and classifies the potential threats associated with it, taking into consideration its functionality layers and the operational requirements in order to achieve a more complete and useful classification. We also provide a preliminary set of security recommendations and approaches that can help to ensure the appropriate and trustworthy use of a DT.",10.1109/COMST.2022.3171465,C. Alcaraz; J. Lopez
Earthquakes: From Twitter Detection to EO Data Processing,2022,1,Topic_1_data_big_big data,0.6078834338992046,"The increase of social media use in recent years has shown potential also for the identification of specific trends in the data that could be used to locate earthquakes. In this work, we implemented a pipeline that uses Twitter data to identify locations of earthquakes and use the information to trigger EO data analysis. We tested the pipeline for almost a year over Japan, an area where earthquake events are frequent, as well as the use of social media in the population. Here, we show the results and discuss the potential development of such procedures. In the future, considering the rapid development and the increase of satellite constellations aimed at global coverage with short revisit times, algorithms of this kind could be used to prioritize satellite acquisitions for the detection of the areas most affected by earthquake damages.",10.1109/LGRS.2022.3149541,S. Andreadis; I. Gialampoukidis; A. Manconi; D. Cordeiro; V. Conde; M. Sagona; F. Brito; N. Pantelidis; T. Mavropoulos; N. Grosso; S. Vrochidis; I. Kompatsiaris
Mask–Mediator–Wrapper Architecture as a Data Mesh Driver,2024,1,Topic_1_data_big_big data,0.4669630683678024,"The data mesh is a novel data management concept that emphasizes the importance of a domain before technology. The concept is still in the early stages of development and many efforts to implement and use it are expected to have negative consequences for organizations due to a lack of technological guidelines and best practices. To mitigate the risk of negative outcomes this paper proposes the use of the mask–mediator–wrapper architecture as a driver for a data mesh implementation. The mask–mediator–wrapper architecture provides a set of prefabricated configurable components that provide basic functionalities that a data mesh requires. This paper shows how the two concepts are compatible in terms of functionality, data modeling, evolvability, and aligned capabilities. A mask–mediator–wrapper-driven data mesh facilitates low-risk adoption trials, rapid prototyping, standardization, and a guarantee of evolvability. We demonstrate a mask–mediator–wrapper-driven data mesh by using our open-source Janus system to experimentally drive an exemplified data mesh.",10.1109/TSE.2024.3367126,J. Dončević; K. Fertalj; M. Brcic; M. Kovač
Requirements for DataOps to foster Dynamic Capabilities in Organizations - A mixed methods approach,2022,-1,Outliers,0.2643765274006947,"Digital technologies and the associated availability of ever-greater volumes of data present companies with existential challenges. The possibilities of utilizing a wide variety of data force organizations to react quickly. One way to tackle issues of organizational data utilization is implementing DataOps practices, which assist organizations in setting up the data management structure agilely. The paper starts precisely here, discussing how DataOps can foster the building of dynamic capabilities suitable to help companies act agilely in this ever-changing digital environment. To do this, the paper reports on a study to derive conceptual requirements for organizations to adopt DataOps practices based on Leavitt's Diamond Model. We approach this task through a mixed-method study design combining a quantitative multi-vocal literature review (MVLR) of the existing literature corpus and complementing these findings through a semi-structured interview study with experts from the domain.",10.1109/CBI54897.2022.00025,I. Gür; F. Möller; M. Hupperz; D. Uzun; B. Otto
A Complete Piping Identification Solution for Piping and Instrumentation Diagrams,2023,-1,Outliers,0.29762832835566655,"This study proposes a pipeline identification method for identifying pipelines in Piping and Instrumentation Diagrams (P&IDs) in image format. Automating this process is an important issue for the process plant industry, as image P &IDs are currently identified manually by humans. As part of research on technology for automatic conversion of image-format piping and instrumentation diagram (P&ID) into digital P&ID, this study proposes a method combining deep learning and traditional image detection to identify various types of pipelines in image format P&ID. The proposed method includes image pipeline extraction, pipeline simulation data augmentation, pipeline identification and fusion. For multiple test P &IDs, using yolov8 based on split coordinate spatial attention and augmented with simulated data, the average precision and recall are 90.5 % and 88.5 %, respectively. Combined with LSD line detection, it takes an average of 100 seconds to recognize a complete image, and the overall pipeline recognition rate can reach 95.5 %.",10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00011,S. Liu; Z. Li; S. Zhao; L. Yang; F. Zhao; C. Ge
Enhancing Retrieval Augmented Generation Systems Using AI Models and Graph Databases,2025,1,Topic_1_data_big_big data,0.5651827724469813,"This paper outlines the process of generating a Neo4j graph database powered by Language Models (LLMs). The primary goal is to extract structured information from unstructured data, including user profiles, paper briefs, and Slack messages, and convert them into Cypher queries. The data is then ingested into Neo4j to build a graph database that captures relationships between users, paper, technologies, and messages. A pipeline was developed to automate the process, ensuring accurate entity and relationship extraction using predefined templates. This approach allows for efficient data representation and supports consultancy in managing large datasets by generating insightful visualizations and querying capabilities.",10.1109/ESCI63694.2025.10988015,S. Mukherjee; S. Shabnam; S. Hasan; D. Ajitha
What About the Data? A Mapping Study on Data Engineering for AI Systems,2024,1,Topic_1_data_big_big data,0.44750610357914056,"AI systems cannot exist without data. Now that AI models (data science and AI) have matured and are readily available to apply in practice, most organizations struggle with the data infrastructure to do so. There is a growing need for data engineers that know how to prepare data for AI systems or that can setup enterprise-wide data architectures for analytical projects. But until now, the data engineering part of AI engineering has not been getting much attention, in favor of discussing the modeling part. In this paper we aim to change this by perform a mapping study on data engineering for AI systems, i.e., AI data engineering. We found 25 relevant papers between January 2019 and June 2023, explaining AI data engineering activities. We identify which life cycle phases are covered, which technical solutions or architectures are proposed and which lessons learned are presented. We end by an overall discussion of the papers with implications for practitioners and researchers. This paper creates an overview of the body of knowledge on data engineering for AI. This overview is useful for practitioners to identify solutions and best practices as well as for researchers to identify gaps.CCS CONCEPTS•Software and its engineering;•Information systems→Data structures;•Computing methodologies → Artificial intelligence;",,P. Heck
Developing Scalable and Fault Tolerant Distributed Architecture for Big Unstructured Data Systems,2025,1,Topic_1_data_big_big data,1.0,"In the era of big data, efficiently processing and retrieving insights from unstructured data presents a critical challenge. This paper introduces a scalable leader-worker distributed data pipeline designed to handle large sets of unstructured data blobs. Leveraging data parallelism, the system distributes workload across worker nodes, ensuring high performance for embarrassingly parallel tasks with minimal inter-process communication. Fault tolerance is achieved through an optimized next-in-line leader re-election strategy, addressing cascading failures and mitigating the thundering herd effect. The use of Apache Zookeeper for service discovery ensures seamless coordination within the cluster while maintaining a loosely coupled architecture for enhanced scalability. Google Protocol Buffers are employed for secure data transmission, offering tamper resistance against eavesdropping and man-in-the-middle attacks. The system also integrates the TF-IDF algorithm for efficient information retrieval, demonstrating the ability to achieve data parallelism while preserving task uniformity across nodes. Performance evaluation highlights significant reductions in response times with increasing cluster size, validating the system's scalability, efficiency, and robustness. The proposed architecture is well-suited for real-world applications requiring secure, fault-tolerant, and scalable solutions for unstructured data analysis.",10.1109/ICMCSI64620.2025.10883441,S. G; K. K; P. V; D. S. R
Efficient Data Processing Pipelines for Mobility Data Using MongoDB,2024,1,Topic_1_data_big_big data,0.7556689234401772,"The increasing availability of mobility data presents new opportunities and challenges for urban studies and related fields. This paper introduces a data processing pipeline developed to handle large-scale Network Signaling Data (NSD) using MongoDB. Our method addresses key challenges in data preprocessing, such as timestamp compression and noise filtering, to enhance the accuracy and efficiency of mobility pattern analysis. We applied this pipeline to a NSD dataset from Orange Romania, focusing on Cluj County for October 2023. By leveraging MongoDB’s aggregation framework, our approach significantly reduced data size and improved computational efficiency, enabling the extraction of meaningful mobility insights. Our findings highlight the importance of advanced data processing techniques in deriving reliable Origin-Destination matrices and demonstrate the potential for further applications in urban planning and smart city initiatives.",10.23919/SoftCOM62040.2024.10721901,C. -F. Andor; V. Alexe; N. Petrovici
Data Logistics Service in eFlows4HPC,2024,1,Topic_1_data_big_big data,0.9467985912505202,"Modern scientific endeavors often require complex, data-intensive workflows leveraging distributed and heterogeneous computing and data resources. Such workflows often include multiple steps of classical simulations, but increasingly also ML and AI components. As a result, they use not only HPC, but also Cloud-like resources. Efficient and user-friendly execution and management of such workflows pose many challenges. In this paper, we share our experience in implementing three such workflows in the eFlows4HPC project. We focus, however, on the data management dimension of the workflows. How to ensure the timely availability of the required data, how to move data to and from compute resources, and how to make the workflows complete and portable. To this end, we implemented the Data Logistics Service, integrated it with the workflow execution engine, and defined multiple data movement pipelines to cater for specific scientific needs. We will share our experience from implementation and operation of the service. This will include building a solution for continuous deployment and access management in a federated environment. On a more abstract level, we also explore how the presented approach fits into the vision of the FAIR paradigm.",10.1109/MIPRO60963.2024.10569664,J. Rybicki; C. Böttcher
Identification of approaches for remanufacturing 4.0,2016,3,Topic_3_industry_manufacturing_chain,0.66140314526532,"The complexity within products, services and industrial production is constantly increasing. Due to the trend of mass production towards lot size ‘one’, topics like rationalization, optimization or Lean Management were followed by almost all companies during the last decades. Especially the industrial segment of remanufacturing is affected by this trend, which issues in small lot sizes and low volumes. Different strategies and approaches of Industry 4.0 were developed during the last years for the manufacturing but not for the remanufacturing industry. For this reason, the aim of this paper is to identify challenges of the remanufacturing industry and furthermore approaches of Industry 4.0 to face these challenges.",10.1109/E-TEMS.2016.7912603,S. Butzer; D. Kemp; R. Steinhilper; S. Schötz
A Method for Analyzing Practicing Managers' Perception on the Disruptive Nature of Digitalization in Machine-Building Industry,2017,3,Topic_3_industry_manufacturing_chain,0.7932222549469661,"The purpose of the paper is to demonstrate a technology foresight method where Visual Analogue Scale is used with an experimental survey design to investigate views and perceptions of possible future disruption caused by digitalization in an established machine-building industry. We demonstrate the usability of the method in detecting differences in future worldviews of practicing managers with results highlighting polarized responses with significant clustering among groups. For example, the respondents that were inclined to view that digital technologies are disruptive, i.e. change the paradigm of value creation in machine-building, also viewed that it is more related to service and business models than products and operation. Digitally enabled concepts like open innovation and industrial internet were perceived to be more disruptive drivers than the digital technologies themselves.",10.23919/PICMET.2017.8125295,M. Sommarberg; S. J. Makinen
Reshoring and Industry 4.0: How Often Do They Go Together?,2018,3,Topic_3_industry_manufacturing_chain,0.618435776288839,"This paper investigates the linkages between the reshoring of manufacturing to high cost countries and the adoption of technologies labeled Industry 4.0. Using a large database of reshoring initiatives to either the U.S. or Europe, results show that the adoption of technologies, such as robotics and three-dimensional printing, is relatively limited among reshoring companies, and often confined to reshoring strategies where the cost focus that originally motivated the relocation offshore is still retained. Reshoring firms with quality-oriented business strategies, and for which cost containment is not the top priority, engage in capital investment in new technologies mainly when they are involved in new product development.",10.1109/EMR.2018.2833475,A. Ancarani; C. Di Mauro
Healthcare Services Innovations Based on the State of the Art Technology Trend Industry 4.0,2018,-1,Outliers,0.545387948824322,"The contextual compendium analysis presented in this paper focuses on the Industry 4.0 and healthcare services innovation that relate to it. The appraisal discerns the specific components of Industry 4.0 and their related innovations or contribution in the healthcare industry. The first component, Cyber-physical systems, has led to Medical Cyber-physical systems applied in different circumstance to improve the efficiency of service provision. The second component, Internet of Things, has brought with it expanded networks, biosensors, smart pharmaceuticals, and other artificial organs. The final component has inspired the integrated of Natural Language Processing model as a calm-system operating in the background to complete a host of the process that improves diagnoses among other service provision and assistance functions. Additionally, the paper discusses Cognitive Computing, mHealth, and eHealth as emerging medical fields that can benefit from Industry 4.0.",10.1109/DeSE.2018.00016,M. Alloghani; D. Al-Jumeily; A. Hussain; A. J. Aljaaf; J. Mustafina; E. Petrov
Enablers for Industry 4.0,2018,3,Topic_3_industry_manufacturing_chain,0.7128577545739138,"Both, globalization and technological advancements do motivate manufacturing organizations to adopt Industry 4.0 practices. It implies an evolutional transformation, which aggregates advanced technologies and cyber-physical interactions. Some of required capabilities are already existing in companies, but some have to be developed/ adopted. The purpose of this paper is to isolate these enables required for Industry 4.0. Consequently, extensive literature review was carried out to synthesize the enablers for Industry 4.0, which are further grouped in an associated Mind Map. The main enablers were isolated as human, organization and technology in the higher level with associated subordinated enablers in an organized manner. The scientific implication is that this Mind Map can be used in analysis, when introducing weighs and importance factors. Practically, the mind map embodies a body of knowledge, i.e. the enablers, that have to be internalized by managers to achieve Industry 4.0 practices. Existing Industry 4.0 research is either focusing on partial topics in depth, or it gives a general overview of readiness for Industry 4.0, which is a wide collection of factors. To our best knowledge there is no similar work guiding throughout enablers visually, easing the adaptation process.",10.1109/ISMSIT.2018.8567293,C. A. Havle; Ç. Üçler
An Edge-Based Framework for Enabling Data-Driven Pipelines for IoT Systems,2019,1,Topic_1_data_big_big data,1.0,"Due to the proliferation of the Internet of Things (IoT) paradigm, the number of devices connected to the Internet is growing. These devices are generating unprecedented amounts of data at the edges of the infrastructure. Although the generated data provides great potential, identifying and processing relevant data points hidden in streams of unimportant data, and doing this in near real time, remains a significant challenge. Existing stream processing platforms require the data to be transported to the cloud for processing, resulting in latencies that can prevent timely decision making or may reduce the amount of data processed. To tackle this problem, we designed an IoT Edge Framework, called R-Pulsar, that extends cloud capabilities to local devices and provides a programming model for deciding what, when, and where data get collected and processed. In this paper, we discuss motivating use cases and the architectural design of R-Pulsar. We have deployed and tested R-Pulsar on embedded devices (Raspberry Pi and Android phone) and present an experimental evaluation that demonstrates that R-Pulsar can enable timely data analytics by effectively leveraging edge and cloud resources.",10.1109/IPDPSW.2019.00146,E. G. Renart; D. Balouek-Thomert; M. Parashar
Fractal Production Reprogramming “Industrie 4.0” Around Resource and Energy Efficiency?,2018,3,Topic_3_industry_manufacturing_chain,0.6097096375508091,"The evolution of the world of energy towards a system of decentralized production and consumption assets makes it possible to understand better the upcoming industrial revolution, which will be one of distributed information flows. A new paradigm, the “blockchain”, enables economic actors to completely decentralize information flows without intermediaries. Even though many technological locks still exist today, it becomes possible already today to imagine that each machine or connected object will be able to communicate autonomously and securely with others, learn and enrich its mode of operation thanks to artificial intelligence technologies. The business models will be strongly impacted by a greater flexibility of the manufacturing capacities, their adaptation in real time to consumer constraints and the optimization of supply management (raw material, energy and components) through smart purchase, onsite production, 3D-printing and smart logistics: a fractal production with shorter supply channels steered by resources and energy efficiency data.",10.1109/EEEIC.2018.8494395,R. Winkler-Goldstein; F. Imbault; T. Usländer; H. de la Gastine
Supply Chain Digitalization Overview SCOR model implication,2020,3,Topic_3_industry_manufacturing_chain,0.8173391577201433,"Digital transformation is reshaping the way business is managed all over the world. It represents a value driven process, increases efficiency for industrial practitioners and offers new opportunities towards new approaches and methods for academic researchers. Being widely known and constantly developed to increase supply chain excellence, by combining practice and research [1] [2], the supply chain operations reference model (SCOR) has been selected for this study. This paper's objective is to bring up the concept of digitalization in supply chain (SC), examine this emerging trend and highlight the SCOR model implication in integrating digitalization in SC.",10.1109/LOGISTIQUA49782.2020.9353936,A. Es-Satty; R. Lemghari; C. Okar
Tsdat: An Open-Source Data Standardization Framework for Marine Energy and Beyond,2021,-1,Outliers,0.4561187249570093,"Many organizations are tasked with the collection and processing of large quantities of data from various measurement devices. Data reported from these sources are often not interoperable with datasets and software used by analysts and other organizations in the same domain, introducing barriers for collaboration on large-scale projects. This poses a particular problem for cross-device comparisons and machine learning applications, which rely on large quantities of data from multiple sources. To address these challenges, the open-source Time-Series Data Pipelines (Tsdat) Python framework was developed by Pacific Northwest National Laboratory, with strategic guidance and direction provided by the National Renewable Energy Laboratory and Sandia National Laboratories to facilitate collaboration and accelerate advancements in the marine energy domain through the development of an open-source ecosystem of tools. This paper will describe the Tsdat framework and the data standards within which it operates. A beta version of Tsdat has been released and is being used by several projects in marine energy, wind energy, and building energy systems.",10.23919/OCEANS44145.2021.9706101,C. Lansing; M. Levin; C. Sivaraman; R. Fao; F. Driscoll
"A Systematic Review of Smart Farming: Technology, Process, and People",2025,-1,Outliers,0.4206262195364614,"This paper investigates the topic of smart farming by addressing three questions: what constitutes smart farming? which agricultural operations are enhanced through smart farming innovations? what particular technologies are implemented in each agricultural operation? This study uses a systematic literature review method with keyword searches in several publication databases. Article selection was based on year criteria, international journal publication, and application of inclusion-exclusion to obtain 57 relevant articles. The articles are analyzed to find answers to research problems and research gaps. The findings indicate that smart farming technologies like Internet of Things (IoT) devices, artificial intelligence (AI), and drones—improve critical processes, including crop management and yield. Technologies are classified according to their deployment in specific agricultural operations, accentuating the efficiency and scalability they contribute to farming activities. The obstacles and considerations associated with adopting smart farming, including technology implications, technical proficiency, and data or process governance, are essential for sustainable implementation. This paper provides researchers and practitioners wishing to advance smart farming systems by mapping technologies and processes. It underscores the necessity of aligning technology with the distinct requirements of agricultural practices.",10.1109/ICADEIS65852.2025.10933166,Y. Kurniawan; M. ER
Financial Technology (FinTech) in Advancing Sustainable Development in the Indian Agrarian Economy: A Systematic Review and Comprehensive Analysis,2025,-1,Outliers,0.4396831213363063,"This review analysis systematically examines the role of Financial Technology (FinTech) in enhancing the financial inclusion of small and marginal farmers in the Indian agrarian economy. A comprehensive analysis of peer-reviewed literature, government reports, and industry publications was conducted, focusing on the integration of digital financial services, such as digital wallets, mobile banking, micro-lending, and blockchain technology. The data collection process involved a thorough search of databases including IEEE Xplore, Google Scholar, and Scopus, using keywords relevant to FinTech adoption in agriculture. Studies published after 2020 were selected, highlighting the latest innovations like AI-driven credit scoring and blockchain in farming. The literature was organized around key themes: financial inclusion, technological integration, sustainability, and social empowerment, particularly gender equality. The review identified FinTech's positive impact on improving access to credit, promoting sustainable farming practices, and empowering women farmers. Limitations include the focus on rural India and potential exclusion of non-English sources. The findings suggest implications for future research and policy to foster sustainable digital development in agriculture.",10.1109/ICITIIT64777.2025.11041145,A. Sharma; A. Johri; D. Kaushal; K. Ahalawat; J. S. Chauhan
Water 4.0: An Integrated Business Model from an Industry 4.0 Approach,2019,3,Topic_3_industry_manufacturing_chain,0.6552093429322471,"Water is one of the most valuable natural resource used for everyday living. The era of Fourth Industrial Revolution (4IR); also referred to as Industry 4.0 has brought about different “Smart thinking” such as smart city, smart mobility, smart manufacturing and smart factory. In the water industry, the Germany Water Partnership is championing the Water 4.0 initiative since 2016. The essence of Water 4.0 initiative is to harness the digital revolution or decentralization of the water industry. This paper takes a close look at the business side of Water 4.0 from an 4IR perspective and aimed to develop an “Integrated Business Model” for Water 4.0 revolution. This paper is expected to answer this research question “can water industry be regarded as an integrated business?”. This paper started with a comprehensive literature reviews around the title of the research and integrated business model in relation to water industry. The literature reviews were analyzed and examined, and relevant factors considered suitable for integrated business model which are carefully selected for Water 4.0 paradigm.",10.1109/IEEM44572.2019.8978859,M. O. Alabi; A. Telukdarie; N. J. van Rensburg
Delivering on Industry 4.0 in a multinational petrochemical company: Design and execution,2019,3,Topic_3_industry_manufacturing_chain,1.0,"This paper provides an overall digital transformation and Internet of Things (IoT) deployment roadmap. It includes the objective of the transformation and the expected benefits of a digital transformation for an Oil & Gas multinational. The paper provides guidelines for allowing companies to digitally transform and deploy IoT technologies to optimize and improve business processes, delivering on digital. The roadmap includes: the end-to-end company business processes (i.e., operations management, human resources, use of Manufacturing Operations Management (MOM) systems, Business to Business (B2B), Business to Customer (B2C), etc.). The objective is to unleash the potential benefits of the IoT and Fourth Industrial Revelation (IR4.0) to support Oil & Gas companies in achieving digital mastery, by creating an ecosystem that is disrupting the current operating model. Key outputs of this research includes; achieving Operational Excellence by making use of the IoT technologies and digital capabilities (e.g., data analytics, robotics process automation, increase productivity, efficiency and profitability, reduce operational cost, and improve the work environment by providing the right data at the right time to the right user, to help make informed decisions. The research focuses on changing the business model from being just a product producer to provider of Cyber-Physical System (CPS)-as-a-Service, including: customer-oriented products, consulting services, and personalized user experiences. The system benefits derived include: improving leadership capabilities to improve current operation (e.g., informed decisions and accountability) and unleashing new business opportunities.",10.1109/ICFIR.2019.8894790,E. A. Buhulaiga; A. Telukdarie; S. J. Ramsangar
Critical Infrastructure for Industry 4 Laboratories and Learning Factories in Academia,2019,3,Topic_3_industry_manufacturing_chain,1.0,"Industry 4 laboratories, Learning Factories and Testlabs are vital facilities for the development and dissemination of the new industrial revolution. However, as they are implemented and reported in the literature at present, they are lacking the specific constituents and their combination and integration that would qualify them as revolutionary - i.e. being clearly distinctive from carefully implemented state of the art technology that has been the norm in the last decades. An essential attribute of Industry 4 systems, as presented in the literature, is the capacity for collection, processing and recording of critical data that describe and characterizes their state and the use of information extracted from the data to monitor, control and optimize the operations. Industry 4 laboratories need to provide real life context for teaching, research, business and industry purposes. This paper discusses the minimal required infrastructure for establishing relevant Industry 4 laboratories.",10.1109/IEEM44572.2019.8978796,R. Marian; D. Campbell; Z. Jin; M. Stumptner; J. Chahl
SGMiner: A Fast and Scalable GPU-Based Frequent Pattern Miner on SSDs,2022,1,Topic_1_data_big_big data,0.7902222692316833,"Frequent itemset mining is extensively employed as an essential data mining technique. Nevertheless, as the data size grows, the applicability of this method decreases owing to the relatively poor performance of the existing methods. Though numerous efficient sequential frequent itemset mining methods have been developed, the performance that can be achieved is clearly limited by the fact that they exploit only one thread. To overcome these limitations, a number of parallel methods using multi-core central processing units (CPUs), multiple machines or many-core graphic processing units (GPU) have been proposed. However, these methods are relatively slow in performance and have low scalability, mainly owing to large memory requirements for intermediate data, significant disk I/Os, and heavy computation. In this study, to resolve the aforementioned problems, we propose  ${\mathsf {SGMiner}}$ , which is a new, fast, and scalable GPU- and disk-based method on a single machine equipped with multiple graphic processing units (GPUs) and multiple solid-state drives (SSDs) for extracting frequent patterns. It is based on an algorithm similar to the Apriori algorithm and neither has intermediate data nor large disk I/O overheads owing to its exploitation of SSDs. Moreover, we propose storing transaction databases, namely bitmap transaction chunks, in SSDs, streaming the chunks to GPU device memory via the main memory with reduced I/O overhead, and performing fast support counting with GPUs based on the chunks. In addition, when exploiting multiple GPUs and SSDs, it proposes a concept of replicating bitmap transaction chunks stored in SSDs to GPUs in a streaming fashion. This could allow an almost equal workload to be distributed evenly across multiple GPUs with reduced I/O overheads. The experiments we conducted demonstrate that  ${\mathsf {SGMiner}}$  outperforms the existing methods in terms of scalability and performance with enhanced robustness.",10.1109/ACCESS.2022.3179592,K. -W. Chon; E. Yi; M. -S. Kim
Impact of the Fourth Industrial Revolution in the Productivity of Public Enterprises of National Level in Colombia,2019,3,Topic_3_industry_manufacturing_chain,0.6862065633940195,"The industrial revolutions have presented profound changes sourced by new technologies and new ways to perceive the world. This changes, due to their deep transformations are impacted on the productivity, economic and social systems, generating challenges and opportunities for the Governments, enterprises and people. The current research aim is to identify the impact of the fourth industrial revolution pillars in the productivity on colombian public enterprises of national level. About methodology, firstly, were classified the enterprises according to different sectors which the enterprises belong. Then, it was uses non probabilistic sample technic for judgment; it was selected 30 projects and 19 national public enterprises, which through primary a secondary source were asked about 4.0 industry application. The research results show that the 40 industry does not has expanded in the colombian state, however, currently there are important forwards in pillars and sectors. The most important pillars used bigdata - it is commonly used with artificial intelligence -, and cloud computing. The research allow to conclude that the application of fourth industrial revolution for colombian public enterprises of national level is in early stage however, there are important impacts in the productivity, of public enterprises in Colombia.",10.1109/CONIITI48476.2019.8960870,Y. De Jesús Muriel-Perea; L. P. Rodríguez Bernal; E. G. Galeano Camacho; F. N. Díaz-Piraquive; N. J. Acero López; M. R. Acero López; L. M. Alvarez Arteaga; W. P. Vargas Moreno; L. C. Cantor López
Impact of Industry 4.0 on Quality Management: identification of main challenges towards a Quality 4.0 approach,2021,3,Topic_3_industry_manufacturing_chain,1.0,"Internet of Things (IoT), artificial intelligence (AI) and especially machine learning (ML) are popular topics of Industry 4.0 (I4.0) and are more and more applied in factory operations. This paper provides a vision of how Quality Management is evolving towards a Quality 4.0 (Q4.0) approach. Although the term Q4.0 was introduced with the fourth industrial revolution, its use and understanding are still unclear and not widespread. In this paper, a bibliometric analysis on the topic has been carried out followed by a more general overview of literature on the impact of digitalization on Quality Management. The literature findings along with the empirical experience of the authors led to the development of a comprehensive framework depicting the challenges of different nature faced during the adoption of a Q4.0 approach from the initial awareness of the topic, till the implementation phase. After its empirical validation, the framework can be used as a basis to develop an implementation framework of Q4.0.",10.1109/ICE/ITMC52061.2021.9570206,D. Corti; S. Masiero; B. Gladysz
A Strategic Model and Framework for Intelligent Process Automation,2022,3,Topic_3_industry_manufacturing_chain,1.0,"Digital transformation has been growing inside companies and impacting the economy and society. Operations are one of the nine business dimensions where this transformation can be accelerated. This study answers the research goal of building a strategic model and framework for intelligent process automation implementation, to accelerate companies’ digital transition. The significant value and contribution of the proposed artifacts were proved by its demonstration and assessment.",10.23919/CISTI54924.2022.9820099,I. C. L. Feio; V. D. Dos Santos
Systematic qualitative review of Cloud Computing Adoption Challenges of the SME retailers in UAE,2022,3,Topic_3_industry_manufacturing_chain,0.8396313744164313,"Despite the numerous benefits of cloud computing and the Internet of Things (IOT), there remains a reluctance among UAE SMEs in the retail industry to use these technologies. This study is being performed to examine the function of IoT technology and cloud computing as a strategic instrument in the digitalization process of a retail SMEs. Despite independent research on these aspects of digital transformation in the past, no studies have been conducted to identify factors influencing SMEs motivation for incorporating these technologies into their strategy formulation and to gain a better understanding of the technology's possibilities in general as a strategic option for retailers in the UAE.The goal of this exploratory study is to sketch out the dynamics of a retailer's environment, which may influence decision-making concerning cloud and IoT deployment in the UAE's SME retail sector. Based on a thorough review of the literature, the ‘Unified Theory of Acceptance and Use of Technology’ (UTAUT) is implemented and extended, with the goal of conceptualizing a model that incorporates the discovered components. The findings of this study article may be used by academics for future research, as well as organizations to acquire fresh insights from their consumers in order to make informed and successful business decisions effectively.",10.1109/ICECCME55909.2022.9987968,V. K. Shrivastava; S. Riaz
Text mining analysis of wind turbine accidents: An ontology-based framework,2017,-1,Outliers,0.2805212445276691,"As the global energy demand is increasing, the share of renewable energy and specifically wind energy in the supply is growing. While vast literature exists on the design and operation of wind turbines, there exists a gap in the literature with regards to the investigation and analysis of wind turbine accidents. This paper describes the application of text mining and machine learning techniques for discovering actionable insights and knowledge from news articles on wind turbine accidents. The applied analysis methods are text processing, clustering, and multidimensional scaling (MDS). These methods have been combined under a single analysis framework, and new insights have been discovered for the domain. The results of our research can be used by wind turbine manufacturers, engineering companies, insurance companies, and government institutions to address problem areas and enhance systems and processes throughout the wind energy value chain.",10.1109/BigData.2017.8258305,G. Ertek; X. Chi; A. N. Zhang; S. Asian
Towards a Data Business Maturity Model for Software-intensive Embedded System Companies,2023,1,Topic_1_data_big_big data,0.30250941138353826,"Data has been quickly becoming as the fuel, the new oil, of growth and prosperity of companies in the modern age. With useful data and suﬃcient tools, companies have the ability to enhance their current products, presents new innovations and services as well as generate new revenue streams with a secondary customer base. While there are ongoing efforts to develop machine learning and data science techniques, little attention has been paid to understanding and characterizing data-related business activities in software-intensive companies.This multiple-case study examines four large international embedded system companies to explore how they are utilizing data and how they have proceeded in their journey in the data business. This study identifies six distinct stages, each with unique challenges, that seems to be common for embedded system companies in their data business. As the result, this study presents an initial data business maturity model for software-intensive embedded system companies. Additionally, this research provides a foundation for future efforts to support software-intensive embedded system companies in establishing data businesses.",10.1109/ICE/ITMC58018.2023.10332290,S. Hyrynsalmi; H. H. Olsson; J. Bosch
A Future Outlook of Power Devices From the Viewpoint of Power Electronics Trends,2024,3,Topic_3_industry_manufacturing_chain,0.37619082641435075,"A outlook for power devices is discussed from the perspectives of power device and power electronics application trends. The driving force of the power device business has changed from home appliances and industrial robots to electric vehicles (EV), and the most important trend has been the increase in power density in power modules by power loss reduction, high-temperature operation, and thermal resistance reduction. In the future, it can be expected that inverter-based resources and power supply integrated circuits (ICs) will be new driving forces for green transformation (GX) and digital transformation (DX). To continue the increasing power density trend of power modules, wide bandgap power devices are attractive. However, economic parameters, such as “cost/power,” must be improved for wide installation in power grids with large volumes. In addition, it can be expected that hetero integration into power modules and GaN power ICs will be a new demand.",10.1109/TED.2023.3332611,W. Saito
Internet-of-Things and Cloud Computing for Smart Industry: A Systematic Mapping Study,2017,3,Topic_3_industry_manufacturing_chain,1.0,"Intelligent industry and manufacturing requires obtaining relevant sensor data and process information in real-time from all components in the manufacturing value-chain. It is envisioned that smart industry is achieved by embedding connectivity into industrial products, using Cloud and Internet-of-things (IoT) to leverage intelligence and actionable knowledge for machines, autonomous collaboration among machines, and integration of products and additional value-added services. For complex industrial systems, it is important to ensure a smooth transformation towards the smart industry vision despite of the associated challenges with respect to e.g., transition from the traditional multi-layered architecture to an open structured service-oriented automation system architecture, changes of business models and strategies, legacy system migration to cloud environment, etc. The focus of this study is therefore to examine the status of the existing research on cloud computing and IoT solutions that enable this transformation towards a smart industry. We applied the systematic mapping study method to obtain an overview of the existing related research literatures that focus on smart industry, industrial automation and manufacturing perspective. We also discuss the future research areas that need to be enhanced.",10.1109/ES.2017.56,H. P. Breivold
Digital transformation of maintenance processes for analytical instrumentation in the Philippine manufacturing industry,2024,3,Topic_3_industry_manufacturing_chain,1.0,"Remote work has been an integral part of business strategies across the globe since the onset of the COVID-19 pandemic. Though business operations and work functions have returned to their usual modes, remote access to business intelligence remains a crucial part of business operations, particularly in the manufacturing sector, allowing top executives and key decision makers to view real-time scenarios related to manufacturing operations and supply chains. Maintenance is another aspect of business operation that is of particular importance to the manufacturing industry, where critical equipment downtime could translate to significant economic losses. Through digital transformation, manufacturing companies can tap into technologies such as cloud computing, IoT (Internet of Things), and AI (artificial intelligence) to allow for more seamless monitoring of operating parameters and enable proactive approaches to equipment maintenance and reliability, thus ensuring continuous and efficient plant operations.",10.1109/ICCAD60883.2024.10553811,T. D. De Sagun; E. D. Gutierrez; J. S. Elnar; M. G. Eroa; S. A. Magon; A. D. De Sagun; V. C. Macatangay
The Influence of Internet on the Performance of Rural Family Entrepreneurship—Data Analysis Based on CFPS2020,2023,3,Topic_3_industry_manufacturing_chain,0.6757434350786747,"Rural family entrepreneurship not only provides new labor resources for the development of industrial chain in China, but also has an effective positive impact on the upgrading of value chain. Based on the data of China family follow-up survey in 2020, this paper makes an empirical study on the influence of Internet on rural family entrepreneurship. The results show that the entrepreneurial income of families using the Internet is about 15% higher than that of families not using the Internet. There is a positive relationship between online time and family entrepreneurial income. If the average online time is increased by one minute per week, the average family entrepreneurial income will increase by 20 yuan. Through the heterogeneity test of age and years of education, it can be concluded that the middle-aged and elderly people are more influenced by the Internet in terms of entrepreneurial performance, and the Internet has a more significant positive impact on the entrepreneurial performance of families with higher years of education. This paper proposes to popularize the use of the Internet in rural areas, strengthen online education and training, provide a new source of labor for industrial development and supply chain improvement, and continue to promote the upgrading and modernization of China's industrial structure.",10.1109/IIoTBDSC60298.2023.00040,P. Zhou
Development of a sustainable business model during Covid-19 for agri-food system,2024,-1,Outliers,0.4071543972635166,"In the world economy, the COVID-19 pandemic has more influence and it is significant negative, that has both direct and indirect effects of the necessary measures put in place to halt the spread of the illness. These impacts also have an effect on the agriculture and food industries. The pandemic confronts the food system with several short-term challenges, but it also provides a potential to accelerate reforms in the food and agriculture sector that will improve its robustness to several dangers, including climate change. The global food production and consumption methods that are currently in place are fundamentally flawed, as the COVID-19 epidemic has brought to light. These flaws involve inefficient use of natural resources, the transfer of unsuccessful manufacturing, and the underutilization of regional particularities, all of which negatively impact the natural and social values of communities. A more appropriate, equitable, and robust post-pandemic food safety and nutrition security scheme is necessary for everyone around the globe. This requires more impactful, eco-sustainable, site-incorporated agri-food manufacturing, optimised food distribution and maintenance, particularly in urban areas, and healthier and more feasible feeding habits.",10.1109/ICACCM61117.2024.11059042,D. N. Anh; S. Chandra; S. M. Vali; A. Ghosh; A. Sharma; N. R. Joshi
Open Technology Management for Maximizing the Public Value of Large Language Models,2024,3,Topic_3_industry_manufacturing_chain,0.7285414743398275,"The legal, ethical, and safe use of machine learning (ML) is crucial for digital transformation in an AI-ready society. The issues are to interpret and manage the behavior of ML models. Unlike ordinary software, the behaviors of ChatGPT with a large language model (LLM) called GPT3.5 which involves massive neural networks, are learned from data, not programmed explicitly, not having a specification, and being changed dynamically in use. So volatile and dynamically changing technology that is limited in its user control with prompts requires managing the technology development process in addition to the technology itself to maximize its benefits. AI and its applications have been the most potential research area for a long time. Due to the Internet's openness, people know its specifications to make the most of it. Because business organizations do not fully understand how ChatGPT acts and do not have opportunities to adapt its proficiency as their competitive advantage, they cannot effectively manage their organizational resources. Aiming to maximize the public value of LLMs, this paper analyzes the impacts of an LLM-based AGI on digital transformation management and proposes an open technology management framework for LLM-based AGI.",10.23919/PICMET64035.2024.10652994,A. Tomita
A novel architecture for enhancing Electronic Health Record interoperability: A Blockchain-based approach,2021,2,Topic_2_data_privacy_security,0.4746808431842252,"Healthcare has been improved by information and communication technologies. Electronic health records are one of the critical drivers of this transition. Moreover, the number of records on personal health grows exponentially with the rise of mobile and omnipresent computing. This trend is characterized by wearable computing technology and various kinds of sensors related to health domain. Results in need of an automated data storage method that healthcare professionals and patients can use. There is currently no trusted medical data sharing and storage system in the chain of health value. Therefore, no platform is used to track the traceability of patients across the entire healthcare chain. Our goal is to create an integrated view based on interoperability principles that will collect such data from patients' medical records, based on the use of sensors from different wearable devices and other sources. The data collected is consolidated into a single electronic health records system, where the patient is the owner of data by interconnecting multiple healthcare providers. This strategy offers improved privacy advantages while ensuring safe access to medical records for patients. The original paper contributes to the description of uniting the entire medical value chain, the design of the system architectural model, the creation of system components, and the validation through a proof of concept (PoC). The PoC can show the feasibility of data exchange between health care professionals and incorporating the health records of heterogeneous sources like health care systems and sensors.",10.1109/TEMSCON-EUR52034.2021.9488599,P. Centobelli; R. Cerchione; E. Riccio
Software Defined Cyberinfrastructure,2017,1,Topic_1_data_big_big data,0.6459979411892942,"Within and across thousands of science labs, researchers and students struggle to manage data produced in experiments, simulations, and analyses. Largely manual research data lifecycle management processes mean that much time is wasted, research results are often irreproducible, and data sharing and reuse remain rare. In response, we propose a new approach to data lifecycle management in which researchers are empowered to define the actions to be performed at individual storage systems when data are created or modified: actions such as analysis, transformation, copying, and publication. We term this approach software-defined cyberinfrastructure because users can implement powerful data management policies by deploying rules to local storage systems, much as software-defined networking allows users to configure networks by deploying rules to switches.We argue that this approach can enable a new class of responsive distributed storage infrastructure that will accelerate research innovation by allowing any researcher to associate data workflows with data sources, whether local or remote, for such purposes as data ingest, characterization, indexing, and sharing. We report on early experiments with this approach in the context of experimental science, in which a simple if-trigger-then-action (IFTA) notation is used to define rules.",10.1109/ICDCS.2017.333,I. Foster; B. Blaiszik; K. Chard; R. Chard
Research on the Application and Development of Intelligent Inspection System,2020,3,Topic_3_industry_manufacturing_chain,0.6523812825982331,"With the development of internet and artificial intelligence, Internet+ and intelligence become the leading development direction of inspection. As the value chain extension and the driving force to reform the testing industry, the intelligent inspection system plays a more and more important role. This paper analyzes the research and application of intelligent inspection systems and studies the relevant macro-environment from four aspects based on the PEST method. Then, combined with the analysis of the upstream and downstream industries, the development prospect of the intelligent inspection system is discussed.",10.1109/ICIBA50161.2020.9277209,Q. Shi; S. Han; J. Li; Y. Li; J. Li; X. Yang; Y. Xu
A Collaborative Framework for Semi-Automatic Scenario-Based Mining of Big Road User Data,2023,-1,Outliers,0.49932726491857454,"Traffic research has benefited from a significant expansion in the amount of available data. Consequently, the need arises for an automatic and efficient method to extract and analyze relevant traffic situations instead of a more traditional and manual approach like manual video annotation. This paper presents a framework to create such a data pipeline. The user must define the target scenarios and the pipeline will abstract the available trajectory data into candidate scenes (groups of interacting trajectories) and select the matches for the target scenarios. These scenes will be mined and modelled automatically for new valuable information. Furthermore, Surrogate Measures of Safety (SMoS) are applied to identify the critical and atypical scenes of the target scenarios. A set of eight scenarios containing interactions between bicycles and MRUs (Motorized Road Users) at the AIM (Application Platform for Intelligent Mobility) Research Intersection in the city of Braunschweig, Germany, was mined by a team of three researchers using the presented framework to validate it with positive results.",10.1109/ITSC57777.2023.10422330,I. I. Da Silva; M. Zhang; K. Gimm
A Denoising and Structured Segmentation Method for IC Images via Integration of YOLOv5 and Lightweight UNet,2025,1,Topic_1_data_big_big data,0.2968358848563786,"The chip industry is a critical foundation for national security and economic development, with integrated circuit (IC) reverse engineering serving as a key component in the semiconductor value chain. IC reverse engineering involves multiple steps, including layer-by-layer imaging using a Scanning Electron Microscope (SEM), device identification, gate-level netlist extraction, and functional inference. Among these, the segmentation of electrical components and metal lines from the background of IC images is a prerequisite for device recognition and related procedures. During the imaging process, significant noise often arises due to circuit damage, which may cause genuine circuit regions to be misidentified as background, thereby interfering with subsequent structural recognition and design processes. To effectively address this issue, this study proposes the use of the YOLOv5 detection model to identify and remove noise regions in circuit images. By treating noise as a detection target and performing network training and performance evaluation, experimental results show that the method achieves 87.26 % detection precision on the test set, demonstrating its effectiveness in noise identification. After denoising, a lightweight segmentation network, MobileNetV2-UNet, is further applied for structured segmentation of the entire IC image. Considering the possibility of residual noise, the proposed model integrates noise-robust mechanisms and specific optimizations for blurred edges. Experiments were conducted on a proprietary IC image dataset provided by an industry partner. The final results show effective segmentation of metal wiring regions, with an mean Intersection over Union (mIoU) of 88.62 % and a mean Pixel Accuracy (MPA) of 94.03 %, outperforming many mainstream methods. These results validate the proposed approach's potential and practical value in complex IC image segmentation tasks.",10.1109/CAIBDA65784.2025.11183565,H. Cheng; C. Yu; Z. Yang; C. Zhang
Integrating Engineering Principles in Data Literacy Workshops for Primary Education,2025,-1,Outliers,0.14942959521033883,"Data literacy is an essential skill in today's and future world. Children should be introduced to data literacy concepts in ways appropriate to their age, using real-world data examples and hands-on experiences. As part of the Erasmus+ DIRECTORS project, three workshops (each having two sessions) for children aged 8-10 (ISCED level 1) are currently being developed and implemented in selected primary schools in Croatia and the Netherlands. The first cycle of workshops was completed in November 2024, and the workshops - together with accompanying open educational resources - are now being improved for the final implementation in April 2025. This paper focuses on the engineering principles “hidden” in our DIRECTORS data literacy workshops. While we primarily emphasize data literacy and digital skills, engineering elements are intertwined throughout our whole program and in all data lifecycle phases: collection, processing, analysis, visualization, as well as critical thinking, and drawing conclusions. We give an overview of the engineering principles present in data lifecycle phases of each workshop. We describe which activities were used to implement them, and we discuss the importance of each principle in our workshops, making small steps to a more successful, albeit somewhat “hidden”, engineering education.",10.1109/MIPRO65660.2025.11132082,I. Bosnić; A. K. Divjak; F. W. Donker; B. van Loenen
"Internet of Things: State-of-the-art, Computing Paradigms and Reference Architectures",2022,1,Topic_1_data_big_big data,1.0,"The Internet of Things (IoT) makes it possible to connect objects or things to the internet, with the purpose of collecting data and controlling processes or machines remotely. IoT enables the physical world to be integrated into the digital world in order to optimize time, save costs and facilitate human labor. An IoT system comprises a rich ecosystem of elements that make up its value chain, which includes a computational and communication architecture or model, components and technologies. IoT has evolved rapidly, producing an exuberant scientific literature. This document presents the state of the art of IoT, with updated sources, that guides the reader, who is entering the world of IoT, to have a starting point for future research. In addition to the review of IoT architectures, components of the IoT ecosystem, computational paradigms and, security and governance aspects. Our main contribution is focused on the analysis of the Middleware layer in IoT architectures, oriented to the storage and processing of data.",10.1109/TLA.2022.9662173,B. Mazon-Olivo; A. Pan
Research on China's GDP forecast based on ARIMA model,2022,3,Topic_3_industry_manufacturing_chain,0.49521617161911835,"With the rapid development of digital technology, the proportion of the tertiary industry in the national economy has become an important indicator to measure a country's economic level. A country's economic and social stability is inseparable from the development of the real economy. Capitalist countries led by the United States have also returned to the manufacturing industry to promote the optimization and upgrading of the national industrial structure and restructure the status of the value chain. These measures have a profound impact on China's import and export trade dominated by the export of primary products, and then affect the stable development of China's national economy. Therefore, in the unstable international environment, in order to more accurately specify import and export trade policies and relevant policies to maintain stable economic growth, the prediction of GDP is particularly important. This paper focuses on the research of the national economy at the macro level, and establishes ARIMA model to reasonably predict the future GDP trend, so as to formulate macroeconomic policies more accurately and maintain the rapid and stable development of the national economy. The empirical results show that ARIMA model can well predict the future trend of GDP and provide guidance for policy-making.",,J. Zhan
The Review for Visual Analytics Methodology,2022,1,Topic_1_data_big_big data,0.46866284173967826,"Big data usage evolves from previously looking into the capacity of big data's descriptive and diagnostic perspectives into currently feeding the demands for predictive big data analytics. The needs come about due to organizations that crave predictive analytics capabilities to reduce risk, make intelligent decisions, and generate different customer experiences. Similarly, visual analytics play an essential role in understanding and fitting the analytics prediction in their business decision. Hence, the combination of descriptive, diagnostics and predictive within Visual Analytics emerges as a balanced field to provide understandable predictive insight. Due to the organizational demand and multi-discipline area, the approach to developing visual analytics is still uncertain in the Big Data Project Lifecycle from methodological perspectives. While there are a few potential methodological approaches that could be used for visual analytics, they are scattered across numerous academic research and industrial practice. To date, there is no coherent review and analysis of the work that has been explored specifically for Visual Analytics methodology. This paper reports on a review of previous literature concerning how Visual Analytics has been executed in the big data life cycle to address the gap. The review is organized in this study from three perspectives: i) general ICT -related methodology (e.g. SDLC, Agile, DevOps), ii) Data Science-related methodology (e.g. CRISP-DM, SEMMA, KDD) and iii) Visual Analytics-related methodologies in which each method will be benchmarked based on the Visual Analytics major part of reality, computer and human, in terms of its width, depth, and flows. This study found insufficiencies, non-specific and vague conditions in handling the Visual Analytics when using current methodological approaches based on the review conducted. The paper also highlights the Visual Analytics-related methodological review, which can shed some light on the approaches and ways of implementing analytics in the big data lifecycle, which can be beneficial for future studies in proposing a more comprehensive methodology for Visual Analytics in the big data lifecycle.",10.1109/HORA55278.2022.9800100,Z. Ahmad; S. Yaacob; R. Ibrahim; W. F. Wan Fakhruddin
Securing Data Platforms in B2B Enterprises: A Framework for the Protection of Customer Data,2024,2,Topic_2_data_privacy_security,0.6959783865770847,"In business-to-business (B2B) enterprises, the development of data platforms for processing customer data is increasingly common. These platforms ingest data into a diverse array of relational and nonrelational data stores, data lakes, and data warehouses. The processing phase employs a varied set of technologies encompassing streaming and batching technologies. The resulting processed data are stored in a multitude of formats, serving diverse purposes, such as deriving analytics, creating training datasets for machine learning models, and building data products for customers. Throughout the data lifecycle, the data encounters various touchpoints with engineers and other employees who have access to it, emphasizing the critical need for enterprises to protect these data. [1] Traditional methods of encrypting data and implementing data access controls may fall short, given the myriad ways in which data are consumed. [2] Furthermore, despite individual technologies offering data protection mechanisms, there is a distinct lack of a comprehensive and holistic approach that encompasses the entire data platform. These technology-specific measures are insufficient for adequately addressing the diverse use cases encountered in real-world scenarios. This study addresses the critical need for robust security measures in data platforms, proposing a comprehensive framework with five key pillars: data masking, data cataloging, data offboarding, data access control and data auditing. Our approach ensures the protection of both the structured and unstructured customer data processed on these platforms. Data Masking safeguards sensitive information, while Data Cataloging provides efficient organization and central cataloging of diverse data assets. Data Offboarding is essential for the secure and compliant deletion of customer data at the end of the data lifecycle, and the Audit pillar ensures ongoing monitoring and adherence to regulatory standards. This framework is especially relevant for B2B enterprises, where customer data security is not only a regulatory obligation, but also fundamental to maintaining trust and business integrity. By presenting a detailed strategy for protecting customer data on these platforms, this study aims to guide B2B enterprises in implementing effective security measures, thereby ensuring the confidentiality, integrity, and compliance of vital data assets in an increasingly data-centric business world.",10.1109/ICDECS59733.2023.10502480,M. Khoje; S. Ponnusamy
On Designing a Generic Framework for Big Data-as-a-Service,2018,1,Topic_1_data_big_big data,0.8806475940870383,"Data is the heart of any system, which is perhaps the reason why big data is such a `big' deal. With the growing application domain of the technology, research towards big data analytical applications' development has gained immense attention lately. The cost-effective and scalable nature of Cloud Computing has made it an obvious infrastructural choice. However, until APIs are developed for provisioning analytical services, big data research will not meet its desired outcome. Moreover, existing frameworks are application-specific or cater to only a specific set of requirements. This paper proposes a framework for creating a technological stack for Big Data-as-a-Service. Besides this, it also presents criteria for technology selection, which is based on the identification of computing and data models for the application. Therefore, the proposed framework is application-independent and accommodates all the segments of the big data lifecycle.",10.1109/ARESX.2018.8723269,S. Khan; K. A. Shakil; S. Arshad Ali; M. Alam
"Mobile apps and data privacy: When the service is free, the product is your data",2017,2,Topic_2_data_privacy_security,1.0,"The mobile phone, especially in its latest incarnation “the smartphone”, has become an everyday necessity to billions of users around the globe. Frequent use has rendered the smartphone a vault where valuable sensitive information is stored. On the other hand, mobile applications try to exploit this loophole by gaining access to data and consequently providing them to other stakeholders in the Internet economy value chain. The paper examines the extent of access to personal information required by mobile applications. More specifically, the most popular free mobile apps offered in Google Play store are analyzed in terms of the relation between the required phone data access and several other app related metrics. Our analysis shows that applications overly extend their reach on mobile users' personal data based on the fact that most users are not sufficiently aware that their privacy is compromised via the phone and thus has to be protected.",10.1109/IISA.2017.8316392,S. E. Polykalas; G. N. Prezerakos; F. D. Chrysidou; E. D. Pylarinou
Nonlinear Function Acceleration Based on FPGA Heterogeneous Platform,2021,1,Topic_1_data_big_big data,0.404710820180592,"To solve the problem that the convolutional neural network model is limited by the resources of the embedded platform, this paper proposed and designed a target detection platform accelerated by a heterogeneous chip based on FPGA . The platform uses FPGA heterogeneous chips to accelerate the compressed yOLO v2 model. In the compression process, channel pruning is used to remove the redundant channels in the convolutional neural network model and reduce the model parameters to save the hardware storage resources. The hardware platform adopts Xilinx Pynq-Z2 board and the image is preprocessed by ARM. Then, the processed image data and model parameters are transmitted to FPGA through AXI bus for layer by layer convolution network acceleration. Aiming at the implemented acceleration platform, the implementation mode of nonlinear function operation in FPGA is further studied to optimize the acceleration effect. The experimental results show that the average processing time of each image is 530ms and the average precision is 0.7582 after adding PE split cache data pipeline, which is 20ms faster than that without PE split cache data pipeline. A part of the nonlinear operations is fitted in FPGA by function mapping method or segmented Taylor expansion to achieve greater acceleration.",10.1109/CAC53003.2021.9728099,T. Xie; Y. Ma; W. Feng; L. Chang; C. Yu
The Importance of Artificial Intelligence in Green Innovation,2024,3,Topic_3_industry_manufacturing_chain,0.5251349116319851,"The study focuses on Artificial Intelligence’s importance in developing green innovation. Artificial intelligence significantly enhances green innovation by enhancing productivity, accelerating environmentally friendly technological advancements, and facilitating better decision-making through energy optimisation, waste reduction, and smart infrastructure support. This research adopted a scoping review approach utilising the Scopus database as a source of documents. The study highlights several aspects, such as publication trend by years, publication source and context analysis. The reviews included documents published since 2021. The method section is derived from the PRISM-ScR checklist table. The highest publisher was the Business Strategy and The Environment journal, and the country with the highest publishing was China. In addition, the study recommends that more efforts should be exerted to increase companies’ awareness of the importance of green innovation and the feasibility of developing green products.",10.1109/ICCSCE61582.2024.10696270,E. H. A. Lawati; M. A. M. Ali; N. M. Tahir
Enhancing Customer Delight-The Implementation of Artificial Intelligence Tools in Sales and Marketing in Online Platform,2024,3,Topic_3_industry_manufacturing_chain,0.5635574557392765,"Artificial Intelligence (AI) plays a crucial role in consumer strategic decisions and competitiveness, transforming the digital landscape. The emergence of artificial intelligence has made marketing more difficult during the last decade. Predictive marketing has improved our understanding of customer decisions. The modern growths in AI-driven computerization represent substantial shifts in the AI ecosystem. AI technologies have the potential to perform a variety of purposes throughout the retail value chain, including the hiring of personnel to complete tasks. Because of the increased level of competition in the retail business, merchants must deliver higher-quality services and prioritize their customers more than ever before. It is seen that the shape of reconfigured concepts, interests in the sphere of AI adoption by enterprises. How companies improve customer pleasure, experience, and loyalty is shifting as a outcome of the use of artificial intelligence (AI) into sales operations. This paper's managerial implications are relevant as deploying AI in competitive businesses can improve decision-making.",10.1109/ICERCS63125.2024.10895532,J. S. Kadyan; M. Jayakumar; A. K. S; L. Jenefa; L. T; D. C. R
Cloud based big data infrastructure: Architectural components and automated provisioning,2016,1,Topic_1_data_big_big data,0.7967081140234645,"This paper describes the general architecture and functional components of the cloud based Big Data Infrastructure (BDI). The proposed BDI architecture is based on the analysis of the emerging Big Data and data intensive technologies and supported by the definition of the Big Data Architecture Framework (BDAF) that defines the following components of the Big Data technologies: Big Data definition, Data Management including data lifecycle and data structures, Big Data Infrastructure (generically cloud based), Data Analytics technologies and platforms, and Big Data security, compliance and privacy. The paper provides example of requirements analysis and implementation of two bioinformatics use cases on cloud and using SlipStream based cloud applications deployment and management automation platform being developed in the CYCLONE project. The paper also refers to importance of standardisation of all components of BDAF and BDI and provides short overview of the NIST Big Data Interoperability Framework (BDIF). The paper discusses importance of automation of all stages of the Big Data applications developments, deployment and management and refers to existing cloud automation tools and new developments in the SlipStream cloud automation platform that allows multi-cloud applications deployment and management.",10.1109/HPCSim.2016.7568394,Y. Demchenko; F. Turkmen; C. de Laat; C. Blanchet; C. Loomis
Context-Driven Granular Disclosure Control for Internet of Things Applications,2019,2,Topic_2_data_privacy_security,0.6317816753605698,"The Internet of Things (IoT) represents a technology revolution transforming the current environment into a ubiquitous world, whereby everything that benefits from being connected will be connected. Despite the benefits, the privacy of these things becomes a great concern and therefore it is imperative to apply privacy preservation techniques to IoT data collection. One such technique is called data obfuscation in which data is deliberately modified to blur the sensitive information, while preserving the data utility. The current obfuscation techniques, however, focus on the privacy of published datasets shared with untrusted parties. The high connectivity and distributed nature of IoT, opens up the possibility of privacy compromise before obfuscation can take effect, and therefore privacy enforcement should be deployed at earlier stages. Additionally, classical privacy treatments are too restrictive for IoT, where coarser/finer data details should be revealed for different applications. Motivated by these challenges, we propose a framework for privacy preservation in IoT environments that is capable of multi-granular obfuscation by enforcing context-driven disclosure policies. Then, we customize our framework for a smart vehicle system and make use of data stream watermarking techniques to protect privacy at different stages of the data lifecycle. To address possible concerns about additional performance overhead, we show the burden to be very lightweight, thus validating the suitability of ubiquitous use of our framework for IoT settings.",10.1109/TBDATA.2017.2737463,A. Soltani Panah; A. Yavari; R. van Schyndel; D. Georgakopoulos; X. Yi
A Framework for Safe AI: Data Governance and Ecosystem Structure,2025,-1,Outliers,0.3599674278122188,"Large Language Models have become a foundational component of modern artificial intelligence, but their development is often hindered by inadequate data governance, resulting in challenges such as hallucinations, intellectual property violations, and security vulnerabilities. In light of emerging regulatory requirements, this paper presents a Collaborative Safe AI Framework (CSAIF) for building safe AI systems through robust data lifecycle management and ecosystem collaboration. This paper analyzes governance principles drawn from the U.S. Blueprint for an AI Bill of Rights and the EU AI Act, emphasizing transparency, traceability, explainability, and auditability. Existing industry practices are reviewed to identify current strengths and limitations. This paper then introduces an approach that treats data as verifiable digital assets and a Data Container architecture to encapsulate both content and governance metadata. This design enables version control, access management, data sovereignty, and usage logging across the AI model lifecycle. The CSAIF defines the responsibilities of data providers, validation entities, application developers, and regulatory actors, and outlines a process that ensures data integrity, lawful use, and accountability. By integrating technical safeguards with operational oversight, the proposed CSAIF establishes a trustworthy foundation for developing and deploying AI models in compliance with legal and ethical standards.",10.1109/AITest66680.2025.00008,W. -T. Tsai; L. Zhang
EDISON Data Science Framework (EDSF): Addressing Demand for Data Science and Analytics Competences for the Data Driven Digital Economy,2021,-1,Outliers,0.15373060189405147,"Emerging data driven economy including industry, research and business, requires new types of specialists that are capable to support all stages of the data lifecycle from data production and input to data processing and actionable results delivery, visualisation and reporting, which can be jointly defined as the Data Science professions family. Data Science is becoming a new recognised field of science that leverages the Data Analytics methods with the power of the Big Data technologies and Cloud Computing that both provide a basis for effective use of the data driven research and economy models. Data Science research and education require a multi-disciplinary approach and data driven/centric paradigm shift. Besides core professional competences and knowledge in Data Science, increasing digitalisation of Science and Industry also requires new type of workplace and professional skills that rise the importance of critical thinking, problem solving and creativity required to work in highly automated and dynamic environment. The education and training of the data related professions must reflect all multi-disciplinary knowledge and competences that are required from the Data Science and handling practitioners in modern, data driven research and the digital economy. In modern conditions with the fast technology change and strong skills demand, the Data Science education and training should be customizable and delivered in multiple forms, also providing sufficient lab facilities for practical training. This paper discusses aspects of building customizable and interoperable Data Science curricula for different types of learners and target application domains. The proposed approach is based on using the EDISON Data Science Framework (EDSF) initially developed in the EU funded Project EDISON and currently being maintained by the EDISON Community Initiative.",10.1109/EDUCON46332.2021.9453997,Y. Demchenko; C. G. J. José; S. Brewer; T. Wiktorski
DataPAL: Data Protection and Authorization Lifecycle framework,2021,2,Topic_2_data_privacy_security,0.863423055519866,"This paper introduces a new model for handling data privacy throughout data lifecycle via the introduction of a policy profile using the Abbreviated Language For Authorization (ALFA) policy language. Our approach extends previous models In three complementary ways: (1) By introducing Administration and Delegation Profile (ADP) in ALFA policy where users and companies can restrict the scope of access/usage policies related to data as well as specify a chain of custody for data (moreover such an approach eases up the tasks of handling users' consent); (2) Thanks to our framework Usage Control System Plus (UCS+) users can monitor the usage of data and revoke its usage upon specific conditions or at will; (3) By introducing new states for policy evaluation, i.e. Admissible/NotAdmissible to filter out those applicable policies that were unauthorized in the first place.",10.1109/SEEDA-CECNSM53056.2021.9566212,S. Bandopadhyay; T. Dimitrakos; Y. Diaz; A. Hariri; T. Dilshener; A. La Marra; A. Rosetti
Simplifying Multi-layer and Multi-tenant Support in OpenStack: The SACHER Use Case,2019,2,Topic_2_data_privacy_security,0.8341776379290092,"The majority of cloud computing deployment environments follow the typical service delivery models, i.e., Software as a Service (SaaS), Platform as a Service (PaaS) and the Infrastructure as a Service (IaaS), that provide specific functionalities to users depending on the service delivery layer and clear resource isolation among different tenants. However, in certain scenarios, such as the Cultural heritage one, it is often required to provide SaaS, IaaS and PaaS cross-functionality support to users and cross-tenant resource visibility among isolated tenants. In the CH scenario, for example, restorers and professionals may often need to connect and to share temporarily data among different tenants or to work with old customized software requiring the possibility to exploit specific management functionalities that are typically available not at the SaaS but at the IaaS or PaaS layers. This paper proposes a middleware called Registry that can provide additional functionalities to different user categories by joining the SaaS with the IaaS layer and that offers also multi-tenancy operations making private data available to certain project on-demand and enabling, in this way, cross-tenant data sharing. The Registry has been designed, developed and tested within the context of the SACHER project that provides a cloud-based infrastructure for the management of the cultural data lifecycle.",10.1109/ISCC47284.2019.8969678,L. Foschini; G. Martuscelli; R. Montanari
IoT-Reg: A Comprehensive Knowledge Graph for Real-Time IoT Data Privacy Compliance,2023,2,Topic_2_data_privacy_security,0.32010337374076936,"The proliferation of the Internet of Things (IoT) has led to an exponential increase in data generation, especially from wearable IoT devices. While this data influx offers unparalleled insights and connectivity, it also brings significant privacy and security challenges. Existing regulatory frameworks like the United States (US) National Institute of Standards and Technology Interagency or Internal Report (NISTIR) 8228, the US Health Insurance Portability and Accountability Act (HIPAA), and the European Union (EU) General Data Protection Regulation (GDPR) aim to address these challenges but often operate in isolation, making their compliance in the vast IoT ecosystem inconsistent. This paper presents the IoT-Reg ontology, a holistic semantic framework that amalgamates these regulations, offering a stratified approach based on the IoT data lifecycle stages and providing a comprehensive yet granular approach to IoT data handling practices. The IoT-Reg ontology aims to transform the IoT domain into a realm where regulatory controls are seamlessly integrated system components by emphasizing risk management, compliance, and the pivotal role of manufacturers’ privacy policies, ensuring consistent adherence, enhancing user trust, and promoting a privacy-centric IoT environment. We include the results of validating this framework against risk mitigation for Wearable IoT devices.",10.1109/BigData59044.2023.10386545,K. U. Echenim; K. P. Joshi
Design and Implementation of IoT-ML Based Data Management for Sustained System Integrity,2022,-1,Outliers,0.328424891613204,"IoT &ML are two examples of recent developments in the management of vast amounts of data (ML). In the past, humans were responsible for all aspects of the big data lifecycle, including the collection, analysis, and acquisition of data. The IoT and Machine Learning, on the other hand, have made it possible for machines rather than people to collect vast amounts of data, analyses that data, and establish new business models. The management of big data has proven to be challenging for larger companies, despite the fact that industry 4.0 has seen tremendous success. Among these are problems with data management accuracy brought on by insufficient training, cloud computing security concerns, and blockchain security flaws. Cloud computing security concerns have also been linked to blockchain security flaws. As a consequence of this, this research has provided an explanation of how machine learning can be utilized to both bolster the safety of blockchain technology and enhance the management of large amounts of data. A primary poll was conducted with fifty individuals, and those individuals were asked a series of questions regarding big data technologies and blockchain security. The findings indicated that providing employees with training and establishing laws and regulations that are based on blockchain technology might, respectively, increase the accuracy of data management and cloud computing security.",10.1109/IC3I56241.2022.10072889,A. Kumari; H. Shah
A Comprehensive Analysis of Privacy Protection Techniques Developed for COVID-19 Pandemic,2021,2,Topic_2_data_privacy_security,0.6960395583492257,"Since the emergence of coronavirus disease–2019 (COVID-19) outbreak, every country has implemented digital solutions in the form of mobile applications, web-based frameworks, and/or integrated platforms in which huge amounts of personal data are collected for various purposes (e.g., contact tracing, suspect search, and quarantine monitoring). These systems not only collect basic data about individuals but, in most cases, very sensitive data like their movements, spatio-temporal activities, travel history, visits to churches/clubs, purchases, and social interactions. While collection and utilization of person-specific data in different contexts is essential to limiting the spread of COVID-19, it increases the chances of privacy breaches and personal data misuse. Recently, many privacy protection techniques (PPTs) have been proposed based on the person-specific data included in different data types (e.g., tables, graphs, matrixes, barcodes, and geospatial data), and epidemic containment strategies (ECSs) (contact tracing, quarantine monitoring, symptom reports, etc.) in order to minimize privacy breaches and to permit only the intended uses of such personal data. In this paper, we present an extensive review of the PPTs that have been recently proposed to address the diverse privacy requirements/concerns stemming from the COVID-19 pandemic. We describe the heterogeneous types of data collected to control this pandemic, and the corresponding PPTs, as well as the paradigm shifts in personal data handling brought on by this pandemic. We systemically map the recently proposed PPTs into various ECSs and data lifecycle phases, and present an in-depth review of existing PPTs and evaluation metrics employed for analysis of their suitability. We describe various PPTs developed during the COVID-19 period that leverage emerging technologies, such as federated learning, blockchain, privacy by design, and swarm learning, to name a few. Furthermore, we discuss the challenges of preserving individual privacy during a pandemic, the role of privacy regulations/laws, and promising future research directions. With this article, our aim is to highlight the recent PPTs that have been specifically proposed for the COVID-19 arena, and point out research gaps for future developments in this regard.",10.1109/ACCESS.2021.3130610,A. Majeed; S. O. Hwang
Supply Chain and Industry 4.0: Impact and Performance Analysis: Case of BIOMERIEUX,2019,3,Topic_3_industry_manufacturing_chain,1.0,"With the advent of the Big Data era, the industry sector has seen an increasingly intensive use of data processing, cyber-physical systems, Internet of Things and cloud technology. This digital transformation is fundamentally characterized by automation and the integration of new technologies into the enterprise value chain. From there, there are several challenges for being transformed into a connected and intelligent factory model such as the interconnection of machines, the dematerialization of communication and distribution channels, and the restructuring of the company for flexible and personalized production [P. Hébert and M. Moudallal, 2016]. Moreover, the relational model with the end customer, which is at the center of supply chain management, is thus changing, giving way to new distribution channels. The changes that are occurring for the industry sector are then called the Fourth Industrial Revolution. This article then attempts to analyze the supply chain system in the context of the new standards imposed by Industry 4.0, through a quantitative analysis conducted on the performance of industrial companies Biomérieux having upgraded its supply chain system. The indicators analyzed in this study thus include some of the most important elements of the supply chain impacted by this industrial revolution, namely; inventory management and purchasing management. The goal is to try to measure the real impact of a 4.0 upgrade of the so-called “classic” supply chain system on the performance of industrial companies.",10.1109/LOGISTIQUA.2019.8907242,N. E. Haoud; M. E. Hasnaoui
A Formal Model for Integrating Consent Management Into MLOps,2024,2,Topic_2_data_privacy_security,0.7050699533922543,"In the artificial intelligence (AI) era, data has become increasingly essential for learning and analysis. AI enables automated decision-making that may lead to violation of the General Data Protection Regulation (GDPR). The GDPR is the data protection law within the European Union (EU) that allows individuals (‘data subjects’) to control their personal data. According to the law, automated decision-making can be permitted where data subjects give explicit consent. Therefore, consent management (CM) has become an essential software component for managing data subjects’ data lifecycle and their consent. Bringing machine learning (ML) into production needs machine learning operations (MLOps). MLOps is a set of processes for delivering ML artifacts reliably and efficiently. However, current MLOps frameworks neglect the integration of CM into their processes, leading to the risk of GDPR violations. This research proposes a formal model for integrating CM into MLOps that takes upfront privacy by design (PbD). Finally, we provided a mapping from the formal model to a class diagram as guidelines to integrate CM into MLOps and demonstrated how to apply the proposed class diagram to existing ML developments, such as machine unlearning, in conjunction with the Purchase dataset.",10.1109/ACCESS.2024.3471773,N. Peyrone; D. Wichadakul
Key Safety Design Overview in AI-driven Autonomous and Battery-electric Vehicles,2024,-1,Outliers,0.22004818129378656,"With the increasing presence of autonomous SAE level 3 and level 4, which incorporate artificial intelligence software, along with the complex technical challenges they present, it is critical to maintain a strong focus on functional safety and robust software design. This paper explores the necessary safety architecture and systematic approach for automotive software and hardware, including fail-soft handling of automotive safety integrity level (ASIL) D (highest level of safety integrity), adoption of machine learning (ML), and artificial intelligence (AI) in automotive safety architecture. By addressing the unique challenges presented by increasing AI-based automotive software, we proposed various techniques, such as mitigation strategies and safety failure analysis, to enforce safety compliance and reliability of automotive software, as well as the role of AI in software reliability throughout the data lifecycle.",10.1109/AKGEC62572.2024.10868556,V. Vyas; Z. Xu
Research on Lifecycle-Driven Government Data Security Model and Data Grouping Technology,2024,2,Topic_2_data_privacy_security,0.4304378363809019,"In the context of the information age, promoting digital government and smart cities has made government data sharing a key trend. Given its special nature, securing government data requires an effective security system for safe and efficient management. This paper explores government data security and technical systems, examines China's current data management situation, and compares management strategies in China, the EU, and the US. This paper adopts a data lifecycle-driven security management approach and leverages two widely recognized frameworks to propose a system that balances data openness and security. Finally, we propose an integrated learning method based on BERT and Random Forest, use real data sets to verify the feasibility of data grouping, and promote the integration of government data management and efficient technology.",10.1109/TrustCom63139.2024.00346,S. Chen; J. Rong; Z. Fu; Q. Yue; A. Fu; X. Liu; A. Zhou; Y. Zhang
Sensor and Decision Fusion-Based Intrusion Detection and Mitigation Approach for Connected Autonomous Vehicles,2024,2,Topic_2_data_privacy_security,0.3419609295024437,"The safety of connected and autonomous vehicle (CAV) depends on the security of in-vehicle communication. The controller area network (CAN) bus holds a crucial position in ensuring in-vehicle security. Injecting attacks (e.g., increasing the speed) by hackers can affect drivers. This article proposes a fusion intrusion detection and resilient approach to maintain system performance against intrusion. The proposed system consists of two parts: sensor validation and sensor value estimation. In the sensor validation step, a new fusion approach uses three feature ranking approaches, autoencoder, and estimator-based detectors. Finally, Yager’s rules are used to handle conflict between classifiers and enrich intrusion detection accuracy. Afterward, in the second part, if any intrusion is detected, the estimated values of that sensor which is under intrusion will be replaced based on estimated values by long short-term memory-based deep regressor (LSTMDR) to avoid any performance disruption of the system. The main contribution of this study is that the proposed fusion approach uses the inherent redundancy among heterogeneous sensors to create a resilient system against compromised sensors. Using Yager’s rule and the ordered weighted average for information fusion significantly increases the reliability of intrusion detection systems and improves their detection rates. It also improves the performance of soft sensors and enhances the effectiveness of the mitigation phase. To evaluate the proposed approach, a real-world dataset entitled AEGIS—advanced big data value chain for public safety and personal security—is used. Test results indicate that the proposed fusion method is robust and reaches more accurate results compared with other detectors in three different considered attacks including replay, denial of service, and false data injection.",10.1109/JSEN.2024.3397966,M. Moradi; M. Kordestani; M. Jalali; M. Rezamand; M. Mousavi; A. Chaibakhsh; M. Saif
Depth Sequence Coding With Hierarchical Partitioning and Spatial-Domain Quantization,2020,1,Topic_1_data_big_big data,0.498498928371184,"Depth coding in 3D-HEVC deforms object shapes due to block-level edge-approximation and lacks efficient techniques to exploit the statistical redundancy, due to the frame-level clustering tendency in depth data, for higher coding gain at near-lossless quality. This paper presents a standalone mono-view depth sequence coder, which preserves edges implicitly by limiting quantization to the spatial-domain and exploits the frame-level clustering tendency efficiently with a novel binary tree-based decomposition (BTBD) technique. The BTBD can exploit the statistical redundancy in frame-level syntax, motion components, and residuals efficiently with fewer block-level prediction/coding modes and simpler context modeling for context-adaptive arithmetic coding. Compared with the depth coder in 3D-HEVC, the proposed one has achieved significantly lower bitrate at lossless to near-lossless quality range for mono-view coding and rendered superior quality synthetic views from the depth maps, compressed at the same bitrate, and the corresponding texture frames.",10.1109/TCSVT.2019.2897403,S. Shahriyar; M. Murshed; M. Ali; M. Paul
Crime Pattern Detection Utilizing Power BI Visualizations on the Microsoft Fabric Data Platform With the Public data.police.uk Dataset,2025,1,Topic_1_data_big_big data,0.5083110101733213,"This paper presents a big data-driven platform for crime pattern detection using Power BI visualizations on Microsoft Fabric, built upon the public data.police.uk dataset. Law enforcement agencies require scalable analytics to extract actionable insights from large, complex datasets. We implemented a Data Lakehouse architecture to process around 8,500 crime data files i n C SV format from multiple regions, with Python-based metadata cataloging for structured access to crime outcomes, stop-and-search records, and street-level incidents. Dataflows and Notebooks in Fabric addressed regional inconsistencies and enabled efficient data transformation. Power BI reports provided intuitive and interactive visualizations for exploring geographic and temporal crime trends. Performance testing demonstrated up to 40% faster query response times compared to traditional warehouses, and regional crime analysis that previously took days was completed within hours. The results indicated that the platform scaled efficiently while maintaining stable performance under growing data volumes. Our approach demonstrates how unified analytics and visualization environments can democratize access to crime data insights, supporting evidence-based policing and public safety decision-making.",10.1109/ACIT65614.2025.11185634,A. Todosijević; P. Dakić; T. Heričko; Ž. Kljajić; V. Todorović
Fault Recognition Technology for Pipeline Systems Based on Multi-feature Fusion of Monitoring Data,2019,-1,Outliers,0.24438490501806776,"Pipeline systems are important parts of modern infrastructure and complex industrial systems. Effective fault identification for pipeline systems is of great significance in ensuring safety and reliability. In this work, a method for pipeline fault identification based on data fusion analysis is proposed, which utilizes pipeline system condition monitoring data. First, monitoring data are analyzed to extract features that represent the operating state of the pipeline system, and a feature evaluation index based on sensitivity and volatility is proposed to optimize the extracted features. Second, multi-feature fusion analysis based on the Dempster-Shafer (DS) theory is performed to identify faults within the pipeline system. Meanwhile, to solve the problem of obtaining the basic probability assignment function (BPA) in DS theory, a BPA acquisition method is proposed, which considers both distance and correlation. Finally, this work is validated using case data of a residential heating test platform. The results show that the proposed method can directly use the monitoring data from the pipeline system for fault analysis and can effectively identify pipeline leakages, blockages, and other faults. The proposed method overcomes the shortcomings of traditional methods, which require a detailed mechanism analysis. It also conforms to emerging technology development trends, which utilize and apply Big Data analysis.",10.1109/ICPHM.2019.8819415,H. Jiang; J. Gao; F. Xia; X. Zhang; T. Zhou; D. Liu
Proactive Content Caching Scheme in Urban Vehicular Networks,2023,1,Topic_1_data_big_big data,1.0,"Stream media content caching is a key enabling technology to promote the value chain of future urban vehicular networks. Nevertheless, the high mobility of vehicles, intermittency of information transmissions, high dynamics of user requests, limited caching capacities and extreme complexity of business scenarios pose an enormous challenge to content caching and distribution in vehicular networks. To tackle this problem, this paper aims to design a novel edge-computing-enabled hierarchical cooperative caching framework. Firstly, we profoundly analyze the spatio-temporal correlation between the historical vehicle trajectory of user requests and construct the system model to predict the vehicle trajectory and content popularity, which lays a foundation for mobility-aware content caching and dispatching. Meanwhile, we probe into privacy protection strategies to realize privacy-preserved prediction model. Furthermore, based on trajectory and popular content prediction results, content caching strategy is studied, and adaptive and dynamic resource management schemes are proposed for hierarchical cooperative caching networks. Finally, simulations are provided to verify the superiority of our proposed scheme and algorithms. It shows that the proposed algorithms effectively improve the performance of the considered system in terms of hit ratio and average delay, and narrow the gap to the optimal caching scheme comparing with the traditional schemes.",10.1109/TCOMM.2023.3277530,B. Feng; C. Feng; D. Feng; Y. Wu; X. -G. Xia
Nonparametric Regression-Based Failure Rate Model for Electric Power Equipment Using Lifecycle Data,2015,0,Topic_0_prediction_degradation_rul,1.0,"In order to analyze the fault trends more accurately, a failure rate model appropriate for general electric power equipment is established based on a nonparametric regression method, improved from stratified proportional hazards model (PHM), which can make maximum use of equipment lifecycle data as the covariates, including manufacturer, service age, location, maintainer, health index, etc. All of covariates are represented in the hierarchy process of equipment health condition, which is beneficial for processing and classifying the lifecycle data into multitype recurrent events quantitatively. Meanwhile, based on new definitions of single health cycle and time between events, recurrent inspecting events distributed with martingale process can correspond with event-specific failure function during equipment lifecycle. On this occasion, more inspecting events can be utilized in a complete cycle to predict potential risk and assess equipment health condition. Then, stratified nonparametric PHM is employed to build the multitype recurrent events-specific failure model appropriate for competing risk problem toward interval censored. Lastly, the example in terms of transformers demonstrates the modeling procedure. Results show the well asymptotic property and goodness-of-fit tested by both of graphical and analytical methods. Compared with existing failure models, such as age-based or CBF model, this improved nonparametric regression model can mine lifecycle data acquisition from asset management system, depict the failure trend accurately considering both individual and group features, and lay the foundation for health prognosis, maintenance optimization, and asset management in power grid.",10.1109/TSG.2015.2388784,J. Qiu; H. Wang; D. Lin; B. He; W. Zhao; W. Xu
Personal Data Lake with Data Gravity Pull,2015,1,Topic_1_data_big_big data,0.5689409933272094,"This paper presents Personal Data Lake, a unified storage facility for storing, analyzing and querying personal data. A data lake stores data regardless of format and thus provides an intuitive way to store personal data fragments of any type. Metadata management is a central part of the lake architecture. For structured/semi-structured data fragments, metadata may contain information about the schema of the data so that the data can be transformed into queryable data objects when required. For unstructured data, enabling gravity pull means allowing third-party plugins so that the unstructured data can be analyzed and queried.",10.1109/BDCloud.2015.62,C. Walker; H. Alrehamy
Data Science Professional Uncovered: How the EDISON Project will Contribute to a Widely Accepted Profile for Data Scientists,2015,-1,Outliers,0.14488285900436587,"The digital revolution made available vast amounts of data both in industry and in the research landscape. The ability to manipulate and extract knowledge and value from this data represents a new profession called the Data Scientist: expected to be the most visible job in future years. The EDISON project has been established in order to support universities, research centers, industry and research infrastructure organisations to cope with the potential shortfall of Data Scientists, to define the framework of competences as well as the body of knowledge for this profession. In this paper the EDISON team describes how it intends to nurture the profession of Data Scientist to cope with the expected increase in demand. The strategy proposed is based on both the analysis of the demand side (industries, research centers and research infrastructure organisations) and the supply side (Universities and training centers) bridging between the providers and employers by cooperating on the establishment of a Competence Framework and a Body of Knowledge for the Data Scientist Professional. The project will exploit piloting initiatives in cooperation with pioneer universities and also involve external experts as evangelists.",10.1109/CloudCom.2015.57,A. Manieri; S. Brewer; R. Riestra; Y. Demchenko; M. Hemmje; T. Wiktorski; T. Ferrari; J. Frey
Acoustic based appliance state identifications for fine-grained energy analytics,2015,-1,Outliers,0.19614654413801344,"Fine-grained monitoring of everyday appliances can provide better feedback to the consumers and motivate them to change behavior in order to reduce their energy usage. It also helps to detect abnormal power consumption events, long-term appliance malfunctions and potential safety concerns. Commercially available plug meters can be used for individual appliance monitoring but for an entire house, each such individual plug meters are expensive and tedious to setup. Alternative methods relying on Non-Intrusive Load Monitoring techniques help disaggregate electricity consumption data and learn about the individual appliance's power states and signatures. However fine-grained events (e.g., appliance malfunctions, abnormal power consumption, etc.) remain undetected and thus inferred contexts (such as safety hazards etc.) become invisible. In this work, we correlate an appliance's inherent acoustic noise with its energy consumption pattern individually and in presence of multiple appliances. We initially investigate classification techniques to establish the relationship between appliance power and acoustic states for efficient energy disaggregation and abnormal events detection. While promising, this approach fails when there are multiple appliances simultaneously in `ON' state. To further improve the accuracy of our energy disaggregation algorithm, we propose a probabilistic graphical model, based on a variation of Factorial Hidden Markov Model (FHMM) for multiple appliances energy disaggregation. We combine our probabilistic model with the appliances acoustic analytics and postulate a hybrid model for energy disaggregation. Our approach helps to improve the performance of energy disaggregation algorithms and provide critical insights on appliance longevity, abnormal power consumption, consumer behavior and their everyday lifestyle activities. We evaluate the performance of our proposed algorithms on real data traces and show that the fusion of acoustic and power signatures can successfully detect a number of appliances with 95% accuracy.",10.1109/PERCOM.2015.7146510,N. Pathak; M. A. Al Hafiz Khan; N. Roy
Towards a UML Profile for Privacy-Aware Applications,2015,2,Topic_2_data_privacy_security,0.8885210604799394,"Personal information is continuously gathered and processed by modern web applications. Due to regulation laws and to protect the privacy of users, customers, and business partners, such information must be kept private. A recurring problem in constructing web applications and services that protect privacy is the insufficient resources for documenting them. As web applications must be developed consistently with the statements of the privacy policy in order to enforce them, a structured documentation is necessary to model privacy protection during application design. To contribute with solutions to this problem, in this paper we propose a UML profile for privacy-aware applications. This profile helps building UML models that specify and structure particular concepts of privacy and, consequently, improve privacy definition and enforcement. After introducing the main privacy concepts, we describe how they are represented in the UML language. The profile's ability to model statements of realistic privacy policies is then demonstrated on a case study.",10.1109/CIT/IUCC/DASC/PICOM.2015.53,T. Basso; L. Montecchi; R. Moraes; M. Jino; A. Bondavalli
Managing Complexity in Distributed Data Life Cycles Enhancing Scientific Discovery,2015,1,Topic_1_data_big_big data,0.6859268289359687,"Distributed data life cycles consist of data sources, data and computing components as well as data sinks and user facing elements. The complexity of the underlying systems is ever rising with the increasing heterogeneity and distribution of components and environments. Researchers would like to focus on their specific research topic without the need to learn these systems in detail. Accessible data life cycles enable scientists to do better science more efficiently and obtain results, which would not have been possible without these advanced technologies. For this objective, abstraction to hide complexity and automation to avoid manual tasks are a necessity. These are embodied in the three conceptual data life cycle challenges, namely data, computing and utilization. Concepts and technologies to manage these challenges are explored and exemplified on the basis of the general data life cycle and MoSGrid (Molecular Simulation Grid) science gateway. In this context, we especially focus on teaching in drug design and quantum chemistry research use cases. Further cases are presented elucidating various challenges in adapting the concepts and technologies to wind energy data analysis and the XSEDE research infrastructure.",10.1109/eScience.2015.72,R. Grunzke; A. Aguilera; W. E. Nagel; J. Krüger; S. Herres-Pawlis; A. Hoffmann; S. Gesing
Data Science and Online Education,2015,-1,Outliers,0.15091897217201602,"We discuss the Data Science program at Indiana University, which is offered in both traditional residential and online formats. We describe Data Science, our chosen curriculum and its motivation. We describe experience in online delivery for both traditional lectures and online programming laboratories, and discuss implications for the technology used.",10.1109/CloudCom.2015.82,G. Fox; S. Maini; H. Rosenbaum; D. Wild
Open Information Linking for Environmental Research Infrastructures,2015,1,Topic_1_data_big_big data,0.4573931759550624,"Environmental research infrastructures (RIs) support data-intensive research by integrating large-scale sensor/observer networks with dedicated data curation services and analytical tools. However the diversity of scientific disciplines coupled with the lack of an accepted methodology for constructing new RIs inevitably leads to incompatibilities between the data models, metadata standards and service descriptions used by different RIs, inhibiting their usefulness for interdisciplinary research. In the absence of a common global ontology of science and infrastructure, these inconsistencies may best be counteracted by selectively bridging the semantics of the various vocabularies, standards and models used by the RIs at present. Open Information Linking for Environmental RIs (OIL-E) was developed within the FP7 project ENVRI to provide a framework for semantic linking of knowledge resources used by different environmental RIs. Built around a multi-viewpoint reference model ENVRI-RM, OIL-E is intended to act as a central exchange for linking information fragments and identifying gaps in the conceptual models of RIs.",10.1109/eScience.2015.66,P. Martin; P. Grosso; B. Magagna; H. Schentz; Y. Chen; A. Hardisty; W. Los; K. Jeffery; C. de Laat; Z. Zhao
Towards an Industry Data Gateway: An Integrated Platform for the Analysis of Wind Turbine Data,2015,2,Topic_2_data_privacy_security,0.2103626729745792,"The increasing amount of data produced in many scientific and engineering domains creates as many new challenges for an efficient data analysis, as possibilities for its application. In this paper, we present one of the use-cases of the project VAVID, namely the condition monitoring of sensor information from wind turbines, and how a data gateway can help to increase the usability and security of the proposed system. Starting by briefly introducing the project, the paper presents the problem of handling and processing large amount of sensor data using existing tools in the context of wind turbines. It goes on to describe the innovative approach used in VAVID to meet this challenge, covering the main goals, numerical methods used for analysis, the storage concept, and architectural design. It concludes by offering a rational for the use of a data gateway as the main entry point to the system and how this is being implemented in VAVID.",10.1109/IWSG.2015.8,A. Aguilera; R. Grunzke; U. Markwardt; D. Habich; D. Schollbach; J. Garcke
"Security, privacy and trust in cloud computing: A comparative study",2015,2,Topic_2_data_privacy_security,0.758587497444854,"""Use the computer without a computer,"" this is the dream of all managers who believe in ""Cloud Computing"". The Cloud has become a dream and an obsession for all fans of the computer because of the many benefits this service offers: availability of services and data is ensured, cost is relative to consumption, ease of deployment, technical infrastructure is adaptable to the volume of business activity, its convenience to the common business applications (CRM, HR, BI, ERP, mail, etc.), the fact that this service provides a business function and not the technical components requiring computer skills…, Besides these advantages, there are serious risks related to the use of Cloud computing, such as: temporary or permanent loss of data, security of data, lack of traceability and accountability… These risks are the main challenges faced while adopting a Cloud computing architecture., In this paper, we studied the literature focusing on three major notions on collaborative systems in Cloud computing: Security, privacy and trust. That's why we will try to bring out the main requirements regarding these three concepts from both points of view (user's and provider's), before presenting some related approaches which treat these three concepts.",10.1109/CloudTech.2015.7336995,M. Alouane; H. El Bakkali
Towards an Environment for Doing Data Science That Runs in Browsers,2015,1,Topic_1_data_big_big data,1.0,"This article proposes a path for doing Data Science using browsers as computing and data nodes. This novel idea is motivated by the cross-fertilized fields of desktop grid computing, data management in grids and clouds, Web technologies such as NoSQL tools, models of interactions and programming models in grids, cloud and Web technologies. We propose a methodology for the modeling, analyzing, implementation and simulation of a prototype able to run a MapReduce job in browsers. This work allows to better understand how to envision the big picture of Data Science in the context of the Javascript language for programming the middleware, the interactions between components and browsers as the operating system. We explain what types of applications may be impacted by this novel approach and, from a general point of view how a formal modeling of the interactions serves as a general guidelines for the implementation. Formal modeling in our methodology is a necessary condition but it is not sufficient. We also make round-trips between the modeling and the Javascript or used tools to enrich the interaction model that is the key point, or to put more details into the implementation. It is the first time to the best of our knowledge that Data Science is operating in the context of browsers that exchange codes and data for solving computational and data intensive programs. Computational and data intensive terms should be understand according to the context of applications that we think to be suitable for our system.",10.1109/SmartCity.2015.145,L. Abidi; C. Cérin; G. Fedak; H. He
A Guide to End-to-End Privacy Accountability,2015,2,Topic_2_data_privacy_security,0.8796448361512742,"Accountability is considered a tenet of privacy management, yet implementing it effectively is no easy task. It requires a systematic approach with an overarching impact on the design and operation of IT systems. This article, which results from a multidisciplinary project involving lawyers, industry players and computer scientists, presents guidelines for the implementation of consistent sets of accountability measures in organisations. It is based on a systematic analysis of the Draft General Data Protection Regulation. We follow a systematic approach covering the whole life cycle of personal data and considering the three levels of privacy proposed by Bennett, namely accountability of policy, accountability of procedures and accountability of practice.",10.1109/TELERISE.2015.12,D. Butin; D. Le Métayer
Multilevel Modeling for Business Process Automation,2015,3,Topic_3_industry_manufacturing_chain,0.6393719534873805,"Many real-world situations are more easily represented using multilevel models that abandon traditional two level instantiation. In business process management, multilevel modeling allows for the explicit representation of the interdependencies between the processes at the different hierarchy levels within an organization. Furthermore, multilevel modeling allows modelers to capture the variability of processes. The multilevel business artifact (MBA) is a conceptual-modeling primitive for the artifact-centric representation of business processes at multiple levels of abstraction. The (semi-)automated execution of the thus modeled business processes requires a suitable logical representation format for MBAs as well as an execution engine. In this paper, we propose a logical representation for MBAs based on State Chart XML as well as an execution engine based on XQuery.",10.1109/EDOCW.2015.30,C. G. Schuetz; B. Neumayr; M. Schrefl
Towards Efficient and Secure Data Storage in Multi-tenant Cloud-Based CRM Solutions,2015,2,Topic_2_data_privacy_security,1.0,"Even though enterprises increasingly recognize the benefits of cloud computing, many are still reluctant using cloud-based applications or services like customer relationship management (CRM) solutions due to security and privacy concerns. This article aims at defining a roadmap to derive a holistic framework providing data privacy and security by design in the context of cloud-based multi-tenant CRM systems. As a CRM system developed for SMEs CAS PIA serves as an example for typically occurring data structures and use cases including the innovative concept of user-defined security levels for different data types. We present a scenario and requirements analysis for motivating the need for a suitable user-context-specific security concept and a data and privacy preserving framework.",10.1109/UCC.2015.107,J. Vuong; S. Braun
Regulation and standardization of data protection in cloud computing,2015,2,Topic_2_data_privacy_security,0.3989774076062092,"Standards are often considered as an alternative form of regulation to legislative rule setting. However, standards also complement legislative acts, supporting their effective implementation and providing precise definitions for sometimes vague legal concepts. As we demonstrate, standards are not mere technical regulations but relate to sensitive political issues. The genesis and contents of ISO/IEC 27018 illustrate the interaction between both forms of regulation in the case of data protection in cloud computing. While the standard has been written with intensive consideration of the legal framework, we argue that the standard could reciprocally influence legal rule-making in the same domain.",10.1109/Kaleidoscope.2015.7383634,M. G. Löhe; K. Blind
Big graph-based Data visualization experiences: The WordNet case study,2015,-1,Outliers,0.43014978472917004,"In the Big Data era, the visualization of large data sets is becoming an increasingly relevant task due to the great impact that data have from a human perspective. Since visualization is the closer phase to the users within the data life cycle's phases, there is no doubt that an effective, efficient and impressive representation of the analyzed data may result as important as the analytic process itself. This paper presents an experience for importing, querying and visualizing graph database and in particular, we describe as a case study the WordNet database using Neo4J and Cytoscape. We will describe each step in this study focusing on the used strategies for overcoming the different problems mainly due to the intricate nature of the case study. Finally, an attempt to define some criteria to simplify the large-scale visualization of WordNet will be made, providing some examples and considerations which have arisen.",,E. G. Caldarola; A. Picariello; A. M. Rinaldi
Coalition Based Bandwidth Allocation in Mobile Social Networks,2015,1,Topic_1_data_big_big data,1.0,"In Mobile Social Networks (MSN), users can subscribe to online services and receive service data through base stations in addition to exchanging data directly. When multiple users in the same cell subscribe to the same service, the base station needs to forward the service data to all these users individually, leading to low utilization of the wireless resource. In order to improve the bandwidth efficiency, we propose an original Coalition based Bandwidth Allocation (CBA) scheme so that users can share bandwidth in the process of data receiving. In CBA, each user can receive data directly from the base station with an independent bandwidth, or join a coalition and share bandwidth with the coalition users. Towards efficient coalition formation, distributed computing is adopted to calculate the profit of each user by considering both data rate and data receiving delay. Based on the achievable profit, each user can rationally decide whether to join/quit a coalition. Then, coalitions can be formed in a distributed way and bandwidth is allocated based on coalition formation. Simulation results show that CBA can significantly improve the bandwidth efficiency of the network at the cost of controlled data receiving delay.",10.1109/CIT/IUCC/DASC/PICOM.2015.328,B. Fan; S. Leng; K. Yang; G. Min
D3-MapReduce: Towards MapReduce for Distributed and Dynamic Data Sets,2015,1,Topic_1_data_big_big data,1.0,"Since its introduction in 2004 by Google, MapReduce has become the programming model of choice for processing large data sets. Although MapReduce was originally developed for use by web enterprises in large data-centers, this technique has gained a lot of attention from the scientific community for its applicability in large parallel data analysis (including geographic, high energy physics, genomics, etc.). So far MapReduce has been mostly designed for batch processing of bulk data. The ambition of D3-MapReduce is to extend the MapReduce programming model and propose efficient implementation of this model to: i) cope with distributed data sets, i.e. that span over multiple distributed infrastructures or stored on network of loosely connected devices, ii) cope with dynamic data sets, i.e. which dynamically change over time or can be either incomplete or partially available. In this paper, we draw the path towards this ambitious goal. Our approach leverages Data Life Cycle as a key concept to provide MapReduce for distributed and dynamic data sets on heterogeneous and distributed infrastructures. We first report on our attempts at implementing the MapReduce programming model for Hybrid Distributed Computing Infrastructures (Hybrid DCIs). We present the architecture of the prototype based on BitDew, a middleware for large scale data management, and Active Data, a programming model for data life cycle management. Second, we outline the challenges in term of methodology and present our approaches based on simulation and emulation on the Grid'5000 experimental testbed. We conduct performance evaluations and compare our prototype with Hadoop, the industry reference MapReduce implementation. We present our work in progress on dynamic data sets that has lead us to implement an incremental MapReduce framework. Finally, we discuss our achievements and outline the challenges that remain to be addressed before obtaining a complete D3-MapReduce environment.",10.1109/SmartCity.2015.141,H. He; A. Simonet; J. A. J. -F. Saray; G. Fedak; B. Tang; L. Lu; X. Shi; H. Jin; M. Moca; G. C. Silaghi; A. B. Cheikh; H. Abbes
Survey of Security Advances in Smart Grid: A Data Driven Approach,2017,2,Topic_2_data_privacy_security,0.23189389614382042,"With the integration of advanced computing and communication technologies, smart grid is considered as the next-generation power system, which promises self healing, resilience, sustainability, and efficiency to the energy critical infrastructure. The smart grid innovation brings enormous challenges and initiatives across both industry and academia, in which the security issue emerges to be a critical concern. In this paper, we present a survey of recent security advances in smart grid, by a data driven approach. Compared with existing related works, our survey is centered around the security vulnerabilities and solutions within the entire lifecycle of smart grid data, which are systematically decomposed into four sequential stages: 1) data generation; 2) data acquisition; 3) data storage; and 4) data processing. Moreover, we further review the security analytics in smart grid, which employs data analytics to ensure smart grid security. Finally, an effort to shed light on potential future research concludes this paper.",10.1109/COMST.2016.2616442,S. Tan; D. De; W. -Z. Song; J. Yang; S. K. Das
EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,2016,-1,Outliers,0.14900097454279468,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",10.1109/CloudCom.2016.0107,Y. Demchenko; A. Belloum; W. Los; T. Wiktorski; A. Manieri; H. Brocks; J. Becker; D. Heutelbeck; M. Hemmje; S. Brewer
Value Creation on Open Government Data,2016,1,Topic_1_data_big_big data,0.47666910776306226,"Governments are one of the largest producers and collectors of data in many different domains. As one major aim of open government data initiatives is the release of social and commercial value, we here explore existing processes of value creation on government data. We identify the dimensions that impact, or are impacted by value creation, and distinguish between the different value creating roles and participating stakeholders. We propose the use of Linked Data as an approach to enhance the value creation process, and provide a Value Creation Assessment Framework to analyse the resulting impact.",10.1109/HICSS.2016.326,J. Attard; F. Orlandi; S. Auer
Crossing analytics systems: A case for integrated provenance in data lakes,2016,1,Topic_1_data_big_big data,0.6612042964273567,"The volumes of data in Big Data, their variety and unstructured nature, have had researchers looking beyond the data warehouse. The data warehouse, among other features, requires mapping data to a schema upon ingest, an approach seen as inflexible for the massive variety of Big Data. The Data Lake is emerging as an alternate solution for storing data of widely divergent types and scales. Designed for high flexibility, the Data Lake follows a schema-on-read philosophy and data transformations are assumed to be performed within the Data Lake. During its lifecycle in a Data Lake, a data product may undergo numerous transformations performed by any number of Big Data processing engines leading to questions of traceability. In this paper we argue that provenance contributes to easier data management and traceability within a Data Lake infrastructure. We discuss the challenges in provenance integration in a Data Lake and propose a reference architecture to overcome the challenges. We evaluate our architecture through a prototype implementation built using our distributed provenance collection tools.",10.1109/eScience.2016.7870919,I. Suriarachchi; B. Plale
Spatio-temporal interpolation methods for solar events metadata,2016,-1,Outliers,0.34759292729660174,"This paper introduces three interpolation methods that enrich complex evolving region trajectories that are captured every day from numerous ground-based and space-based solar observatories. The interpolation module takes a trajectory as its input and generates an enriched trajectory with interpolated time-geometry pairs. we created three different interpolation techniques that are: MBR-Interpolation (Minimum Bounding Rectangle Interpolation), CP-Interpolation (Complex Polygon Interpolation), and FP-Interpolation (Filament Polygon Interpolation). The methods combine K-means clustering algorithm, shape signature representation, and linear interpolation to generate the missing polygons. This is the first research of this kind that attempts to address the problem of solar big data interpolation. Finally, we outline future improvements and opportunities for solar data interpolation.",10.1109/BigData.2016.7840970,S. F. Boubrahimi; B. Aydin; D. Kempton; R. Angryk
Addressing exploitability of Smart City data,2016,1,Topic_1_data_big_big data,0.7514544523028005,"Central to a number of emerging Smart Cities are online platforms for data sharing and reuse: Data Hubs and Data Catalogues. These systems support the use of data by developers through enabling data discoverability and access. As such, the effectiveness of a Data Catalogue can be seen as the way in which it supports `data exploitability': the ability to assess whether the provided data is appropriate to the given task. Beyond technical compatibility, this also regards validating the policies attached to data. Here, we present a methodology to enable Smart City Data Hubs to better address exploitability by considering the way policies propagate across the data flows applied in the system.",10.1109/ISC2.2016.7580764,E. Daga; M. d'Aquin; A. Adamou; E. Motta
"Management, analysis, and visualization of experimental and observational data — The convergence of data and computing",2016,1,Topic_1_data_big_big data,0.6764117410926328,"Scientific user facilities — particle accelerators, telescopes, colliders, supercomputers, light sources, sequencing facilities, and more — operated by the U.S. Department of Energy (DOE) Office of Science (SC) generate ever increasing volumes of data at unprecedented rates from experiments, observations, and simulations. At the same time there is a growing community of experimentalists that require real-time data analysis feedback, to enable them to steer their complex experimental instruments to optimized scientific outcomes and new discoveries. Recent efforts in DOE-SC have focused on articulating the data-centric challenges and opportunities facing these science communities. Key challenges include difficulties coping with data size, rate, and complexity in the context of both real-time and post-experiment data analysis and interpretation. Solutions will require algorithmic and mathematical advances, as well as hardware and software infrastructures that adequately support data-intensive scientific workloads. This paper presents the summary findings of a workshop held by DOE-SC in September 2015, convened to identify the major challenges and the research that is needed to meet those challenges.",10.1109/eScience.2016.7870902,E. W. Bethel; M. Greenwald; K. K. van Dam; M. Parashar; S. M. Wild; H. S. Wiley
"Information Modelling and Semantic Linking for a Software Workbench for Interactive, Time Critical and Self-Adaptive Cloud Applications",2016,1,Topic_1_data_big_big data,0.7880803404894183,"Cloud environments can provide elastic, controllable on-demand services for supporting complex distributed applications. However the engineering methods and software tools used for developing, deploying and executing classical time-critical applications do not, as yet, account for the programmability and controllability that can be provided by clouds, and so time-critical applications do not yet benefit from the full potential of virtualisation technologies. A software workbench for developing, deploying and controlling time-critical applications in cloud environments can address this, but needs to be able to interoperate with existing cloud standards and services in a fashion that can still adapt to the continuing evolution of the field. Semantic linking can enhance interoperability by creating mappings between different vocabularies and specifications, allowing different technologies to be plugged together, which can then be used to buld such a workbench in a flexible manner. A semantic linking framework is presented that uses a multiple-viewpoint model of a cloud application workbench as a means to relate different cloud and quality of service standards in order to aid the development of time-critical applications. The foundations of such a model, developed as part of the H2020 project SWITCH, are also presented.",10.1109/WAINA.2016.38,P. Martin; A. Taal; F. Quevedo; D. Rogers; K. Evans; A. Jones; V. Stankovski; S. Taherizadeh; J. Trnkoczy; G. Suciu; Z. Zhao
UVisP: User-centric Visualization of Data Provenance with Gestalt Principles,2016,1,Topic_1_data_big_big data,0.4640173521428603,"The need to understand and track files (and inherently, data) in cloud computing systems is in high demand. Over the past years, the use of logs and data representation using graphs have become the main method for tracking and relating information to the cloud users. While being used, tracking related information with 'data provenance' (i.e. series of chronicles and the derivation history of data on metadata) is the new trend for cloud users. However, there is still much room for improving data activity representation in cloud systems for end-users. We propose ""User-centric Visualization of data provenance with Gestalt (UVisP)"", a novel user-centric visualization technique for data provenance. This technique aims to facilitate the missing link between data movements in cloud computing environments and the end-users uncertain queries over their files security and life cycle within cloud systems. The proof of concept for the UVisP technique integrates an open-source visualization API with Gestalt's theory of perception to provide a range of user-centric provenance visualizations. UVisP allows users to transform and visualize provenance (logs) with implicit prior knowledge of 'Gestalt's theory of perception.' We presented the initial development of the UVisP technique and our results show that the integration of Gestalt and 'perceptual key(s)' in provenance visualization allows end-users to enhance their visualizing capabilities, to extract useful knowledge and understand the visualizations better.",10.1109/TrustCom.2016.0294,J. Garae; R. K. L. Ko; S. Chaisiri
Data safety: Concept and analysis method review,2016,2,Topic_2_data_privacy_security,0.42698161596275425,"Along with the increasing trend of data-centric view in safety-related systems and the popularity in use of data-driven systems, data safety has become a significant part of system safety. This paper gives a detailed definition of intrinsic nature of data, based on which this paper reviews the concept of data safety, its related literatures, standards, process framework and several data safety analysis approaches. Finally, conclusion on data safety review is drawn and its developing trend illustrated.",10.1109/ICIST.2016.7483409,P. Cong; L. Minyan; L. Luyi
Trends and challenges in smart healthcare research: A journey from data to wisdom,2017,-1,Outliers,0.34844690893744046,"Smart Healthcare is a relatively new context-aware healthcare paradigm influenced by several fields of knowledge, namely medical informatics, communications and electronics, bioengineering, ethics and so on. Thus, many challenging problems are related to smart healthcare but in many cases they are explored individually in their respective fields and, as a result, they are not always known by the smart healthcare research community working in more specific domains. The aim of this article is to identify some of the most relevant trends and research lines that are going to affect the smart healthcare field in the years to come. To do so, the article considers a systematic approach that classifies the identified research trends and problems according to their appearance within the data life cycle, this is, from the data gathering in the physical layer (lowest level) until their final use in the application layer (highest level). By identifying and classifying those research trends and challenges, we help to pose questions that the smart healthcare community will need to address. Consequently, we set a common ground to explore important problems in the field, which will have significant impact in the years to come.",10.1109/RTSI.2017.8065986,A. Solanas; F. Casino; E. Batista; R. Rallo
Ripple: Home Automation for Research Data Management,2017,1,Topic_1_data_big_big data,0.6776983621047683,"Exploding data volumes and acquisition rates, plus ever more complex research processes, place significant strain on research data management processes. It is increasingly common for data to flow through pipelines comprised of dozens of different management, organization, and analysis steps distributed across multiple institutions and storage systems. To alleviate the resulting complexity, we propose a home automation approach to managing data throughout its lifecycle, in which users specify via high-level rules the actions that should be performed on data at different times and locations. To this end, we have developed Ripple, a responsive storage architecture that allows users to express data management tasks via a rules notation. Ripple monitors storage systems for events, evaluates rules, and uses serverless computing techniques to execute actions in response to these events. We evaluate our solution by applying Ripple to the data lifecycles of two real-world projects, in astronomy and light source science, and show that it can automate many mundane and cumbersome data management processes.",10.1109/ICDCSW.2017.30,R. Chard; K. Chard; J. Alt; D. Y. Parkinson; S. Tuecke; I. Foster
Understanding data quality: Ensuring data quality by design in the rail industry,2017,1,Topic_1_data_big_big data,0.5186295977242743,"The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.",10.1109/BigData.2017.8258380,Q. Fu; J. M. Easton
Non-Sequential Striping for Distributed Storage Systems with Different Redundancy Schemes,2017,1,Topic_1_data_big_big data,0.7837698889759546,"Modern distributed storage systems often store redundant data in multiple replications or erasure coding according to their access frequencies. Multiple replications scheme is well-performance for hot data while erasure coding scheme is storage-efficient for warm and cold data. When hot data turn cold, an encoding procedure starts to do the conversion. However, due to sequential striping, current conversion methods do not perform well for different data layouts, and cause risky blocks and expensive network consumption.In this paper, we propose Sice, a new encoder which deploys non-sequential striping. It constructs non-sequential stripes according to the data layout, performs conversion quickly with low overheads and ends to no reduction of system reliability. The results of both simulation and evaluation show that Sice gains almost the same good performance for different data layouts and has a great scalability. Sice helps HDFS-RAID reduce network consumption by about 65% and reduce influence on concurrent I/O-intensive applications by about 60%.",10.1109/ICPP.2017.32,Y. Xie; D. Feng; F. Wang
"Cloud computing privacy issues, challenges and solutions",2017,2,Topic_2_data_privacy_security,0.9496152201946683,"There are many cloud computing initiatives that represent a lot of benefit to enterprise customers. However, there are a lot of challenges and concerns regarding the security and the privacy of the customer data that is hosted on the cloud. We explore in this paper the various aspects of cloud computing regarding data life cycle and its security and privacy challenges along with the devised methodology to address those challenges. We mention some of the regulations and law requirements in place to ensure cloud customer data privacy.",10.1109/ICCES.2017.8275295,A. M. El-Zoghby; M. A. Azer
"Preparing data managers to support open ocean science: Required competencies, assessed gaps, and the role of experiential learning",2017,-1,Outliers,0.285139146077124,"Ocean science is experiencing an explosion of data as researchers employ a widening variety of sensors, operating at higher fidelity and frequency, to inform our understanding of the global ocean. This is further complicated by the increasing integration of open science data from other disciplines to analyze complex systems, like climate change, animal migration, and sea/air interaction. This shift has been unplanned, chaotic, and emergent, and has placed the onus on researchers to stay current with best practices for managing, analyzing, and sharing data. Ocean scientists who do not have the technical skill to manage this data are turning to technologists, on the assumption they have the expertise required to help. To test this assumption, we examined an experiential learning program that placed technologists at ocean data centres in Canada, conducting interviews with students and employers to identify the competencies they believed were required to manage ocean data, which were missing in students' education up to that point, and which students gained during the work term placement.",10.1109/BigData.2017.8258412,L. Wilson; A. Colborne; M. Smit
Improvement of airworthiness certification audits of software-centric avionics systems using a cross-discipline application lifecycle management system methodology,2017,-1,Outliers,0.127767258782459,"The use of an Application Lifecycle Management (ALM) system to promote cross-discipline data capture, tracking and traceability has shown to provide a dramatic improvement during airworthiness audits of software-centric avionics systems by reducing both the time to locate relevant information and rework associated with errors in captured data and their traceability. When configured correctly, an ALM system enables the capture of all data associated with the development of a software-centric avionics system from customer specifications through system-level requirements and design data to the lowest level of data including software source code and system and software test results. The comprehensive nature of the data capture and the integrated traceability facilitates quick and efficient retrieval of pertinent data when performing airworthiness audits and is significantly superior to more manual methods and methods that rely upon disparate tools and data sources. This paper will describe a data model that allows for the capture and tracing of cross-discipline data in an ALM system and will present data to exhibit improvements over the use of legacy systems and methods. Analysis at UTC Aerospace Systems has shown that a significant reduction in preparation and audit time, which includes activities such as creating a document and records catalog, completing a pre-audit review and summarizing traceability architectures before an audit; and locating documents and records, assessing traceability across disciplines, itemizing open problem reports and determining the current status of development and verification activities during an audit. A summary of these improvements is shown in Table 1. Organization of the data in the ALM system is a critical factor in achieving reduced search times and preventing rework associated with traceability and lost or missing records. A data model diagram is established to allow for simpler organization of the life cycle data to support product development and verification, which is used by all disciplines including Systems Engineering, Software Engineering, Systems Test Engineering, Software Test Engineering, and associated disciplines including Configuration Management, Project Management and Quality Assurance.",10.1109/SysEng.2017.8088264,K. Roseberry; T. Scott-Parry
Challenges of Translating HPC Codes to Workflows for Heterogeneous and Dynamic Environments,2017,1,Topic_1_data_big_big data,1.0,"In this paper we would like to share our experience for transforming a parallel code for a Computational Fluid Dynamics (CFD) problem into a parallel version for the RedisDG workflow engine. This system is able to capture heterogeneous and highly dynamic environments, thanks to opportunistic scheduling strategies. We show how to move to the field of “HPC as a Service” in order to use heterogeneous platforms. We mainly explain, through the CFD use case, how to transform the parallel code and we exhibit challenges to 'unfold' the task graph dynamically in order to improve the overall performance (in a broad sense) of the workflow engine. We discuss in particular of the impact on the workflow engine of such dynamic feature. This paper states that new models for High Performance Computing are possible, under the condition we revisit our mind in the direction of the potential of new paradigms such as cloud, edge computings...",10.1109/HPCS.2017.130,F. Benkhaldoun; C. Cérin; I. Kissami; W. Saad
A Study on Library Service Innovation Based on Data Curation,2017,1,Topic_1_data_big_big data,0.4528946456228449,"This paper concisely introduces data curation and service innovation, analyzes the motivation and innovation of libraries to provide service based on data curation, and builds a data curation-based library innovative service system from the perspective of individual library institution, in view of biological organisms and in combination with theories such as Data Curation Lifecycle Model and Four Dimensional Model of Service Innovation.",10.1109/SKG.2017.00044,J. Zhang; F. Zhao
Cyberinfrastructure to support data management,2017,1,Topic_1_data_big_big data,0.43544208754986025,"Management of oceanographic data is particularly challenging due to the variety of protocols for data collection and analysis and the vast range of oceanographic variables studied. This paper describes the end-to-end cyber infrastructure developed to support stakeholders in the ocean science community throughout the data life cycle: from immediately after data collection through numerical analysis and synthesis, visualization, and decision making, to data publication and reuse. Our intent is to provide an overview of the system architecture and descriptions of system components.",,R. Bochenek; C. Turner
"xDCI, a data science cyberinfrastructure for interdisciplinary research",2017,1,Topic_1_data_big_big data,0.6219020215670972,"This paper introduces xDCI, a Data Science Cyber-infrastructure to support research in a number of scientific domains including genomics, environmental science, biomedical and health science, and social science. xDCI leverages open-source software packages such as the integrated Rule Oriented Data System and the CyVerse Discovery Environment to address significant challenges in data storage, sharing, analysis and visualization. We provide three example applications to evaluate xDCI for different domains: analysis of 3D images of mice brains, videos analysis of neonatal resuscitation, and risk analytics. Finally, we conclude with a discussion of potential improvements to xDCI.",10.1109/HPEC.2017.8091022,A. Krishnamurthy; K. Bradford; C. Calloway; C. Castillo; M. Conway; J. Coposky; Y. Guo; R. Idaszak; W. C. Lenhardt; K. Robasky; T. Russell; E. Scott; M. Sliwowski; M. Stealey; K. Urgo; H. Xu; H. Yi; S. Ahalt
"Decentralized Services Computing Paradigm for Blockchain-Based Data Governance: Programmability, Interoperability, and Intelligence",2020,-1,Outliers,0.31426113842838005,"With the explosion of “big data” in the past decade, exploring and mining the value hidden in the data has already generated a lot of innovative applications, especially the recent advances of AI applications. The data governance, including activities of data creation, sharing, exchange, management, analytics, tracing, and accounting, has drawn a lot of attentions. Services computing establishes the foundation of current data governance, typically in a centralized fashion, e.g., the cloud-based storage services and analytic services. However, the potential values of big data distributed on the Internet are far away from being adequately explored. Considering the infrastructure revolution made by the blockchain, in this position article, we try to rethink a new data governance fashion that is built upon the blockchain-based decentralized services computing paradigm. The core principle is that data owners are able to publish their data as a set of services that can be deployed independently from the application systems where the data were born. Meanwhile, data owners can define service rules/policies where their data should be stored and how the data can be shared, and keep governing the whole lifecycle record of how their data are actually used. Similar to existing services computing paradigm, data users can search, discover, integrate, and analyze the data in a decentralized fashion. With this perspective, we try to discuss some key insights and enumerate several related new technologies and open challenges, in terms of programmability, interoperability, and intelligence.",10.1109/TSC.2019.2951558,X. Liu; S. X. Sun; G. Huang
Data Preservation through Fog-to-Cloud (F2C) Data Management in Smart Cities,2018,1,Topic_1_data_big_big data,1.0,"A vast amount of data is constantly being produced in the world from several sources, including the IoT at smart cities. Such complex universe of digital data is being set up through these daily accumulated fresh data over other historical repositories. This data can then be used in different forms and then be reused in further processes, hence, drawing the life cycle of data. Conventionally, the smart city resources management are based on cloud models where sensors data are collected to prepare a centralized and rich set of open data. Cloud-based frameworks provide several benefits, consisting of their ubiquity and (almost) unlimited resources capacity. But accessing data from the cloud results in large network traffics, high latencies and are not recommended for real-time solutions. In order to solve this, fog computing has been defined as a promising technology. It uses devices at the edge to perform closer computation and, therefore, reduces network traffic, decreases latencies, while enhancing security levels. In this paper we present the Data Preservation block, as part of our hierarchical fog-to-cloud F2C data management architecture. This model has the advantages of both fog and cloud technologies, as it allows reduced latencies for critical applications while being able to use the high computing capabilities of cloud technology. Furthermore, we have estimated the amount of collected data to be stored in the storage media, from fog to cloud layers, at the Barcelona smart city, and shown the tremendous advantages of such data organization.",10.1109/CFEC.2018.8358732,A. Sinaeepourfard; J. Garcia; X. Masip-Bruin; E. Marin-Tordera
Micro Transfer Learning Mechanism for Cross-Domain Equipment RUL Prediction,2025,0,Topic_0_prediction_degradation_rul,1.0,"Transfer learning generally addresses to reduce the distribution distance between source-domain and target-domain. However, it is unreasonable to use a distribution to represent the life-cycle signals as they are always time-varying, and the improper assumption affects the efficacy of transfer remaining useful life (RUL) prediction. To fill this gap, this research proposes a micro transfer learning mechanism for multiple differentiated distributions, and a transfer RUL prediction model is constructed. First, a multi-cellular long short-term memory (MCLSTM) neural network is applied to obtain multiple differentiated distributions of the monitoring data at some point. Then the domain adversarial mechanism is used to achieve the knowledge transfer of multiple differentiated distributions at the cell level. Furthermore, an active screen mechanism is designed for weighting the domain discrimination losses of multiple differentiated distributions. Through the transfer RUL prediction experiments on aero-engines and actual wind turbine gearboxes, the superiority of this model over the advanced transfer prediction models is verified. Note to Practitioners—The work is motivated by the accuracy reduction problem caused by the time-varying characteristics of life-cycle data in the cross domain equipment RUL prediction scenario, where a fixed single distribution is difficult to cover the full life-cycle data. This article proposes a micro transfer learning mechanism containing multiple differentiated distributions, and a novel transfer RUL prediction model based on the mechanism is constructed for solving the problem caused by the time-varying characteristics of life-cycle data. There are four steps for implementing this method in practice: 1) collecting the full-life cycle signals of historical equipment; 2) modeling the degradation curves of equipment by MCLSTM; 3) solving the cross domain RUL prediction by narrowing the distributions of degradation curves by the micro transfer learning mechanism; and 4) making prognostics for new equipment. The novelty is that the proposed mechanism can self-adaptively align multiple differentiated subspaces of the source domain and the target domain, that is, it can adaptively extract the domain invariant features over time. As a result, the proposed method has two main advantages: 1) capable of characterizing the degradation processes of different equipment; and 2) superior prognostic results on cross domain RUL prediction.",10.1109/TASE.2024.3366288,S. Xiang; P. Li; J. Luo; Y. Qin
Blockchain-based Data Traceability Platform Architecture for Supply Chain Management,2020,-1,Outliers,0.2968767762009392,"With the rapid development of economic globalization, cooperation between countries, between enterprises, has become a key factor whether country and enterprises can make great economic progress. In these cooperation processes, it is necessary to trace the source of business data or log data for auditing and accountability. However, multi-party enterprises participating in cooperation often do not trust each other, and the separate accounting of the enterprises leads to isolated islands of information, which makes it difficult to trace the entire life cycle of the data. Therefore, there is an urgent need for a mechanism that can establish distributed trustworthiness among multiparty organizations that do not trust each other, and provide a tamper-resistant data storage mechanism to achieve credible traceability of data. This work proposes a data traceability platform architecture design plan for supply chain management based on the multi-disciplinary knowledge and technology of the Fabric Alliance chain architecture, perceptual identification technology, and cryptographic knowledge. At the end of the paper, the characteristics and shortcomings of data traceability of this scheme are evaluated.",10.1109/BigDataSecurity-HPSC-IDS49724.2020.00025,Y. Wei
Sticky Policies: A Survey,2020,2,Topic_2_data_privacy_security,0.6890846693204107,"In the digital age, where the Internet connects things across the globe and individuals are constantly online, data security and privacy are becoming key drivers (and barriers) of change for adoption of innovative solutions. Traditional approaches, whereby communication links are secured by means of encryption, and access control is run in a static way by a centralized authority, are showing their limits when applied to massive-scale, interconnected and distributed systems. Regulations, while still fragmented, are moving to adapt to changes in technology and society, with the aim to protect confidential information by governments, businesses, and individual citizens. In this landscape, proper mechanisms should be defined to allow a strict control over the data life-cycle and to guarantee the privacy and the application of specific regulations on personal information's disclosure, usage and access. Sticky policies represent one approach to improve owners' control over their data. In such an approach, machine-readable policies are attached to data. They are called 'sticky' in that they travel together with data, as data travels across multiple administrative domains. In this article we survey the state-of-the-art in sticky policies, discussing limitations, open issues, applications and research challenges, with a specific focus on their applicability to Internet of Things, cloud computing, and Content Centric Networking.",10.1109/TKDE.2019.2936353,D. Miorandi; A. Rizzardi; S. Sicari; A. Coen-Porisini
Master Data Management Maturity Assessment: A Case Study in the Supreme Court of the Republic of Indonesia,2018,1,Topic_1_data_big_big data,0.308459746798584,"Master data management (MDM) of employee is used by the Information System of Personnel (SIKEP) and the Integrated Supreme Court Information System (SIMARI) in Supreme Court of Indonesia Republic (Supreme Court). SIKEP not yet used by all echelon I units in the Supreme Court. Therefore, it is necessary to identify problems where much data in SIKEP is not yet complete and valid. It is necessary to manage the master data of employees in the Supreme Court. The purpose of master data management assessment is to find out the extent of maturity of master data management (MDM) employee of Supreme Court Indonesia Republic (Supreme Court). This assessment is using management master data maturity model (MD3M) base on other research. The objective of this research paper is a practical nature. From a corporate point of view, the objective is to give organization to assess their employee MDM maturity. The result of assessment can identify potential improvement areas of Employee MDM. Final result of the maturity of the master data management of the Supreme Court is as follows; for business function data model and data quality maturity level is 1. The use and ownership and maturity level is 2. Business security function even reach the level of maturity 4. Therefore, the overall result of the maturity level of master data management in the Supreme Court is 1. This means that the Supreme Court already has awareness in the management of master data.",10.1109/CITSM.2018.8674373,N. Qodarsih; S. B. Yudhoatmojo; A. N. Hidayanto
Data Quality and Trust : A Perception from Shared Data in IoT,2020,-1,Outliers,0.39457105411683824,"Internet of Things devices and data sources areseeing increased use in various application areas. The pro-liferation of cheaper sensor hardware has allowed for widerscale data collection deployments. With increased numbers ofdeployed sensors and the use of heterogeneous sensor typesthere is increased scope for collecting erroneous, inaccurate orinconsistent data. This in turn may lead to inaccurate modelsbuilt from this data. It is important to evaluate this data asit is collected to determine its validity. This paper presents ananalysis of data quality as it is represented in Internet of Things(IoT) systems and some of the limitations of this representation. The paper discusses the use of trust as a heuristic to drive dataquality measurements. Trust is a well-established metric that hasbeen used to determine the validity of a piece or source of datain crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework forrepresenting data quality effectively within the big data modeland why a trust backed framework is important especially inheterogeneously sourced IoT data streams.",10.1109/ICCWorkshops49005.2020.9145071,J. Byabazaire; G. O'Hare; D. Delaney
A Data Provenance Visualization Approach,2018,1,Topic_1_data_big_big data,0.4578368321962773,"Data Provenance has created an emerging requirement for technologies that enable end users to access, evaluate, and act on the provenance of data in recent years. In the era of Big Data, the amount of data created by corporations around the world has grown each year. As an example, both in the Social Media and e-Science domains, data is growing at an unprecedented rate. As the data has grown rapidly, information on the origin and lifecycle of the data has also grown. In turn, this requires technologies that enable the clarification and interpretation of data through the use of data provenance. This study proposes methodologies towards the visualization of W3C-PROV-O Specification compatible provenance data. The visualizations are done by summarization and comparison of the data provenance. We facilitated the testing of these methodologies by providing a prototype, extending an existing open source visualization tool. We discuss the usability of the proposed methodologies with an experimental study; our initial results show that the proposed approach is usable, and its processing overhead is negligible.",10.1109/SKG.2018.00019,I. M. Yazici; E. Karabulut; M. S. Aktas
Technology against COVID-19 A Blockchain-based framework for Data Quality,2021,-1,Outliers,0.193982140594499,"The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.",10.1109/CiSt49399.2021.9357200,I. Ezzine; L. Benhlima
"Cognitive Identity Management: Synthetic Data, Risk and Trust",2020,2,Topic_2_data_privacy_security,0.652422127368993,"Synthetic, or artificial data is used in security applications such as protection of sensitive information, prediction of rare events, and training neural networks. Risk and trust are assessed specifically for a given kind of synthetic data and particular application. In this paper, we consider a more complicated scenario, - biometric-enabled cognitive cognitive biometric-enabled identity management, in which multiple kinds of synthetic data are used in addition to authentic data. For example, authentic biometric traits can be used to train the intelligent tools to identify humans, while synthetic, algorithmically generated data can be used to expand the training set or to model extreme situations. This paper is dedicated to understanding the potential impact of synthetic data on the cognitive checkpoint performance, and risk and trust prediction.",10.1109/IJCNN48605.2020.9207385,S. Yanushkevich; A. Stoica; P. Shmerko; W. Howells; K. Crockett; R. Guest
The Data Interplay for the Fog of Things: A Transition to Edge Computing with IoT,2018,1,Topic_1_data_big_big data,1.0,"The progress towards proving full deployable Internet of Things solutions for cross-domain data exchange is moving slowly and in todays IoT's challenges the need for integrating data across different software platforms and its use over heterogeneous technology remains open. The use of Cloud computing for data sharing as part of the IoT solution(s) design is provided on the basis that sharing data is enabled by the cloud. The emerging of Fog computing is generating that new scenarios are being re-defined, and in particular cases the need for combining both technologies enabling interoperability is explored/studied. This paper presents an approach of the Data Interplay for the Fog of Things. The need for a more structured way to exchange data seamlessly between edge applications and the cloud is required. The data interplay between Fog and Cloud infrastructures addresses the need for big volume generation and storage of data and the way on how to transfer the data from the edge to the cloud and viceversa.",10.1109/ICC.2018.8423006,L. Andrade; M. Serrano; C. Prazeres
Framing the scope of the common data model for machine-actionable Data Management Plans,2018,1,Topic_1_data_big_big data,0.4988358130816844,"Currently, research requires processing data at a large scale. Data is not anymore a collection of static documents, but often a continuous stream of information flowing into information systems. Researchers need to manage their data efficiently not only to keep it safe, but also to ensure that it can be later correctly interpreted and reused. Existing solutions are not sufficient. Traditional Data Management Plans are manually created text documents that describe how research data will be handled. Yet, researchers must implement all actions by themselves. Machine-actionable Data Management Plans are a new approach that allows systems to act on behalf of researchers and other stakeholders involved in data management, to help them manage data in an efficient and scalable way. This paper summarises the results of work performed by the Research Data Alliance working group on Data Management Plan Common Standards to realise this vision. The paper describes results of consultations and proof of concept tools that help in: identifying needs for information of stakeholders involved in data management; defining the scope of the common data model for Machine-actionable Data Management Plans to allow for exchange of information between systems; identifying necessary services and components of infrastructure that support automation of data management tasks.",10.1109/BigData.2018.8622618,T. Miksa; J. Cardoso; J. Borbinha
The eXtreme-DataCloud project: data management services for the next generation distributed e-infrastructures,2018,1,Topic_1_data_big_big data,0.8457363699027501,"The eXtreme-DataCloud (XDC) is an EU H2020 funded project aimed at developing scalable technologies for federating storage resources and managing data in highly distributed computing environments. XDC software stack is based on existing tools, whose technical maturity is proved, that the project enriches with new functionalities and plugins to address real life requirements from user communities belonging to a variety of scientific domains: Life Science, Astrophysics, High Energy Physics, Photon Science and Clinical Research. The XDC toolbox includes services like ONEDATA (AGH), dChace (DESY), EOS (CERN), the INDIGO-PaaS Orchestrator (INFN), DYNAFED (CERN). The targeted platforms are the current and next generation e-Infrastructures deployed in Europe, such as the European Open Science Cloud (EOSC), the European Grid Infrastructure (EGI), the Worldwide LHC Computing Grid (WLCG) and the computing infrastructures funded by other public and academic initiatives. The focus of the project is on policy driven data management, on orchestration based on Quality of Services and on pre-processing of ingested data with user-defined applications. Smart caching and storage federation technologies for the creation of the so-called “DataLakes” are also important topics for XDC. In this manuscript, we present the recent advancements in the project services architecture and the developments carried on in preparation of the first XDC release planned for the end of 2018.",10.1109/ROLCG.2018.8572025,D. Cesini; A. Costantini; P. Fuhrmann; F. Aguilar; C. Duma; C. Ohmann; R. Lemrani; O. Keeble; S. Battaglia; V. Poireau; M. Viljoen; G. Donvito
Master Data Management Maturity Assessment: A Case Study of Organization in Ministry of Education and Culture,2018,1,Topic_1_data_big_big data,0.3106520838614744,"The Master Data Management (MDM) empowers the organization to consolidate and integrate multiple master data sources into a single source of truth. Successful implementations of Master Data Management enable the organization to have standardized content that reflects business-based logic and rules processing. The goal of most MDM is to ensure that master data includes reference details that reflect the current state of the business. Master data management maturity model is a measurement of the ability of an organization for continuous improvement in master data management. The aim of this study is to measure MDM implemented in case organization with Master Data Management Maturity Model (MD3M). MD3M has five key topics and 13 focus areas. Those key topics and focus areas are designed to measure master data management maturity, each focus area has packed capabilities to be accomplished. The measurement showed that its maturity level is 0. Nevertheless, it not means that the organization did not implement MDM. The findings showed that almost 50% of the capabilities in MD3M had been implemented. By implementing further improvement about the missing capabilities, the organization will achieve higher maturity.",10.1109/IC3INA.2018.8629524,F. G. Pratama; S. Astana; S. B. Yudhoatmojo; A. Nizar Hidayanto
Future Security Challenges for Smart Societies: Overview from Technical and Societal Perspectives,2020,2,Topic_2_data_privacy_security,0.3922457529012414,"Human societies went through a long journey from fundamental Society 1.0 to smart Society 5.0. From a security point of view, each phase of development has its characteristics and specialisation. In modern research, when considering security matters, the focus cannot be only on technology but also society, as a group of human beings, needs a different perspective. Rapid technological development brings higher standards and quality of life, while our security standards and personal perceptions of security concepts need to adapt and follow the pace of these developments. This paper stimulates a new concept of societal security, along with a review of various kinds of threats to Society 5.0. The main objective of the paper is to shed light on the risks and threats facing future societies. Related to the here and now, it points out where interdisciplinary research is indispensable and still needs to be done to find answers and solutions. However, the originality of this paper lies in the fact that it combines both the technical and societal perspectives.",10.1109/ICSGCE49177.2020.9275630,M. Aldabbas; X. Xie; B. Teufel; S. Teufel
Research of Prognostic and Health Management for Avionics System Based on Massive Data Mining,2018,0,Topic_0_prediction_degradation_rul,1.0,"The new-generation avionics system is an electronically intensive device. Its complex system structure, multiple failure modes and complex failure mechanism make it difficult to carry out effective fault prediction and health management based on traditional methods. A prognostic and health management (PHM) based on massive data mining of avionics system is presented in this paper. Firstly, the characteristics of the new generation avionics system are analyzed from the aspects of system structure, service environment, and data source and data storage mode. Secondly, the key technologies based on mass data mining are discussed, including data preprocessing, integrated management, diagnosis and prognostic model and so on. Finally, a massive data mining platform of avionics system PHM based on private cloud is proposed. The overall framework, hardware and software structure of the platform are described in detail. The massive data mining platform of avionics system can quickly and effectively carry out data mining, and provide a verification platform for the avionics system PHM, which is of great significance to promote the integration and engineering realization of avionics system PHM.",10.1109/PHM-Chongqing.2018.00066,X. Jiao; B. Jing; B. Jiao
Big Data Privacy Breach Prevention Strategies,2020,2,Topic_2_data_privacy_security,0.7533903643787621,"Over the length of time, there has been an exponential increase in the number of netizens throughout the world. The surge in internet users has been especially prominent in times of the Covid-19 pandemic. Consequently, there has been a rise in social data collection from social media sites and apps. In addition to it, there is machine data generated by the sensors and other industrial and medical equipment and finally, transactional data generated from transactions performed online as well as offline. The big data collected from these principal sources provides invaluable insights to various small as well as leading organizations. Hence, these organizations and researchers direct considerable attention and efforts towards the high volume, velocity and variety (referred to as the “3V”) challenges. However, due to the sheer volume of the big data, high computing power and substantial storage are needed. This feat is achieved by the use of a network of distributed systems. Since multiple systems are involved in this process, the risk of privacy breach is increased manifold. The prevention of such breaches is crucial as it may result in leakage of highly sensitive data and impose a severe threat to the privacy of individuals. The primary objective of this paper is to provide a detailed synopsis of countermeasures that can be adopted against possible data breaches at the several stages of the big data life cycle (i.e., data generation, data storage and data processing) to ensure better security of the exabytes of big data generated each day.",10.1109/iSSSC50941.2020.9358878,S. Varshney; D. Munjal; O. Bhattacharya; S. Saboo; N. Aggarwal
Business Ecosystem Strategy Using New Hydroponic Culture Method,2019,3,Topic_3_industry_manufacturing_chain,0.34722911977747895,"The current, important global issues include ""global warming,"" ""population increase,"" and ""food shortage."" Japan, however, experiences problems such as ""declining birthrate and aging,"" ""declining agricultural workers,"" and ""increasing abandoned cultivation land."" In this study, we developed a new ""EZ hydroponic culture method"" to solve these problems. In this unique flooded agriculture system, many permeable pots with one seedling, together with fertilizer and culture medium, are mounted on a foamed polystyrene plate and floated on the surface of the water. Its characteristics are high profitability, low initial investment amount, low labor load, and low environmental load. However, this new hydroponic cultivation method differs considerably from traditional agricultural systems in terms of not only technical aspects but also management. It becomes even more prominent by collecting big data using internet of things (IoT) technology and drones as well as artificial intelligence (AI). Therefore, the construction of a new business ecosystem strategy becomes increasingly important. In this research, we proposed the ""EZ hydroponic cultivation business ecosystem strategy"" using the ""Five Frameworks for Constructing Keystone Strategy"" proposed by PICMET 2018. Furthermore, its effectiveness was confirmed based on three key performance indicators (productivity, robustness, and niche creation).",10.23919/PICMET.2019.8893714,H. Kubo; K. Okoso
Equipment health assessment and fault-early warning algorithm based on improved SVDD,2018,0,Topic_0_prediction_degradation_rul,1.0,"With the rapid development of Internet-of-Things and big data, health assessment of equipment has become a hot spot in recent years. It is critical to bridge the gap between real-time factory data and health status evaluation, which helps decide appropriate maintenance time by quantitative fault-early warning. For this purpose, this paper proposes a framework to realize real-time equipment health management. The framework begins with principal component analysis (PCA) for feature reduction and support vector data description (SVDD) method for identifying abnormal observations. To promote the computational efficiency of the static health assessment model, an improved incremental learning SVDD method based on KKT (Karush-Kuhn-Tucker) condition (KISVDD) is proposed. Then health degree (HD) is defined derived from deviation degree (DD) based on Euclidean distance. Subsequently, a fault-early warning threshold setting method based on sliding window is established to realize quantitative maintenance time prediction. Thereafter, the proposed scheme is compared with different types of algorithms in a case study to demonstrate the effectiveness of the proposed model using actual production data. The results show that the proposed model outperforms traditional ones in accuracy and computational efficiency.",10.1109/COASE.2018.8560464,L. Zhang; F. Qiao; J. Wang
Ethical Concerns: An overview of Artificial Intelligence System Development and Life Cycle,2020,1,Topic_1_data_big_big data,0.43266886339409666,"The expansive growth of Artificial Intelligent Systems (AIS),areas of applications, and their architectural complexity resulting in compound Systems of Systems has also carried with this, a myriad of ethical challenges. The ethical concerns are not limited to just the applications but also the data collection processes and derivative computed Information and Knowledge. A by product that has resulted is the extended marketing venues of data conversion to commodities. This paper examines how these issues effect ethics throughout the life cycle of the AIS and carries into the life cycle of the data.",10.1109/ISTAS50296.2020.9462201,H. A. Shazly; A. Ferraro; K. Bennet
Unsupervised Health Indicator Fusing Time and Frequency Domain Information and Its Application to Remaining Useful Life Prediction,2025,0,Topic_0_prediction_degradation_rul,1.0,"The prediction of component remaining useful life (RUL) is essential in making an appropriate maintenance plan for equipment. Constructing a reliable health indicator (HI) is crucial for RUL prediction. HI can be generated by quantifying distribution discrepancies. Most existing methods construct HIs based on the time domain, whereas in certain cases, time-domain data contain fewer degradation characteristics than frequency-domain data. To enhance the applicability and quality of HIs under different conditions, this article presents a novel unsupervised approach for generating HI from both the time and frequency domains. Considering the frequency-domain data characteristics of mechanical vibration signals, an exponential mixture model (EMM) is first applied to extract the frequency-domain distribution characteristics. Furthermore, a Gaussian mixture model (GMM) is used to mine time-domain distribution characteristics. Subsequently, a distribution contact ratio metric (DCRM) is employed to respectively generate the time and frequency domain HIs by quantifying the discrepancies between baseline distribution and data distributions at different degradation moments. The final HI is constructed by weighting the time and frequency domain HIs. RUL prediction is achieved using the Proposed-HI and a variant of recurrent neural network. Finally, the efficiency and superiority of this approach are validated using multiple gear life-cycle datasets, and the presented HI exhibits a higher RUL prediction accuracy than classical and advanced unsupervised HIs.",10.1109/TIM.2025.3529072,D. Chen; J. Zhou; Y. Qin
GS1 Connected Car: An Integrated Vehicle Information Platform and Its Ecosystem for Connected Car Services based on GS1 Standards,2018,-1,Outliers,0.5302032862161914,"In recent years, the connected automotive industry has grown explosively. Various connected car services are emerging, such as remote vehicle diagnostics, driver's health monitoring, infotainment, and vehicle safety management. As a result, the number and type of vehicle data are increasing tremendously day by day. However, existing connected automotive solutions have a limitation in that each company manages its own closed data silos. This restricts connected car services from using data sources in various domains. Hence, we propose the GS1 Connected Car, an integrated vehicle information platform, and its ecosystem. We suggest GS1-based automotive data standards for not only in-vehicle data but also all the automotive-related data generated during the lifecycle of vehicles. We provide standardized data collection to EPCIS, the discovery of global automotive services using ONS, IoT Mash-up service between an in-car dashboard platform and IoT devices, video infotainment called GS1 video, and automotive lifecycle management application. We have implemented our platform in a real car by developing an Android-based vehicle dashboard, including service discovery, Mash-up services, and GS1 Video. Also, a mobile application for lifecycle management and Amazon skills for collecting driver's information are developed. Our demonstration and case study show the feasibility of the proposed platform, widening the scope of future connected car services.",10.1109/IVS.2018.8500698,J. Han; H. Kim; S. Heo; N. Lee; D. Kang; B. Oh; K. Kim; W. Yoon; J. Byun; D. Kim
"Lifecycle Management Protocols for Batteryless, Intermittent Sensor Nodes",2020,1,Topic_1_data_big_big data,1.0,"Nodes in batteryless sensor networks operate intermittently, making tasks such as node-to-node communication and coordinated computation extremely challenging. Adding to this challenge, a node typically has little control over its intermittency. Therefore, in this paper, we introduce a new class of protocols, which we call lifecycle management protocols (LMPs), to better control and manage the intermittency of batteryless nodes. These protocols may be designed and optimized for a particular task; here, we propose and evaluate a set of LMPs designed to enable direct communication between intermittent batteryless sensor nodes with active radios.",10.1109/IPCCC50635.2020.9391571,M. L. Wymore; V. Deep; V. Narayanan; H. Duwe; D. Qiao
An Intelligent Recommendation Mobile Application Privacy Risk Evaluation Method Based on Optimized SVM,2020,2,Topic_2_data_privacy_security,1.0,"In recent years, intelligent recommendation (IR) technique has been widely utilized in mobile applications (App), which promotes the development of mobile internet and improves the user experience. However, to provide the service related to users' personalized taste, the mass privacy information of user needs to be collected by IR Apps. This makes IR Apps in front of various security risks, especially the privacy security risk. It is of great significance in both theory and practice to make a privacy risk evaluation on IR Apps. In this work, an approach for IR App privacy risk evaluation based an optimized SVM classifier from a hierarchical privacy security risk factor Set is proposed, which reveals the changing relationship between risk factors and risk levels, and also gives a way to solve the nonlinearity, high-dimensional, and multi-sample problems in the IR App privacy risk evaluation work. In this evaluation model, first we give out a hierarchical privacy security risk factor set oriented the hole data life cycle by comprehensive analysis. Then a feature selection method based on MRMD algorithm is applied to calculate the correlation between features and risk levels, and the factors have a strong correlation with risk level evaluation are selected to constitute a feature vector, which reduce the redundancy. Finally, the extracted features are given input the evaluation model based on an optimized SVM classifier to calculate the risk level. The experiment results show the improvement and a certain generalization in optimized model. This work also enriches the quantitative method in risk evaluation, and can provide quantitative and visual decision support for application providers to improve the privacy protection and for government to keep abreast of development of application security in practice.",10.1109/ITNEC48623.2020.9085180,T. Qingqing; N. Mengting; W. Juan
The Research on User Privacy Protection of library Intelligent Service,2020,2,Topic_2_data_privacy_security,1.0,"With the continuous deepening of the concept of intelligent service, the insecurity factor of library users’ privacy data those are leaked is increasing. Under the background of the fusion of technology application and concept innovation, establishing a set of user privacy protection mechanism to balance the use of library data and the protection of user privacy in order to facilitate the construction of library service. Implementing the design of user privacy protection mechanism from three levels of macro background, core elements and service supervision by reviewing the existing literature resources, introducing data life cycle model, and setting up the model of potential threats constitution for library user privacy. Under the scope of intelligent service, the library realizes the balance of rights and responsibilities between data mining and privacy protection and the organic integration of policy ideas and technology applications.",10.1109/ISCEIC51027.2020.00050,X. Ziyue; C. Ya
Data Life Aware Model Updating Strategy for Stream-based Online Deep Learning,2020,1,Topic_1_data_big_big data,0.5169887339377388,"Many deep learning applications deployed in dynamic environments change over time, in which the training models are supposed to be continuously updated with streaming data in order to guarantee better descriptions on data trends. However, most of the state-of-the-art learning frameworks support well in offline training methods while omitting online model updating strategies. In this work, we propose and implement iDlaLayer, a thin middleware layer on top of existing training frameworks that streamlines the support and implementation of online deep learning applications. In pursuit of good model quality as well as fast data incorporation, we design a Data Life Aware model updating strategy (DLA), which builds training data samples according to contributions of data from different life stages, and considers the training cost consumed in model updating. We evaluate iDlaLayer's performance through both simulations and experiments based on TensorflowOnSpark with three representative online learning workloads. Our experimental results demonstrate that iDlaLayer reduces the overall elapsed time of MNIST, Criteo and PageRank by 11.3%, 28.2% and 15.2% compared to the periodic update strategy, respectively. It further achieves an average 20% decrease in training cost and brings about 5 % improvement in model quality against the traditional continuous training method.",10.1109/CLUSTER49012.2020.00049,W. Rang; D. Yang; D. Cheng; K. Suo; W. Chen
Research on Data Security Protection Method Based on Big Data Technology,2020,2,Topic_2_data_privacy_security,0.4193868739974994,"The construction of power Internet of things is an important development direction of power grid enterprises in the future. Big data not only brings economic and social benefits to the power system industry, but also brings many information security problems. Therefore, in the case of accelerating the construction of ubiquitous electric Internet of things, it is urgent to standardize the data security protection in the ubiquitous electric Internet of things environment. By analyzing the characteristics of big data in power system, this paper discusses the security risks faced by big data in power system. Finally, we propose some methods of data security protection based on the defects of big data security in current power system. By building a data security intelligent management and control platform, it can automatically discover and identify the types and levels of data assets, and build a classification and grading information base of dynamic data assets. And through the detection and identification of data labels and data content characteristics, tracking the use of data flow process. So as to realize the monitoring of data security state. By protecting sensitive data against leakage based on the whole life cycle of data, the big data security of power grid informatization can be effectively guaranteed and the safety immunity of power information system can be improved.",10.1109/ICCSN49894.2020.9139118,D. Liu; R. Wang; H. Zhang; L. Ma; X. Liu; H. Huang; Y. Chang
Trustworthy Data Acquisition and Faulty Sensor Detection using Gray Code in Cyber-Physical System,2021,-1,Outliers,0.22357828756405282,"Due to environmental influence and technology limitation, a wireless sensor/sensors module can neither store or process all raw data locally nor reliably forward it to a destination in heterogeneous IoT environment. As a result, the data collected by the IoT's sensors are inherently noisy, unreliable, and may trigger many false alarms. These false or misleading data can lead to wrong decisions once the data reaches end entities. Therefore, it is highly recommended and desirable to acquire trustworthy data before data transmission, aggregation, and data storing at the end entities/cloud. In this paper, we propose an In-network Generalized Trustworthy Data Collection (IGTDC) framework for trustworthy data acquisition and faulty sensor detection in the IoT environment. The key idea of IGTDC is to allow a sensor's module to examine locally whether the raw data is trustworthy before transmitting towards upstream nodes. It further distinguishes whether the acquired data can be trusted or not before data aggregation at the sink/edge node. Besides, IGTDC helps to recognize a faulty or compromised sensor. For a reliable data collection, we use collaborative IoT technique, gate-level modeling, and programmable logic device (PLD) to ensure that the acquired data is reliable before transmitting towards upstream nodes/cloud. We use a hardware-based technique called “Gray Code” to detect a faulty sensor. Through simulations we reveal that the acquired data in IGTDC framework is reliable that can make a trustworthy data collection for event detection, and assist to distinguish a faulty sensor.",10.1109/CSE50738.2020.00016,H. ur Rahman; G. Duan; G. Wang; M. Z. A. Bhuiyan; J. Chen
Big Data Privacy Management: A Vision Paper,2020,2,Topic_2_data_privacy_security,1.0,"The growing trend towards big data and cloud computing provides enormous data-driven applications such as location-based services. However, it also creates many potential risks for some individuals in big data scenarios. These risks are further complicated by the security and privacy constraints on the individuals' data that are inherently sensitive. To handle these risks and challenges, this paper proposes our vision of an active, adaptive, and scalable framework for sharing and processing potentially sensitive data. We identify the main privacy risks and technical challenges and present some preliminary solutions. The key idea of this framework is that it integrates risk active monitoring, risk active assessment, privacy active management, accountable systems, and law and regulation to ensure big data security and privacy in the whole system. We believe that this conceptual framework may provide a helpful and useful foundation for big data privacy management and will open up many existing research challenges.",10.1109/TPS-ISA50397.2020.00016,X. Meng; X. Zhang
Machine Tool Interoperability in Smart Manufacturing and Industry 4.0,2025,3,Topic_3_industry_manufacturing_chain,0.7965391403856232,"Real-time decision making is supported by data-hungry emerging technologies such as machine learning and digital twins. Innovations in manufacturing process control utilizing these technologies are constrained by interoperability. Marketing materials, sales pitches, and academic literature portray interoperability on the factory floor as seamless and robust. However, this study demonstrates that in spite of plentiful mature standards, interoperability on the factory floor is neither seamless nor robust. To exemplify interoperability in the context of an established and widely adopted standard, this study analyzes the interoperability of machine tools produced by a premier equipment builder which has exceeded US ${\$}$ 1 billion in sales with more than 200,000 machines currently in operation worldwide. Through demonstrating and discussing non-interoperability and influencing factors, this study aims to provide insights and actionable intelligence applicable to industry and academia for laymen, end users, systems integrators, standards organizations, equipment builders, and data scientists. By leveraging phronesis and empirical evidence to demonstrate the state of interoperability, this study provides a fresh perspective on an age-old manufacturing challenge.",10.1109/ACCESS.2025.3585766,M. R. Mccormick; M. Shafae; T. Wuest
On Technology Structure of Data Full-Life-Cycle Security in Industrial Internet,2020,2,Topic_2_data_privacy_security,0.42211801214463207,"This paper focuses on the design of the technology structure of data full-life-cycle security in Industrial Internet and offers a guidance-level solution for government bodies and enterprises involved. Data security requires complete coverage of its life-cycle. This paper, based on the theory of the six-phrase division of data life-cycle, gives an analysis on the threats and risks faced by data in its acquisition, transmission, storage, processing, exchange, and destruction, and the corresponding technologies to handle them. With Industrial Internet architecture as its object, an interrelationship of ""time dimension"" of data life-cycle and ""spatial dimensions"" of its components is established, a practical mapping of data life-cycle within the architecture is created and a specific technology structure is proposed for data security.",10.1109/ITCA52113.2020.00099,S. Liang; Y. Shuai-feng; L. Jun; G. Xu; L. Cai-yun
A Novel Cross-Scenario Transferable RUL Prediction Network With Multisource Domain Meta Transfer Learning for Wind Turbine Bearings,2025,0,Topic_0_prediction_degradation_rul,1.0,"Accurately predicting the remaining useful life (RUL) of bearings is quite significant for ensuring the operation reliability of wind turbines. Due to the limitation of real-world life cycle data, the accuracy of existing wind turbine RUL prediction methods needs to be improved. This article suggests a multisource domain meta transfer learning (MD-MTL)-based cross-scenario transferable RUL prediction method. The MD-MTL uses test rig data on multiple operating conditions to build up the prediction model and migrate the prediction knowledge to real wind turbines. In addition, a novel RUL prediction network based on a convolutional encoder and ProbSparse multihead attention decoder (CE-PSAD) is proposed for the improvement of prediction accuracy. It can extract key degradation features and long-term dependence relationships from raw vibration signals, and complete the precise mapping to RUL. The efficiency of the proposed method is proven based on two test rig datasets and two wind turbine bearing datasets.",10.1109/TIM.2025.3533626,L. Cao; X. Wang; H. Zhang; Z. Meng; J. Li; M. Liu
Capturing and Modeling Uncertainty in Prognostics and Health Management using Machine Learning,2020,0,Topic_0_prediction_degradation_rul,0.6598847613871449,"Prognostics and Health Management (PHM) systems are backbone life cycle management techniques with data-oriented methods that make predictions of system health, thereby enhancing maintenance practices and capabilities. The accuracy and dependability of PHM systems are proportional to the validity of the data used. Uncertainty and errors in sensor data lead to skewed predictions by any PHM model. Uncertainty management is an important aspect that has to be considered for the design of an efficient PHM. Recent research has inevitably focused on using the emerging technologies of big data and machine learning for prognostics. This paper gives an outline of existing methods to capture and model uncertainty by providing an overview of integrating machine learning techniques to incorporate data uncertainty into a data-driven PHM model for a complex system.",10.1109/ICCES48766.2020.9137918,S. Harshavardhanan; M. J. Nene
Multimedia Big Data Security,2018,2,Topic_2_data_privacy_security,0.8173512172029858,"The 21st century is all about data where the main producers are Social Networks, Smartphones, Personal Medical Reports etc. In earlier times data volume used to be in MegaBytes and GigaBytes but in the present scenario PettaBytes or ZettaBytes of data is coming with a very high speed with different varieties from the above sources. Data and Security in today's time are complimentary in nature that needs to be addressed simultaneously and to provide security to such huge amount and variety of data, different encryption/decryption models are used. Multimedia data and its use have reached to gigantic level and as it belongs to the millions of user's, the security of data is the primary concern. So, there should be the ways through which one can ensure the best possible practice to enhance the security of Data. This paper focuses on various techniques to secure the multimedia data which existed in various types like image, audio, chat, videos and text and also this paper provides an overview of available multiple encryption schemes to secure the different forms of multimedia data and critically analyse them.",10.1109/ICRIEECE44171.2018.9008517,P. Chauhan; A. Choudhary; A. K. Gupta
From Spatial-Temporal Cluster Relationships to Lifecycles: Framework and Mobility Applications,2020,-1,Outliers,0.3648028263761832,"Spatial-temporal data analysis relates to the application of data analysis techniques to data where space and time are both relevant. Usually, the results of these techniques are used to classify or predict a phenomenon, but little attention is given to the explanation of how such phenomenon happened. For example, one may predict that a sporting event will happen at a particular location and date, but little is known about the indications that such event will happen (e.g. a higher number of vehicles on certain streets, parking lots becoming full, large number of vehicles going to supermarkets, or a sudden drop in pedestrian and vehicular traffic movement when the match starts). In this paper, we report on our ongoing work on using spatial-temporal cluster relationships to identify cluster lifecycles. These lifecycles are a series of stages through which a cluster passes during its lifetime, much like a human lifecycle of birth, growth, reproduction, and death. We focus on the identification of cluster lifecycle stages, namely start, expand, shrink, and end, and on their use to predict spatial-temporal phenomena, such as traffic congestion, human events, or animal movement.",10.1109/BigData50022.2020.9377763,I. Portugal; P. Alencar; D. Cowan
An Enhanced Approach for Remaining Useful Life Prediction of Bearings Using Incomplete Lifecycle Data,2025,0,Topic_0_prediction_degradation_rul,1.0,"Accurately predicting the remaining useful life (RUL) of rolling bearings is crucial for ensuring optimal equipment operation and timely maintenance. This study addresses key limitations of traditional deep learning approaches, including limited lifecycle data and a significant reliance on supervised learning. To address these challenges, we propose the Incomplete Lifecycle Data Enhanced Network (ILDENet), a self-supervised learning network incorporating contrastive learning to improve feature discrimination. The network operates in three distinct phases: first, feature parameters indicative of bearing degradation trends are extracted across time, frequency, and time-frequency domains. Second, pre-training is performed in a mask self-supervised learning mode with contrastive learning on extensive incomplete lifecycle data, effectively distinguishing similar and dissimilar degradation patterns. Finally, the model undergoes fine-tuning in a supervised phase using complete lifecycle data. To further improve prediction accuracy, we employ a channel-independent transformer network, while RUL estimation is accomplished by integrating multi-source features through a multi-layer perceptron (MLP) layer. The proposed method is rigorously evaluated on both the IEEE PHM 2012 and XJTU-SY datasets and benchmarked against state-of-the-art approaches. Experimental results demonstrate that the proposed algorithm effectively captures the complex dynamic behavior of rolling bearings, achieving high accuracy and generalization in RUL prediction.",10.1109/ACCESS.2025.3553279,X. An; C. Zhang; C. Liu; W. Zhao
Privacy in Internet of Things: From Principles to Technologies,2019,2,Topic_2_data_privacy_security,0.3774259174807413,"Ubiquitous deployment of low-cost smart devices and widespread use of high-speed wireless networks have led to the rapid development of the Internet of Things (IoT). IoT embraces countless physical objects that have not been involved in the traditional Internet and enables their interaction and cooperation to provide a wide range of IoT applications. Many services in the IoT may require a comprehensive understanding and analysis of data collected through a large number of physical devices that challenges both personal information privacy and the development of IoT. Information privacy in IoT is a broad and complex concept as its understanding and perception differ among individuals and its enforcement requires efforts from both legislation as well as technologies. In this paper, we review the state-of-the-art principles of privacy laws, the architectures for IoT and the representative privacy enhancing technologies (PETs). We analyze how legal principles can be supported through a careful implementation of PETs at various layers of a layered IoT architecture model to meet the privacy requirements of the individuals interacting with IoT systems. We demonstrate how privacy legislation maps to privacy principles which in turn drives the design of necessary PETs to be employed in the IoT architecture stack.",10.1109/JIOT.2018.2864168,C. Li; B. Palanisamy
Where's My Data? Evaluating Visualizations with Missing Data,2019,1,Topic_1_data_big_big data,0.4416970198182173,Many real-world datasets are incomplete due to factors such as data collection failures or misalignments between fused datasets. Visualizations of incomplete datasets should allow analysts to draw conclusions from their data while effectively reasoning about the quality of the data and resulting conclusions. We conducted a pair of crowdsourced studies to measure how the methods used to impute and visualize missing data may influence analysts' perceptions of data quality and their confidence in their conclusions. Our experiments used different design choices for line graphs and bar charts to estimate averages and trends in incomplete time series datasets. Our results provide preliminary guidance for visualization designers to consider when working with incomplete data in different domains and scenarios.,10.1109/TVCG.2018.2864914,H. Song; D. A. Szafir
Analytics Everywhere: Generating Insights From the Internet of Things,2019,1,Topic_1_data_big_big data,1.0,"The Internet of Things is expected to generate an unprecedented number of unbounded data streams that will produce a paradigm shift when it comes to data analytics. We are moving away from performing analytics in a public or private cloud to performing analytics locally at the fog and edge resources. In this paper, we propose a network of tasks utilizing edge, fog, and cloud computing that are designed to support an Analytics Everywhere framework. The aim is to integrate a variety of computational resources and analytical capabilities according to a data life-cycle. We demonstrate the proposed framework using an application in smart transit.",10.1109/ACCESS.2019.2919514,H. Cao; M. Wachowicz; C. Renso; E. Carlini
A Framework for Big Data Governance to Advance RHINs: A Case Study of China,2019,1,Topic_1_data_big_big data,0.7108886927501432,"The emergence of big data presents a serious challenge to the fast growth of regional health information networks (RHINs) globally. In China, many constructors of RHINs have spontaneously and independently created governance measures, which may be valuable as a point of reference for other countries. This paper aimed to propose a big data governance framework for healthcare data based on the governance activities associated with the processing of RHINs in China. Typical methodology for RHIN case studies in China, including rich personal experience in nationwide consulting, literature review, expert consultation, and interpretative structural modeling methods, was adopted. Based on the analysis of ten typical RHIN case studies, healthcare big data governance practices in China were summarized. A framework with 3 domains and 12 elements was proposed, which include a drive domain (big data strategy planning, laws and regulations, open transaction, and industry support), capability domain (healthcare big data organization, collection, storage, process and analysis, and usage), and support domain (healthcare big data resource planning, standards system, and privacy and security protection). We obtained 12 guidelines for healthcare big data governance. A big data governance framework with 3 domains and 12 elements was presented based on Chinese practice, which might serve as valuable references for the cross-dimensional development of RHINs, provide overall guidance for the sustainable development of regional health informatization, and contribute to realizing the business value of healthcare big data.",10.1109/ACCESS.2019.2910838,Q. Li; L. Lan; N. Zeng; L. You; J. Yin; X. Zhou; Q. Meng
KDD meets Big Data,2016,-1,Outliers,0.3699418644920122,"Cross-Industry Standard process model (CRISPDM) was developed in the late 90s by a consortium of industry participants to facilitate the end-to-end data mining process for Knowledge Discovery in Databases (KDD). While there have been efforts to better integrate with management and software development practices, there are no extension to handle the new activities involved in using big data technologies. Data Science Edge (DSE) is an enhanced process model to accommodate big data technologies and data science activities. In recognition of the changes, the author promotes the use of a new term, Knowledge Discovery in Data Science (KDDS) as a call for the community to develop a new industry standard data science process model.",10.1109/BigData.2016.7840770,N. W. Grady
Metrology for the factory of the future: towards a case study in condition monitoring,2019,-1,Outliers,0.42951276867837,"The “Factory of the Future” (FoF) as a fully inter-connected production environment with an autonomous flow of information and decision-making constitutes the digital transformation of manufacturing to improve efficiency and competitiveness. Transparency, comparability and sustainable quality all require reliable measured data, processing methods and results. Hence, a metrological framework for the complete lifecycle of measured data in industrial applications is required: from calibration capabilities for individual sensors with digital pre-processed output to uncertainty quantification associated with machine learning (ML) in industrial sensor networks. This contribution presents a European metrology research project to develop such a framework. Special focus in this contribution is set on an implementation testbed where the framework will be used to demonstrate the practical applicability for condition monitoring.",10.1109/I2MTC.2019.8826973,T. Dorst; B. Ludwig; S. Eichstädt; T. Schneider; A. Schütze
SensiTrack - A Privacy by Design Concept for Industrial IoT Applications,2019,2,Topic_2_data_privacy_security,1.0,"In the course of further digitization of industrial manufacturing, companies are using tracking systems to monitor their production processes. This creates a large amount of data with location references, having an influence on the employees privacy - especially if personal references to employees are possible. To investigate this topic, a use case was carried out in an industrial environment. Therefore, a self-developed asset tracking (AT) system was developed and compared with a Commercial-of-the-shelf (COTS) AT-System with regard to functionality and employee privacy. One mayor finding was that the Bluetooth Low Energy (BLE) based legacy system has weaknesses with regard to privacy, e.g. the advertising-data of mobile devices of the employees were stored in the cloud application of the system provider. This data can be used to determine the position of employees. With the self-developed system these weaknesses were avoided by using a Privacy by Design (PbD) approach. Therefore, this paper focuses on employee privacy in industrial AT applications and provides a PbD concept for wireless sensor networks in industrial environments.",10.1109/ETFA.2019.8869186,C. Jandl; J. Nurgazina; L. Schöffer; C. Reichl; M. Wagner; T. Moser
Common Security Criteria for Vehicular Clouds and Internet of Vehicles Evaluation and Selection,2019,2,Topic_2_data_privacy_security,0.4327069835052542,"Internet of Things (IoT) is becoming increasingly important to intelligent transportation system stakeholders, including cloud-based vehicular cloud (VC) and internet of vehicles (IoV) paradigms. This new trend involves communication and data exchange between several objects within different layers of control. Security in such a deployment is pivotal to realize the general IoT-based smart city. However, the evaluation of the degree of security regarding these paradigms remains a challenge. This study aims to discover and identify common security criteria (CSC) from a context-based analysis pattern and later to discuss, compare, and aggregate a conceptual model of CSC impartially. A privacy granularity classification that maintains data confidentiality is proposed alongside the security selection criteria.",10.1109/TrustCom/BigDataSE.2019.00118,M. Aladwan; F. Awaysheh; J. Cabaleiro; T. Pena; H. Alabool; M. Alazab
Towards Closed Loop 5G Service Assurance Architecture for Network Slices as a Service,2019,-1,Outliers,0.5196605086158846,"5G intends to use network slicing to support multiple vertical industries. The dynamic resource sharing and diverse customer requirements bring new challenges towards service assurance (SA), such as automation and customer-centric. As a response to these challenges, this paper proposes a hierarchical, modular, distributed, and scalable SA architecture. This paper highlights an important key feature SA coordination, which is facilitated by three new SA functions, SA interpretation, SA policy management, and data fabric. Three closed-loops are introduced to coordinate and realize automation of service management. Challenges associated with realizing SA are briefly discussed and will be addressed by leveraging the 5G infrastructure developed within the H2020-ICT-17 project 5G-VINNI.",10.1109/EuCNC.2019.8802040,M. Xie; W. Y. Poe; Y. Wang; A. J. Gonzalez; A. M. Elmokashfi; J. A. Pereira Rodrigues; F. Michelinakis
Data quality enhancement in Internet of Things environment,2015,-1,Outliers,0.3710922316560101,"The Internet of Things (IoT) is the first real evolution of the Internet, in contrast with the Web which has been evolving over time. IoT enables the crossing-over of the physical and cyber worlds by deploying sensor devices on a global scale. This crossing-over promises exciting applications that, thanks to intelligent ubiquitous services, will facilitate our daily life. However, the data that represent the bridge between the real and the digital worlds are endangered and their quality is uncertain. In fact, data are the base of insights mining which are used to make intelligent decisions and provide services. If the data are inaccurate, decisions are likely to be unsound. Data accuracy and trustworthiness are crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing data quality in the context of the Internet of Things by providing an overview of the state of the art of data quality in the context of IoT. Data outliers and their underlying knowledge are surveyed alongside their discovery and detection process. The outliers' impact in the context of IoT and its applications is identified. Techniques for data cleaning are reviewed and compared with respect to an extended taxonomy. Finally, open challenges and possible future research directions are discussed.",10.1109/AICCSA.2015.7507117,A. Karkouch; H. Al Moatassime; H. Mousannif; T. Noel
Kullback-Leibler Divergence Constructed Health Indicator for Data-Driven Predictive Maintenance of Multi-Sensor Systems,2019,0,Topic_0_prediction_degradation_rul,1.0,"The unexpected failure of modern industrial systems is often managed using data-driven predictive maintenance (PdM) tools that continuously monitor a system’s health condition (HC) through raw sensor data to predict impending malfunction. However, research demonstrates that data-driven PdM tools perform poorly when applied to raw multi-sensor data, as it is not guaranteed that all sensor data describe a system’s health condition. As systems become more complex and require multi-sensor measurements, to perform accurate data-driven PdM, an accurate health condition representation of a system’s life cycle data is required. This work introduces a Kullback-Leibler divergence (KLD) based health indicator (HI) constructor for multi-sensor systems. This method applies information entropy to construct a HI representation that describes the occurrence of faults and their influences during a system’s life cycle. Additionally, the proposed method conducts feature selection to expose and remove sensors that do not capture information related to a system’s HC. The utility of the proposed method is tested on the Commercial Modular Aero-Propulsion System Simulation turbofan engine data and the OEM Group’s Cintillio SAT Batch Spray semiconductor manufacturing equipment data.",10.1109/INDIN41052.2019.8972069,O. O. Aremu; D. O. O’Reilly; D. Hyland-Wood; P. R. McAree
Maturity Assessment and Strategy to Improve Master Data Management of Geospatial Data Case Study: Statistics Indonesia,2019,1,Topic_1_data_big_big data,0.3089524050020398,"Geospatial data is important in supporting census and survey in Statistics Indonesia. Some problems exist about it are inaccurate maps, the existence of incomplete data attribute, and there are many versions exist in silo systems. They can impact the result of census or survey data quality. Improvement applied to overcome problems is managing geospatial data by adopting geodatabase. To improve the application in the future, it needs to be evaluated. By the reason, this research aims to measure the maturity level of master data management of geospatial data and arrange strategy to improve master data management of geospatial data in the future. This research used Master Data Management Maturity Model to measure the level of maturity and mapped the result to reference and master data management activity of Data Management Body of Knowledge. The result is Statistics Indonesia has not achieved any level yet for geospatial master data management and must concern to apply five activities for improvement in the future namely are to understand reference and master data integration needs, to identify reference and master data sources and contributors, to define and maintain the data integration architecture, to define and maintain match rules, and to establish golden record.",10.1109/ICST47872.2019.9166400,P. Rishartati; N. D. Rahayuningtyas; J. Maulina; A. Adetia; Y. Ruldeviyani
Security Analysis in Context-Aware Distributed Storage and Query Processing in Hybrid Cloud Framework,2019,2,Topic_2_data_privacy_security,0.7985290282566708,"Recent studies have shown that several government and business organizations experience huge data breaches. Data breaches increase in a daily basis. The main target for attackers is organization sensitive data which includes personal identifiable information (PII) such as social security number (SSN), date of birth (DOB) and credit card /debit card (CCDC). The other target is encryption/decryption keys or passwords to get access to the sensitive data. The cloud computing is emerging as a solution to store, transfer and process the data in a distributed location over the Internet. Big data and internet of things (IoT) increased the possibility of sensitive data exposure. Most methods used for the attack are hacking, unauthorized access, insider theft and false data injection on the move. Most of the attacks happen during three different states of data life cycle such as data-at-rest, data-in-use, and data-in-transit. Hence, protecting sensitive data at all states particularly when data is moving to cloud computing environment needs special attention. The main purpose of this research is to analyze risks caused by data breaches, personal and organizational weaknesses to protect sensitive data and privacy. The paper discusses methods such as data classification and data encryption at different states to protect personal and organizational sensitive data. The paper also presents mathematical analysis by leveraging the concept of birthday paradox to demonstrate the encryption key attack. The analysis result shows that the use of same keys to encrypt sensitive data at different data states make the sensitive data less secure than using different keys. Our results show that to improve the security of sensitive data and to reduce the data breaches, different keys should be used in different states of the data life cycle.",10.1109/CCWC.2019.8666498,G. Begna; D. B. Rawat
IsoKV: An Isolation Scheme for Key-Value Stores by Exploiting Internal Parallelism in SSD,2019,1,Topic_1_data_big_big data,0.7716282159841515,"Modern data centers aim to take advantage of high parallelism in storage devices for I/O intensive applications such as storage servers, cache systems, and key-value stores. Key-value stores are the most typical applications that should provide a highly reliable service with high-performance. To increase the I/O performance of key-value stores, many data centers have actively adopted next-generation storage devices such as Non-Volatile Memory Express (NVMe) based Solid State Devices (SSDs). NVMe SSDs and its protocol are characterized to provide a high degree of parallelism. However, they may not guarantee predictable performance while providing high performance and parallelism. For example, heavily mixed read and write requests can result in performance degradation of throughput and response time due to the interference between the requests and internal operations (e.g., Garbage Collection (GC)). To minimize the interference and provide higher performance, this paper presents IsoKV, an isolation scheme for key-value stores by exploiting internal parallelism in SSDs. IsoKV manages the level of parallelism of SSD directly by running application-driven flash management scheme. By storing data with different characteristics in each dedicated internal parallel units of SSD, IsoKV reduces interference between I/O requests. Also, IsoKV synchronizes the LSM-tree logic and data management in SSD to eliminate GC. We implement IsoKV on RocksDB and evaluate it using Open-Channel SSD. Our extensive experiments have shown that IsoKV improves overall throughput and response time on average 1.20× and 43% compared with the existing scheme, respectively.",10.1109/HiPC.2019.00039,H. Lim; H. Kim; K. Myung; H. Young; Y. Son
Cyberinfrastructure Center of Excellence Pilot: Connecting Large Facilities Cyberinfrastructure,2019,1,Topic_1_data_big_big data,0.6214305662536704,"The National Science Foundation's Large Facilities are major, multi-user research facilities that operate and manage sophisticated and diverse research instruments and platforms (e.g., large telescopes, interferometers, distributed sensor arrays) that serve a variety of scientific disciplines, from astronomy and physics to geology and biology and beyond. Large Facilities are increasingly dependent on advanced cyberinfrastructure (i.e., computing, data, and software systems; networking; and associated human capital) to enable the broad delivery and analysis of facility-generated data. These cyberinfrastructure tools enable scientists and the public to gain new insights into fundamental questions about the structure and history of the universe, the world we live in today, and how our environment may change in the coming decades. This paper describes a pilot project that aims to develop a model for a Cyberinfrastructure Center of Excellence (CI CoE) that facilitates community building and knowledge sharing and that disseminates and applies best practices and innovative solutions for facility CI.",10.1109/eScience.2019.00058,E. Deelman; A. Mandal; V. Pascucci; S. Sons; J. Wyngaard; C. Vardeman; S. Petruzza; I. Baldin; L. Christopherson; R. Mitchell; L. Pottier; M. Rynge; E. Scott; K. Vahi; M. Kogan; J. Mann; T. Gulbransen; D. Allen; D. Barlow; S. Bonarrigo; C. Clark; L. Goldman; T. Goulden; P. Harvey; D. Hulsander; S. Jacobs; C. Laney; I. Lobo-Padilla; J. Sampson; J. Staarmann; S. Stone
Embedding Data Science into Computer Science Education,2019,-1,Outliers,0.1520610876364435,"As a fast-growing field with widespread impact on economy and its promising hiring potential, Data Science has been enthusiastically sought-out by many disciplines in the academia, including Mathematics, Statistics, Library Science, Management Information Systems, as well as Computer Science, as an attracting area for recruiting students, soliciting grants, and expanding their existing programs. Although some standalone programs in Data Science have been established around the country, many institutions encounter the challenges in balancing the curricula among different disciplines, allocating new resources, and cooperating with the existing majors regarding enrollment management, student advising, and faculty preparation. In this paper, a strategy-based framework is proposed for those who have limited resources of all kinds to introduce Data Science through their existing Computer Science programs at a baccalaureate level with a minimum curriculum disruption. Instead of a standalone program, to embed Data Science into Computer Science education is demonstrated to be a practical, effective, cost-saving approach based on an extensive study of the synergy between Data Science and Computer Science education. While the proposed framework is not a one-size-fits-all approach, it provides a doable route for blending Data Science into Computer Science education in systematic ways. It has become the consensus that an adequate exposure to Data Science will better prepare computer science students for taking the challenges in this ever-changing, data embraced world.",10.1109/EIT.2019.8833753,I. B. Hassan; J. Liu
Best Practices in Data Management at Ocean Networks Canada: a Citizen Scientist case study,2019,1,Topic_1_data_big_big data,0.42730786523477055,"Ocean Networks Canada (ONC), an initiative of the University of Victoria, operates world-leading ocean observatories and data repository services. ONC has been a member of the World Data System since 2014, meeting rigorous data repository certification criteria. As such, ONC is committed to adhering to research data community standards and best practices in data management including FAIR (findable, accessible, interoperable and reusable) data principles. ONC's data infrastructure, Oceans 2.0, serves a growing array of instruments and platforms from data collection, assurance, and description, to data preservation, discovery, integration, analysis, and distribution. Many existing frameworks within Oceans 2.0 are a result of other oceanographic organizations forming partnerships with ONC for data solutions and services. Here, we feature the best practices in data management ONC applied to a Community Fishers Program for citizen scientists that ingests CTD (conductivity, temperature, and depth), oxygen, and chlorophyll profiles. While the initiative began through a partnership with the Pacific Salmon Foundation's Salish Sea Marine Survival Project in 2015, additional partnerships have recently lead to data acquisition, processing, and product improvements for increased functionality and scalability.",10.23919/OCEANS40490.2019.8962800,M. Wolf; M. Hoeberechts; R. Flagg; R. Jenkyns; M. Morley; B. Biffard; M. Kot; N. Hogman; M. Tomlin; G. Trejos
Cybersecurity Planning for Artificial Intelligent Systems in Space,2019,2,Topic_2_data_privacy_security,0.22906476628844255,"CubeSats continue to proliferate and are an excellent low-cost method of remote sensing. A key piece of intelligent systems is sensory input, data storage, and data communications. With the continued miniaturization of technology, CubeSats will increase their sensory inputs with future miniaturization and enhance their robustness for autonomous operations if data and communications are secure. These futures inspire an intelligent system solution to on-orbit communications. This paper explores a dual-microprocessor approach to improve hardware cybersecurity of intelligent systems, with a view toward intensional intelligence as a means of adjudicating access to sensitive data onboard the CubeSat. With enhanced cybersecurity, Artificial Intelligent Systems (AIS) will add vital utility to otherwise vulnerable, autonomous systems. Using Systems Models-Based Thinking, we shed light on our plan to apply artificial intelligent system concepts to advance CubeSat technology. Managing technology for AIS reduces some of the uncertainties and risks associated with the space environment.",10.23919/PICMET.2019.8893814,G. O. Langford; L. Beaulieu; J. R. Carpenter; I. Watkins; B. Marsh; T. Heidorn; C. Chase
Quality Driven Judicial Data Governance,2019,1,Topic_1_data_big_big data,0.40215684504230115,"With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.",10.1109/QRS-C.2019.00026,T. He; S. Chen; L. Hao; J. Liu
A proposed mapping method for aligning machine execution data to numerical control code,2019,3,Topic_3_industry_manufacturing_chain,0.5970101472903256,"The visions of the digital thread and smart manufacturing have boosted the potential of relating downstream data to upstream decisions in design. However, to date, the tools and methods to robustly map across the related data representations is significantly lacking. In response, we propose a mapping technique for standard manufacturing data representations. Specifically, we focus on relating controller data from machining tools in the form of MTConnect, an emerging standard that defines the vocabulary and semantics as well as communications protocols for execution data, and G-Code, the most widely used standard for numerical control (NC) instructions. We evaluate the efficacy of our mapping methodology through an error measurement technique that judges the alignment quality between the two data representations. We then relate the proposed methodology to a case study, that includes verified process plans and real execution data, derived from the Smart Manufacturing Systems Test Bed hosted at the National Institute of Standards and Technology.",10.1109/COASE.2019.8842832,L. Monnier; W. Z. Bemstein; S. Foufou
Differential Attribute Desensitization System for Personal Information Protection,2019,2,Topic_2_data_privacy_security,1.0,"Big data brings much convenience to our daily life, while it also raises concerns to personal information leakage. It is important for data owner to protect sensitive data from being attacked. In this paper, we elaborate on the possible attacks that may be suffered at various stages of personal data life cycle and the corresponding privacy protection methods. Based on these methods, a differential attribute desensitization system (DADS) for personal information protection has been proposed. Data owner can define sensitive attribute level in the DADS. And then DADS can automatically identify sensitive data, and take differentiated data desensitization measures for structured and unstructured multi-attribute information. Experimental results show that DADS can automatically adapt to different types of sensitive information and protect personal sensitive information effectively.",10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00231,J. Peng; X. Huang; M. Li; J. Zhang; Y. Zhang; N. Gao
Master Data Management Maturity Model: A Case Study at Statistics Business Register in Statistics Indonesia,2019,1,Topic_1_data_big_big data,0.3119238236758682,"Statistics Indonesia is known as Badan Pusat Statistik or BPS in Indonesia, has a mission to provide high quality statistical data through an integrated statistics by implementing national and international standards. To enables practical application of standard statistical units and their classifications, BPS initiated create Master Data to integrated source of frame for all economic survey, that's called statistical business registers (SBR). Main objective of SBR become the primary and only establishment and enterprise directory. But, the current situation, several directories of establishments exist, maintained by various SMEs. Therefore, assessment to SBR in BPS is important, because it becomes a benchmark for the success of SBR in BPS. The measurement was conducted using Master Data Management Maturity Model (MD3M), by providing questionnaires for the two SMEs at BPS, filled with interview. From the domains assessed, the result shows that the maturity level rate of SBR is at level 1 on a score from 1 to 5. This means that a first awareness for issues regarding the topic of MDM has been raised on an operational level. Initial steps are initialized.",10.1109/ICOIACT46704.2019.8938482,D. Krismawati; Y. Ruldeviyani; R. Rusli
Privacy-Aware Job Submission in the Cloud,2019,2,Topic_2_data_privacy_security,0.815243442652236,"The services offered by cloud computing are provided to individuals and organizations by varied shared resources which are forming the hardware layer of cloud data centers. Cloud users do not deal or interact directly with those resources, instead, they deal with the virtualized version of them, in other words, users deal with the virtualization layer which conceals to a great extent the specifics of the physical hardware layer. Based on the virtualization concept, more than one virtual machine can be co-hosted on the same physical machine. In spite of the wide range of benefits, co-hosting virtual machines on the same host comes with privacy and security threats. From one side, cloud providers are serving the virtual machines without being aware of their contents. On the other side, once cloud users submit their jobs to be serviced in the cloud, they lose their control on their jobs' sensitive information. Thus, cloud users' hesitation from moving to the cloud is logical since their sensitive jobs' content leakage or misuse is possible, especially when cloud services are not designed with privacy considerations. This paper proposes an approach to make the jobs with sensitive information more secure when submitted to the cloud environment. The core idea of the approach is to request the inclusion of the privacy specification of a set of one or more provider services in the Service Level Agreement contract.",10.1109/MENACOMM46666.2019.8988562,A. Al-Dulaimy; W. Itani; M. Shamseddine; J. Taheri
Improving Predictive Healthcare Analytics with Sophisticated Machine Learning Solutions on the Cloudera Platform,2025,-1,Outliers,0.4336932043147601,"Remote Patient Monitoring (RPM) is essential for proactive healthcare, particularly in chronic illness management and post-acute care. Predictive analytics improves RPM by detecting health problems early and personalizing treatments. Unified data architecture on the Cloudera Platform allows real-time intake, processing, and analytics across remote healthcare systems. Apache NiFi and Kafka gather streaming data from wearable devices, while Apache Spark and Cloudera Machine Learning transform and forecast. Cloudera Data Warehouse allows structured querying, while Cloudera DataFlow and Operational Database provide low-latency data access and alerts. HIPAA-compliant security and role-based governance protect sensitive patient data. The end-to-end architecture enables scalable, responsive, and intelligent RPM processes, improving clinical outcomes, hospital readmissions, and patient engagement via rapid data-driven insights. This technique has examined five Cloudera Platform variants: CDE, Cloudera Data Platform (CDP), Cloudera Data Warehouse (CDW), Cloudera Operational Database (COD), and Cloudera DataFlow (CDF). The Kaggle database displays three files with data from the Patient Side, Offline Doctor Survey, and Online Doctor Survey target audiences. This reveals that Cloudera Data Engineering (CDE) outperforms the other models. Sensitivity was 98.16 %, Specificity 98.59 %, Accuracy 98.37%, and F1- Score 98.37%.",10.1109/ICCSP64183.2025.11088450,A. N; S. M. A; U. Mageswari R; P. V. S; S. Inbarajan; E. Baskaran
Enhancing Digital Product Passport Through Decentralized Digital Twins,2025,-1,Outliers,0.26345758234507755,"The Digital Product Passport (DPP) is a key enabler of the European Union’s vision for a circular economy. Achieving the full potential of DPP requires addressing the challenges of traditional product lifecycle systems (PLM). Traditional PLM focuses on streamlining data management and decision making. However, their centralized architecture limits transparent, crossorganizational collaboration, impacting the circular economy efforts. This paper proposes a blockchain based framework, tailored to support DPP implementation by enabling the creation and sharing of lifecycle data using digital twin technology. The proposed architecture implements two types of digital twins - Component Digital Twin and Product Digital Twin modeled using the Asset Administration Shell (AAS) standard to ensure interoperability. The architecture leverages Ethereum smart contracts for blockchain interaction and IPFS for off-chain decentralized storage. Two approaches for secure data sharing are implemented: Direct and Signature-based data sharing. Performance evaluation shows low latency for key operations like twin creation (167 ms) and data sharing (64 ms). By leveraging decentralization in DPPs, the proposed framework fosters collaboration, transparency, and circular economy practices, empowering stakeholders to access and share critical product data throughout the lifecycle.",10.1109/NTMS65597.2025.11076730,R. Kannappan; J. Hatin; E. Bertin; N. Crespi
Constructing a Security Monitoring and Protection Mechanism for Power Grid Data Throughout its Life Cycle based on Graph Convolutional Networks,2025,2,Topic_2_data_privacy_security,0.24280976687247527,"With the rapid advancement of smart grids, ensuring the stable operation of the power system hinges on the full life cycle security monitoring and protection of power grid data. However, traditional approaches, characterized by heavy reliance on manual experience and isolated system architectures, struggle to cope with the increasingly intricate and dynamic risks of network attacks and data leakage. This paper introduces a graph convolutional network (GCN)-based full life cycle security monitoring and protection mechanism for power grid data. Through experimental testing, it is concluded that the protection mechanism based on GCN generally attains accuracy values between 0.6 and 0.9, significantly surpassing the 0.2 to 0.7 range of traditional protection mechanisms. This method demonstrates the capability to accurately identify abnormal behavior in power grid data and effectively resist adversarial attacks, offering a promising solution for enhancing the security and reliability of smart grids.",10.1109/ICDSIS65355.2025.11071168,G. Fu; M. He; Y. Chen; Y. Hu; X. Zhang
Safe and Secure Data Fusion — Use of MILS Multicore Architecture to Reduce Cyber Threats,2019,2,Topic_2_data_privacy_security,0.2803737906903194,"Data fusion, as a means to improve aircraft and air traffic safety, is a recent focus of some researchers and system developers. Increases in data volume and processing needs necessitate more powerful hardware and more flexible software architectures to satisfy these needs. Such improvements in processed data also mean the overall system becomes more complex and correspondingly, resulting in a potentially significantly larger cyber-attack space. Today's multicore processors are one means of satisfying the increased computational needs of data fusion-based systems. When coupled with a real-time operating system (RTOS) capable of flexible core and application scheduling, large cabinets of (power hungry) single-core processors may be avoided. The functional and assurance capabilities of such an RTOS can be critical elements in providing application isolation, constrained data flows, and restricted hardware access (including covert channel prevention) necessary to reduce the overall cyber-attack space. This paper examines fundamental considerations of a multiple independent levels of security (MILS) architecture when supported by a multicore-based real-time operating system. The paper draws upon assurance activities and functional properties associated with a previous Common Criteria evaluation assurance level (EAL) 6+ / High-Robustness Separation Kernel certification effort and contrast those with activities performed as part of a MILS multicore related project. The paper discusses key characteristics and functional capabilities necessary to achieve overall system security and safety. The paper defines architectural considerations essential for scheduling applications on a multicore processor to reduce security risks. For civil aircraft systems, the paper discusses the applicability of the security assurance and architecture configurations to system providers looking to increase their resilience to cyber threats.",10.1109/DASC43569.2019.9081638,P. Huyck
Adaptive Multi-Layered Authentication Framework for Secure and Individualized Access to Health Data Sets,2025,2,Topic_2_data_privacy_security,0.6777548729802126,"Because the magnitude of health data is increasing exponentially, it mandates a comprehensive mechanism to authenticate, secure, and trace accesses to sensitive patient data samples. Current authentication systems for data in the healthcare sector are bound by the static nature of their cryptographic protocols, centralized access control processes, and the very limited provenance analyses associated with access. The aforementioned gaps in patient-specific authentication, behavioral validation, and future-proofing against quantum threats leave such systems exposed to threats for data tampering, replay attacks, and unauthorized access, thereby affecting data integrity and non-repudiation guarantees. This document contains a detailed and original multi-layered framework for patient-specific data authentication, using five analytically grounded mechanisms to accompany it in process. The PHT-AEE enhances the integrity of and resistance to replay commands for dynamic hash trees tagged with entropy for the securing of the data chunks through a patient-centric hash tree with adaptive entropy encoding. Identity-bound homomorphic encryption includes patient biometric identity with a session timestamp to eliminate impersonation and ensure temporal validity within the process. The federated zero-trust proof with behavioural access drift uses federated learning integrated with an LSTM-based model of behavioural drift to enable zero-trust, behaviourally aware access control. The graph neural network for provenance integrity tracing builds and analyses data transformation paths that may have been tainted through contextual graph reasoning. Finally, the Quantum-Ledger Integrated Access Resolver consolidates future-resilient access authorization in post-quantum keys integrated with immutable blockchain records. Such integrated approach leads to tremendous advancements in access security, provenance accuracy, and cryptographic resilience. Collectively, the proposed methods enhance the integration of data integrity, non-repudiation, session trust, and legal auditability, establishing a future-ready authentication model tailored for scalable, secure, and personalized healthcare systems.",10.1109/ICSCDS65426.2025.11166873,R. Singh; A. Khan; N. Purohit; P. Lokulwar
Sticky-PRE: A Sticky Proxy Re-Encryption Protocol for Persistent Vehicle Data Privacy,2025,2,Topic_2_data_privacy_security,0.43106164275837644,"Connected vehicles generate a vast amount of data and share it with external entities such as the cloud, neighboring vehicles, Road-Side Units (RSUs), and other third-party services in a Vehicle to Everything (V2X) setting. This data is vulnerable and can lead to the leakage of personal information of vehicle owners, such as driving habits, travel routes, and identity theft, among others. Moreover, with the implementation of the General Data Protection Regulation (GDPR), it becomes imperative to empower users with control over their data and the ability to choose whom they share it with. To address this objective, we present a protocol that utilizes a combination of sticky policies and a proxy re-encryption scheme. This protocol ensures that user-defined access controls on the data persist even when crossing organizational boundaries and addresses the confidentiality, integrity, and accountability of vehicle data. Furthermore, we assess our protocol using a semi-honest threat model and analyze its vulnerabilities. Lastly, we perform a quantitative analysis of the data flow model to observe the system's performance.",10.1109/IV64158.2025.11097565,A. Ashutosh; O. Hasan; P. Baishnav; H. Kosch; L. Brunie
A Hierarchical Privacy-Preserving Framework for Heterogeneous Data Utilization in Satellite Internet,2025,2,Topic_2_data_privacy_security,1.0,"Satellite Internet generates vast data resources characterized by high sensitivity and complex scenarios, which pose significant challenges to data security and privacy protection. This paper proposes a hierarchical privacy-preserving framework for heterogeneous data utilization in Satellite Internet, aiming to enhance security while maximizing application value. The framework operates across two dimensions: the application scenario dimension and the data utilization dimension. For the former, five patterns are defined for intra-domain and crossdomain data utilization scenarios, each accompanied by tailored privacy protection strategies. For the latter, the framework encompasses three phases-data fusion, computation, and service-enabled by privacy-preserving methodologies. Furthermore, we analyze the applicability of existing privacypreserving techniques and propose future directions to support scalable Satellite Internet development.",10.1109/ICCCBDA64898.2025.11030413,S. Zhang; X. Peng; S. Lei; Z. Cao; Y. Shi; Y. Ma; X. Gu; H. Gao
Design and Research of Big Data Engine in Power and Energy System,2025,3,Topic_3_industry_manufacturing_chain,0.2223212128709796,"In view of the expanding amount of information in the current power and energy system, this paper designs a big data engine covering the functions of data access and collection, data cleaning, data analysis and conversion, machine learning and data model, data storage and synchronization, and business development interface, which can manage, analyze and support the realization of data assets throughout their life cycle. The big data engine designed in this paper can be deployed in any type of power and energy systems as a unified basic platform to meet the data quality needs of various intelligent applications in power and energy systems, and provide a solid foundation for business expansion.",10.1109/ACPEE64358.2025.11041188,X. Liang; Y. Jiang; X. Zhong; H. Pan; F. Liang; J. Shen
Traceability Data in the form of Digital Food Product Passports for Fish Supply Chains,2025,-1,Outliers,0.29229576247475314,"The idea of a Digital Product Passport (DPP) is to provide access to product specific data for actors and consumers in supply chains to satisfy different needs. Solutions for DPP exist in sectors other than food. However, literature on Digital Food Product Passport (DFPP) is very limited, and there are no clear guidelines regarding what data should be contained in a DFPP. Various platforms for traceability exist and traceability data obtained along food supply chains can be used as a fundamental data basis for DFPPs. In this paper we define what data to be included in a DFPP at different steps of the supply chains based on a collection of end-to-end traceability data. The focus is on how to use such data systematically in a DFPP and how to support the different stakeholders involved. We demonstrate how this approach can be applied to implement a blockchain-based DFPP for a fish supply chain.",10.1109/EEITE65381.2025.11166326,S. Jiang; M. K. Natvig; T. Bakkejord Ræder; P. Bourgos; N. Tsampanaki
Hierarchical Blockchain for Mapping Manufacturing Process Flow,2025,-1,Outliers,0.29092384914944647,"As manufacturing processes become increasingly complex, maintaining quality and improving efficiency requires mapping of process flows. Mapping process flows, in turn, depends on comprehensive end-to-end data traceability. Such traceability relies on lifecycle data that capture every stage, from raw-material handling to final-product assembly, and provide indispensable insights for process refinement. However, conventional centralized database-based systems for managing these data introduce single points of failure and remain vulnerable to tampering and cyberattacks. As a result, data traceability and authenticity are compromised. Therefore, this research develops a novel blockchain architecture coupled with digital twin (DT) model to secure end-to-end documentation of manufacturing process flows. First, a hierarchical blockchain framework is developed to record production events and ensure comprehensive, tamper-proof records of process activities. Second, the DT model, operating in collaboration with the blockchain tiers, enables real-time alignment between the manufacturing floor and its virtual twin. Third, a unified data representation is designed to transform diverse manufacturing datasets into a homogeneously structured format. Experimental results show that the proposed framework significantly enhances data authenticity while reducing the time required to map manufacturing process flows.",10.1109/CASE58245.2025.11164079,T. Kuo; H. Yang
Semantic Integration in Big Data: State-of-the-Art,2019,1,Topic_1_data_big_big data,1.0,"Nowadays, web users and systems continually overload the web with an exponential generation of a massive amount of data. This leads to making big data more important in several domains such as social networks, internet of things, health care, E-commerce, aviation safety, etc. The use of big data has become increasingly crucial for companies due to the significant evolution of information providers and users on the web. However, big data remain meaningless without semantics. In order to get a good comprehension of big data, we raise questions about how big data and semantic are related to each other and how semantic may help. To overcome this problem, researchers devote considerable time to the integration of ontology in big data to ensure reliable interoperability between systems in order to make big data more useful, readable and exploitable. This technology can hide the heterogeneity of different data resources. Moreover, in given domains, users can exchange knowledge without caring to choose the suitable semantic that makes their content more expressive. This paper aims to provide a comprehensive overview for readers about big data and the appropriate tools to manipulate and analyse them such as Hadoop. Afterwards, we talk about ontology and how it can be used to improve big data management and analyses for decision makers. Finally, different semantic integration approaches are seen in a comparative study. This survey is concluded with a discussion and some perspectives.",10.13052/jmm1550-4646.1533,Z. Sayah; O. Kazar; A. Ghenabzia
From Static Records to Smart Passports: Evolving Digital Product Passports Toward Product-Service System Integration,2025,-1,Outliers,0.255491029220696,"The Digital Product Passport (DPP) has emerged as a transformative enabler for the circular economy, sustainability, and regulatory compliance across supply chains. While recent advancements have focused on improving traceability, standardization, and lifecycle data aggregation, most existing systems remain static and product-centric. In this work, we do a comprehensive state-of-the-art review of current DPP frameworks, technologies, and research trends, drawing from literature and EU research and innovation projects. We identify limitations in current architectures with respect to service integration, real-time data synchronization, modularity, and stakeholder trust. As a response, we introduce DPSSP4.0, a novel architecture for dynamic, decentralized, and service-aware Digital Product-Service System Passports (DPSSPs). The proposal builds upon the Asset Administration Shell (AAS) standard and incorporates microservices, semantic data spaces, and certification layers to ensure extensibility, interoperability, and trust. This positions DPSSP4.0 as a forward-looking enabler of Smart Product-Service Systems in the Industry 4.0 era.",10.1109/DCOSS-IoT65416.2025.00163,P. Maló; B. Almeida; M. Mateus; F. Querido; D. Inácio; T. Teixeira; G. Di Orio; F. Marques
Enabling AI in Future Wireless Networks: A Data Life Cycle Perspective,2021,-1,Outliers,0.4350205289607237,"Recent years have seen rapid deployment of mobile computing and Internet of Things (IoT) networks, which can be mostly attributed to the increasing communication and sensing capabilities of wireless systems. Big data analysis, pervasive computing, and eventually artificial intelligence (AI) are envisaged to be deployed on top of the IoT and create a new world featured by data-driven AI. In this context, a novel paradigm of merging AI and wireless communications, called Wireless AI that pushes AI frontiers to the network edge, is widely regarded as a key enabler for future intelligent network evolution. To this end, we present a comprehensive survey of the latest studies in wireless AI from the data-driven perspective. Specifically, we first propose a novel Wireless AI architecture that covers five key data-driven AI themes in wireless networks, including Sensing AI, Network Device AI, Access AI, User Device AI and Data-provenance AI. Then, for each data-driven AI theme, we present an overview on the use of AI approaches to solve the emerging data-related problems and show how AI can empower wireless network functionalities. Particularly, compared to the other related survey papers, we provide an in-depth discussion on the Wireless AI applications in various data-driven domains wherein AI proves extremely useful for wireless network design and optimization. Finally, research challenges and future visions are also discussed to spur further research in this promising area.",10.1109/COMST.2020.3024783,D. C. Nguyen; P. Cheng; M. Ding; D. Lopez-Perez; P. N. Pathirana; J. Li; A. Seneviratne; Y. Li; H. V. Poor
Performance Evaluation of an Internet of Healthcare Things for Medical Monitoring Using M/M/c/K Queuing Models,2021,1,Topic_1_data_big_big data,0.8899042763026511,"Due to the non-stop and rapid spreading of virus pandemics all over the world, traditional healthcare monitoring capabilities of hospitals and/or medical centers are under a severe over-load. Modern computing infrastructures with the harmony of various layers of computing paradigms (e.g., cloud/fog/edge computing) for healthcare monitoring are apparently the essential computing backbone that help access and process instantly the medical data of every single patient at the very edge of the healthcare system to combat with global or regional virus contagion. Previous studies proposed different computing system architectures for healthcare monitoring but few works considered the evaluation of pure performance of medical data transmission in a comprehensive manner. In this paper, we proposed an M/M/c/K queuing network model for the performance evaluation of an Internet of Healthcare Things (IoHT) infrastructure in association with a three layer cloud/fog/edge computing continuum. The model considers a life cycle of medical data from body-attached IoT sensors in edge layer all the way to local clients (e.g., local medical doctors, physicians) through fog layer and to remote clients (e.g., medical professionals, patient's family members) through cloud layer. Furthermore, we also explore the impact of the alteration in system configuration and computing capability of computing layers in two scenarios on various performance metrics. Critical performance metrics related to quality of service are evaluated in a comprehensive manner, such as (i) mean response time of medical data transmission to fog (local) clients and to cloud (remote) clients, (ii) utilization of cloud/fog/edge computing layers, (iii) service throughput, (iv) number of medical messages in a period of time, and (v) drop rate. The simulation results pinpoint bottle-neck parameters and configurations of the IoHT infrastructure's system architecture in relation to the frequency of medical data collection for health check of patients. Thus, the findings of this study can help improve medical administration in hospitals and healthcare centers and help design computing infrastructures in accordance for medical monitoring in the severe circumstances of virus pandemics.",10.1109/ACCESS.2021.3071508,F. A. Silva; T. A. Nguyen; I. Fé; C. Brito; D. Min; J. -W. Lee
An Ontological-Based Model to Data Governance for Big Data,2021,1,Topic_1_data_big_big data,0.700207953745625,"Nowadays, companies and official bodies are using the data as a principal asset to take strategic decisions. The advances in big data processing, storage and analysis techniques have allowed to manage the continuous increase in the volume of data. This increase in the volume of data together with its high variability and the large number of sources lead to a constant growing of the complexity of the data management environment. Data governance is the key for simplifying that complexity: it is the element that controls the decision making and responsibilities for all the processes related to data management. This paper discusses an approach to data governance based on ontological reasoning to reduce data management complexity. The proposed data governance system is built over an autonomous system based on distributed components. It implements semantic techniques and automatic ontology-based reasoning. The different components use a Shared Knowledge Plane to interact. Its fundamental piece is an ontology that represents all the data management processes included in data governance. A prototype of such a system has been implemented and tested for Telefonica's global video service. The results obtained show the feasibility of using this type of technology to reduce the complexity of managing big data environments.",10.1109/ACCESS.2021.3101938,A. Castro; V. A. Villagrá; P. García; D. Rivera; D. Toledo
Multi-Instance Deep Learning Based on Attention Mechanism for Failure Prediction of Unlabeled Hard Disk Drives,2021,0,Topic_0_prediction_degradation_rul,1.0,"Failure of hard disk drives (HDDs) is the most critical reliability issue of data center. Therefore, predicting the failure of the HDD is an important means to ensure the storage security of the data center. However, most current research works had not paid attention to the fact that the self-monitoring, analysis and reporting technology (SMART) data in a returned failed HDD are a long-term sequence that consists of many unlabeled data, as the healthy and faulty data are highly mixed. Because the failure data in the rapid degradation period are less than the health data in the normal state, the mixture of healthy and faulty data results in an extremely data imbalance. This brings a great challenge to find the hidden fault information, and thus failure prediction becomes a difficult task. To cope with the above problems, a multi-instance long-term data classification method based on long short-term memory (LSTM) network and attention mechanism are proposed to predict the failure of HDDs. Regarding long time sequence HDD data as an instance bag, multi-instance learning (MIL) divides it into multiple instances in the subconcept layer, and then studies the connection between instances and bag labels. Based on the analysis of HDD data of a communication company and Backblaze data center, our proposed method can obtain much better results than other methods.",10.1109/TIM.2021.3068180,G. Wang; Y. Wang; X. Sun
Privacy in the Cloud: A Survey of Existing Solutions and Research Challenges,2021,2,Topic_2_data_privacy_security,1.0,"Private data is transmitted and stored online every second. Therefore, security and privacy assurances should be provided at all times. However, that is not always the case. Private information is often unwillingly collected, sold, or exposed, depriving data owners of their rightful privacy. In this article, various privacy threats, concepts, regulations, and personal data types are analyzed. An overview of Privacy Enhancing Technologies (PETs) and a survey of anonymization mechanisms, privacy tools, models, and metrics are presented together with an analysis of respective characteristics and capabilities. Moreover, this article analyses the applicability of the reviewed privacy mechanisms on today's Cloud Services and identifies the current research challenges to achieve higher privacy levels in the Cloud.",10.1109/ACCESS.2021.3049599,P. Silva; E. Monteiro; P. Simões
Systematic Mapping of Open Data Studies: Classification and Trends From a Technological Perspective,2021,1,Topic_1_data_big_big data,0.4653099313029787,"The objective of this paper is to classify and analyse all research on open data performed in the scientific community from a technological viewpoint, providing a detailed exploration based on six key facets: publication venue, impact, subject, domain, life-cycle phases and type of research. This paper therefore provides a consolidated overview of the open data arena that allows readers to identify well-established topics, trends, and open research issues. Additionally, we provide an extensive qualitative discussion of the most interesting findings to pave the way for future research. Our first identification phase resulted in 893 relevant peer-reviewed articles, published between 2006 and 2019 in a wide variety of venues. Analysis of the results shows that open data research grew slowly from 2006 but increased significantly as from 2009. In 2019, research interest in open data from a technological perspective overall decreased. This fact could indicate that research is beginning to stabilise, i.e., the open data research hype is over, and the research field is reaching maturity. Main findings are (i) increasing effort in researching on Semantic Web technologies as a mechanism to publish and reuse linked open data, (ii) software systems are proposed to solve open data technical problems; and (iii) considering technological aspects of legislation and standardization is needed to widely introduce open data in society. Finally, we provide complementary insights regarding open data innovation projects, with special emphasis on publication (e.g., open data portals) and consumption (e.g., open data as business enabler) of open data.",10.1109/ACCESS.2021.3052025,R. Enríquez-Reyes; S. Cadena-Vela; A. Fuster-Guilló; J. -N. Mazón; L. D. Ibáñez; E. Simperl
Data Life Aware Model Updating Strategy for Stream-Based Online Deep Learning,2021,1,Topic_1_data_big_big data,0.5160064693417103,"Many deep learning applications deployed in dynamic environments change over time, in which the training models are supposed to be continuously updated with streaming data to guarantee better descriptions of data trends. However, most state-of-the-art learning frameworks support well in offline training methods while omitting online model updating strategies. In this work, we propose and implement iDlaLayer, a thin middleware layer on top of existing training frameworks that streamlines the support and implementation of online deep learning applications. In pursuit of good model quality and fast data incorporation, we design a Data Life Aware model updating strategy (DLA), which builds training data samples according to contributions of data from different life stages, and considers the training cost consumed in model updating. We evaluate iDlaLayer's performance through simulations and experiments based on TensorflowOnSpark with three representative online learning workloads. Our experimental results demonstrate that iDlaLayer reduces the overall elapsed time of ResNet, DeepFM and PageRank by 11.3, 28.2, and 15.2 percent compared to the periodic update strategy, respectively. It further achieves an average 20 percent decrease in training cost and brings about a 5 percent improvement in model quality against the traditional continuous training method.",10.1109/TPDS.2021.3071939,W. Rang; D. Yang; D. Cheng; Y. Wang
Research Data Management and Data Stewardship Competences in University Curriculum,2021,-1,Outliers,0.14815506356434469,"Skills for data governance and management are critical for the wide adoption of Open Science practices and effective use of the data in research, industry, business and other economic sectors. The FAIR (Findable - Accessible - Interoperable - Reusable) data management principles and data stewardship provide a foundation for effective research data management. The 2018 “Turning FAIR into Reality” report and other documents recommend that data skills should be more widely included in university curricula and that a concerted effort should be made to coordinate and accelerate the pedagogy for professional data roles. Throughout Europe and beyond, many organisations, projects and initiatives work on providing training on FAIR data competences. However, wider adoption of the FAIR data culture can be achieved by including FAIR competences into university curricula. This paper presents the ongoing work of the FAIRsFAIR project to develop a Data Stewardship competence framework and to provide recommendations for implementing this framework in university curricula by means of defining the Data Stewardship Body of Knowledge Model Curricula. The proposed approach and identified competences and knowledge topics are supported by a job market analysis. The presented work is actively using the EDISON Data Science Framework as a basis for Data Stewardship competences definition and methodology for linking competences, skills, knowledge, and intended learning outcomes when designing curricula.",10.1109/EDUCON46332.2021.9453956,Y. Demchenko; L. Stoy
An Architecture and Information Meta-model for Back-end Data Access via Digital Twins,2021,-1,Outliers,0.1562564307223091,"The lifecycle data of industrial devices is typically maintained in separate data sources operating in silos. The lack of interoperability between the data sources due to the usage of different APIs, data formats and data models results in time-consuming and error-prone manual data exchange efforts. The notion of digital twins is known to be a solution to the data silo problem and the associated interoperability issues. Despite many studies on digital twins, there is still a need for common architectures that offer means for defining digital twins, ingesting backend data in the digital twins, and enabling interoperable data exchange across the lifecycle of the devices via their digital twins. This paper aims to close this gap by proposing a cloud-based architecture, a common information meta-model for defining the digital twins, and variety of APIs to query the lifecycle data of interest via the digital twins.",10.1109/ETFA45728.2021.9613724,S. Malakuti; P. Juhlin; J. Doppelhamer; J. Schmitt; T. Goldschmidt; A. Ciepal
One Data ASN Framework (ODAF) for Indonesian State Civil Apparatus,2021,1,Topic_1_data_big_big data,0.3507066524106286,"Data management and governance framework purpose to support data life cycle and ensure the consistency within an organizational function in terms of framework implementation to achieve organizational objectives. Data accuracy, integrity, interoperability, and quality become a challenge that occurs in One Data State Civil Apparatus (ASN) implementation. Therefore, it is needed to define a suitable framework for ASN data management and governance. This research conducted identify strategic issues and current problems, literature review and previous study to determine the theoretical framework, evaluation the theoretical framework, hybrid, and synthesis framework to define the suitable component, and validation the proposed framework by expert judgment. The One Data ASN Framework (ODAF) consists of eight components there are regulation/policy, vision, mission, goals, principles, committee, type, standard, metadata, data organizing, data dissemination, coaching, monitoring, and evaluation.",10.1109/ICIMCIS53775.2021.9699290,E. Cahyaningsih; N. L. E. Silalahi; H. Noprisson; V. Ayumi
A Usability Study on Data Provenance Visualization Approaches,2021,1,Topic_1_data_big_big data,0.4469995453073107,"Data visualization becomes an arising need to the end-users to assess, analyze, and reveal the data&#x0027;s insights, especially in the Big Data age. The amount of data generated by platforms and systems over the world increases rapidly. As an illustration, in Social Media and e-Science, domain data increases at a tremendous amount. Data provenance is a sort of metadata on the origin and life cycle of data is the issue of both domains for data analysis. Therefore, this requires further researches to ensure better understanding and efficient data interpretation. Our former comprehensive study has addressed several visualization approaches to meet these needs, such as data summarization and graph comparison in the e-Science domain. Proposed approaches implemented into a visualization tool to illustrate their applicability for the end-users. One of the objections was to involve end-users in the visualization process as much as possible. In this study, we present a detailed user study to assess our implemented visualization environment concerning Human Machine Interface (HMI) from the end-user perspective. Implemented visualization approaches are evaluated using task-level satisfaction and general-level transaction user surveys with well-defined study cases. In addition, System Usability Scale metrics are applied to measure the SUS score of the overall visualization system. The user study experiments proof that the proposed visualization approaches are applicable, and processing overhead is significantly low by facilitating the visualization system.",10.1109/UYMS54260.2021.9659779,I. M. Yazici; M. S. Aktas
Science Capsule: Towards Sharing and Reproducibility of Scientific Workflows,2021,1,Topic_1_data_big_big data,0.6794601207643491,"Workflows are increasingly processing large volumes of data from scientific instruments, experiments and sensors. These workflows often consist of complex data processing and analysis steps that might include a diverse ecosystem of tools and also often involve human-in-the-loop steps. Sharing and reproducing these workflows with collaborators and the larger community is critical but hard to do without the entire context of the workflow including user notes and execution environment. In this paper, we describe Science Capsule, which is a framework to capture, share, and reproduce scientific workflows. Science Capsule captures, manages and represents both computational and human elements of a workflow. It automatically captures and processes events associated with the execution and data life cycle of workflows, and lets users add other types and forms of scientific artifacts. Science Capsule also allows users to create &#x0027;workflow snapshots&#x0027; that keep track of the different versions of a workflow and their lineage, allowing scientists to incrementally share and extend workflows between users. Our results show that Science Capsule is capable of processing and organizing events in near real-time for high-throughput experimental and data analysis workflows without incurring any significant performance overheads.",10.1109/WORKS54523.2021.00014,D. Ghoshal; L. Bianchi; A. Essiari; D. Paine; S. S. Poon; M. Beach; A. T. N'Diaye; P. Huck; L. Ramakrishnan
A survey on privacy preservation in video big data,2021,2,Topic_2_data_privacy_security,1.0,"The explosive growth of video data application raises the risk of privacy leaks in the video big data environment. Compared with other data formats, video big data presents special characteristics including various representation, high dimension, complex content, and so on, making the privacy protection for video big data more difficult. This paper analyzes the life cycle and overall architecture of video big data, and then discusses the privacy preservation method recently developed for video applications. These methods are categorized into three types: privacy preservation method for video storage, for video analysis, and for video release. Via analyzing their application scenarios and technique concepts, we finally come to a conclusion about their advantages, disadvantages, and possible applications, and present several research issues for video big data privacy preservation.",10.1109/ICECCME52200.2021.9591105,B. Feng; Y. Lin; T. Xu; J. Duan
Data-driven Development of Digital Health Applications on the Example of Dementia Screening,2021,-1,Outliers,0.3767646893664328,"Following the paradigm of precision medicine, the combination of health data and Machine Learning (ML) is promising to improve the quality of healthcare services e.g. by making diagnoses and therapeutic interventions as early and precise as possible. The implementation of this approach requires sufficient amounts of data with a high quality along the data life cycle. This goal seems recently achievable through the implementation of several national digital health strategies and the hope of a growing societal acceptance of digital health applications due to the implications of the COVID-19 pandemic. But, a collection of tools and methods is missing, which supports developers to use data as driving force of the development process. Due to the iterative nature of software application development, it allows the continuous improvement through the integration of collected digital data. We refer to this as a data-driven approach and identify steps to take and tools for its implementation. Associated challenges and opportunities of this translational approach are outlined on the example of a self-developed dementia screening application. Using our methodology, we compared multiple ML algorithms based on the data of an observational study (n=55) and achieved models with sensitivity up to 89% for unhealthy participants within this use case.",10.1109/MeMeA52024.2021.9478676,M. Schinle; C. Erler; T. Schneider; J. Plewnia; W. Stork
Research on Security and Privacy Problem in the Data Life Cycle for the IoT Scenario,2021,2,Topic_2_data_privacy_security,0.8117975363747283,"Sensors have been deployed into different scenarios to collect data, including health data, environmental data, etc. Data have been collected, transmitted, analyzed, etc. Those data are highly related to people's privacy, protecting data privacy becomes necessary. Different methods have been applied to protect data privacy during the life cycle in the Internet of Things scenarios. At the data collecting phase, data aggregation methods are proposed. At the data transmission phase, mutual authentication and key establishment schemes are proposed to help entities to build a secure two-way communication channel, data can be transmitted securely. At the data analyzing phase, privacy-preserving machine learning methods have been discussed, including collaboratively learning and other encrypted machine learning as a service technology, they can protect users' data privacy at the training phase and inference phase respectively. In this study, we mainly discussed these kinds of methods for protecting data security and privacy in the Internet of Things scenario.",10.1109/ASSP54407.2021.00021,S. Yang; Y. Chen; Z. Yang
Scientific Workflow Provenance Architecture for Heterogeneous HPC Environments,2021,1,Topic_1_data_big_big data,0.790766599832349,"Provenance in computing systems is the key to establishing data integrity. It provides a historical ledger of data's life cycle through creation, ownership, consumption, and manipulation. With provenance in hand, it is possible to reverse engineer the state of the data that can lead to understanding how it was derived and verify its accuracy. This need for data integrity is extremely critical in scientific workflows to ensure verifiability and repeatability of the derived results. Due to the vast computational power required by scientific workflows, many operate within high performance computing (HPC) environments, where data is consumed and manipulated by a multitude of processes running on highly distributed infrastructure. The current landscape of HPC environments range from on-premise systems to cloud and grid based solutions. While the majority of research in digital provenance has been focused on standalone HPC environments, provenance in a heterogeneous HPC environment remains a challenge. In this paper we propose HyperProvenance, a high level system architecture especially for next generation heterogeneous HPC environments, which aims to increase confidence in workflow result accuracy through secure provenance collection.",10.1109/IEMCON53756.2021.9623106,A. Williams; D. K. Tosh
Zone Based Writing Optimization in User Space,2021,1,Topic_1_data_big_big data,0.7669647043436307,"Write operation of the latest storage media such as NAND SSDs has certain restrictions. Data can only be written in an additional write mode. When erasing, some valid data still needs to be written again, which will cause write amplification. Researches about write amplification focused on conventional NVMe protocol and did not consider some forward features. In response to this problem, this paper designs a user-mode write operation optimization algorithm based on the dynamic adjustment of zone space. When the block is erased, the impact caused by write amplification can be significantly resolved, and operations of multiple tenants can be effectively isolated. The Zone Based Algorithm will implement a mapping algorithm based on zone space, and select a suitable GC to maximize the performance. We also designed and implemented a variety of mapping algorithms and GC algorithms to improve the processing efficiency of IO requests. Experiments based on FEMU show at least 25% improvement when increasing data of different life span.",10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00104,R. Shen; Y. Liu; R. Zhang; J. Zhu; Z. Liu; L. Xiao
Generalized Variable-Step Multiscale Lempel-Ziv Complexity: A Feature Extraction Tool for Bearing Fault Diagnosis,2022,0,Topic_0_prediction_degradation_rul,0.8941679765596418,"Feature extraction is a key step for fault diagnosis of rolling bearings based on vibration measurements. Lempel-Ziv complexity (LZC) has been widely applied to extracting fault features. However, LZC with single-scale analysis may result in the loss of fault information. Then, the multiscale LZC is proposed to uncover the multiscale features. Nevertheless, multiscale LZC would shorten the length of sequences, leading to inaccurate calculation results. Moreover, the mean operation during the coarse-grained process of the traditional multiscale analysis cannot reveal the dynamic changes of the original sequences, which may also result in the loss of potential information. Thus, this paper develops a feature extraction method called generalized variable-step multiscale LZC (GVSMLZC) for the vibration-based fault diagnosis of rolling bearings. GVSMLZC reveals more information and gains more robust complexity by improving the coarse-grained process. Moreover, GVSMLZC enhances the capability of mining dynamic characteristics by generalizing first-order moments to second-order moments. Based on the proposed GVSMLZC, fault type identification and fault detection of life-cycle data schemes are designed to realize the fault diagnosis of rolling bearings. Simulated and experimental results demonstrate that GVSMLZC outperforms multiscale LZC, variable-step multiscale LZC, and multiscale sample entropy in extracting bearing fault information.",10.1109/JSEN.2022.3187763,J. Shi; Z. Su; H. Qin; C. Shen; W. Huang; Z. Zhu
An Early Fault Detection Method of Rotating Machines Based on Unsupervised Sequence Segmentation Convolutional Neural Network,2022,0,Topic_0_prediction_degradation_rul,0.8373018440844383,"Early fault detection (EFD) is vital for mechanical systems to reduce downtime and increase stability. The main challenge of EFD for rotating machines is to extract discriminative features from noisy signals to identify early faults. However, the lack of labels for the whole lifecycle data hinders the application of some powerful supervised deep learning methods in EFD. Besides, many EFD methods have to set a criterion manually, such as a threshold, to judge whether an early fault has occurred. To address these challenges, this article proposes a novel EFD method based on unsupervised sequence segmentation convolutional neural network (USSCNN). At first, frequency-domain features are extracted from raw signals and converted to 2-D gray images. Then, historical lifecycle data are labeled by USSCNN so that a CNN classifier can be trained with these labeled data. The deep features of the historical data learned by the CNN classifier are utilized to train the health index (HI) assessment model. The proposed method is tested on three bearing datasets. The results have shown that the proposed method can detect incipient faults earlier than the comparing methods with lower false alarms. Also, the HIs learned by the HI assessment model shown that the proposed method can extract discriminative features for EFD. More importantly, the proposed method can detect an early fault by the well-trained classifier, which avoids manual criterion-making. Results of comparison demonstrated the effectiveness and the robustness of the proposed method.",10.1109/TIM.2021.3132989,W. Song; W. Shen; L. Gao; X. Li
Health Indicator Construction Method of Bearings Based on Wasserstein Dual-Domain Adversarial Networks Under Normal Data Only,2022,0,Topic_0_prediction_degradation_rul,1.0,"Rolling bearings are the most critical parts of rotating machinery and their damage is the leading cause of system failures. To ensure the reliability of the system, it demands to construct a health indicator (HI) to assess the state of degradation. However, existing HI construction methods (HICMs) have two limitations. First, the integration of well-designed features relies heavily on the experience of domain expert knowledge. Second, the construction of intelligent HI relies too much on life-cycle data. To cope with these limitations, this article proposed an HICM–Wasserstein dual-domain adversarial networks (WD-DAN), namely HICM-WD-DAN, which can extract generalized features with only normal data during the training. The dual-domain restriction of regularization promotes the generated signals approach to normal samples, making the constructed HI more robust and accurate. Moreover, to balance the weights of dual-domain parts automatically, an independent weighting structure is introduced. Finally, considering the actual degradation state of the system, the modified monotonicity and trendability indexes are proposed to evaluate the performance of HI. The effectiveness of HICM-WD-DAN is verified by bearings’ life-cycle data, and the results show that the constructed HI can represent the irreversible degradation process of bearings accurately and monotonously.",10.1109/TIE.2022.3156148,J. Li; Y. Zi; Y. Wang; Y. Yang
Prognostics for Electromagnetic Relays Using Deep Learning,2022,0,Topic_0_prediction_degradation_rul,1.0,"Electromagnetic Relays (Electromagnetic Relay (EMR)s) are omnipresent in electrical systems, ranging from mass-produced consumer products to highly specialised, safety-critical industrial systems. Our detailed literature review focused on EMR reliability highlighting the methods used to estimate the State of Health or the Remaining Useful Life emphasises the limited analysis and understanding of expressive EMR degradation indicators, as well as accessibility and use of EMR life cycle data sets. Prioritising these open challenges, a deep learning pipeline is presented in a prognostic context termed Electromagnetic Relay Useful Actuation Pipeline (EMRUA). Leveraging the attributes of causal convolution, a Temporal Convolutional Network (TCN) based architecture integrates an arbitrary long sequence of multiple features to produce a remaining useful switching actuations forecast. These features are extracted from raw, high volume life cycle data sets, namely EMR switching data (Contact-Voltage, Contact-Current). Monte-Carlo Dropout is utilised to estimate uncertainty during inference. The TCN hyperparameter space, as well as various methods to select and analyse long sequences of multivariate time series data are investigated. Subsequently, our results demonstrate improvements using the developed statistical feature-set over traditional, time-based features, commonly found in literature. EMRUA achieves an average forecasting mean absolute percentage error of ±12 % over the course of the entire EMR life.",10.1109/ACCESS.2022.3140645,L. Kirschbaum; V. Robu; J. Swingler; D. Flynn
Privacy by Design: A Microservices-Based Software Architecture Approach,2022,2,Topic_2_data_privacy_security,1.0,"Data privacy regulations have increased significantly recently. As a result, privacy by design (PbD) has become a critical consideration for enterprises that handle personal data. PbD is no longer a plain principle. Rather than that, the General Data Protection Regulation (GDPR) addresses PbD as a required legal requirement for controllers who may face fines for non-compliance with the GDPR. In this paper, we propose a practical solution, “PbD Microservice,” that can help organizations to achieve privacy regulatory compliance. We will focus on GDPR, one of the most important regulations that provides a high level of protection and control over personal data. The proposed solution depends on a microservices-based software architecture approach, which bridges the gap between data privacy regulation requirements from one side, and the system architecture and design, from the other. Moreover, the provided practical solution complies with the PbD and GDPR principles.",10.1109/MIUCC55081.2022.9781685,B. Mashaly; S. Selim; A. H. Yousef; K. M. Fouad
Data Security Risk Assessment Method for Connected and Automated Vehicles,2022,2,Topic_2_data_privacy_security,0.36937599723735026,"Considering the data security problems of connected and automated vehicles (CAVs), this paper analyzes the security vulnerability and risk challenges for the communication data in vehicles. Based on state of the art technical manuals, we explore the security reverse engineering technology, and propose the risk assessment method for data security of CAVs. Considering the impact level of data security for the national security, public interests, personal legitimate rights and privacy, we present the possibility evaluation level of data security and get the security risk value of ICVs. The proposed method has been verfied and provides risk assessment results of the vehicle component, i.e., T-box. Finally, the suggestion to construct an effective security protection policies for data security is put forward, and the method for the research of ICV data security is provided to push the development of automotive security system.",10.1109/ICITE56321.2022.10101389,S. Zhou; X. Yang; M. Li; H. Yang; H. Ji
Error and Uncertainty in Earth Observation Value Chains,2020,-1,Outliers,0.16554148278225309,"Earth observation systems are providing a growing amount of high resolution and multi-spectral data including agricultural crop yield predictions, local weather forecasts, wild fire tracking, and water quality monitoring. With the advent of multiple elements and sophisticated processing and modeling, there are also increasing avenues for introduction of errors. It is important to quantify and characterize these errors in the remotely sensed data as it gets increasingly used to inform vital decisions. Here, a data value-chain approach is presented for conceptualizing introduction and propagation of errors in remote sensing data acquisition, processing, and decisions. A detailed case of calibration errors and their propagation to higher level data products is then discussed. An NDVI analysis is used as an example, and it is shown (for the selected region), that a 3% error in reflectance ratio of red and NIR bands translates into a difference of 60 % for category 5 NDVI classification (representing high vegetation). This amplification of errors along the data value chain is caused by nonlinear operators such as the application of quotients to differences of reflectance values as well as the use of classifiers based on fixed color thresholding.",10.1109/IGARSS39084.2020.9323463,A. Siddiqi; S. Baber; O. de Weck; C. Durell
Defining an Initial Classification Scheme for Non-Deterministic AI Technologies,2022,-1,Outliers,0.20855923888143676,"A growing number of aviation applications using Artificial Intelligence (AI) and Machine Learning (ML) algorithms demonstrate how these methods may increase the accuracy of predictions and identify trends or correlations that are difficult for humans to recognize. This is particularly true when the patterns are buried and must be extracted from a large volume of data samples. While the potential benefits of AI and ML are clear for some applications, the inherent difficulty in explaining the behavior of these algorithms presents challenges for potential certified use in civil aviation. If one cannot explain the AI decision or predict the ML output, one cannot provide guarantees on safety or assurances on system performance.Traditional software and system assurance processes rely on mathematical stability, behavior bounded by physics, and/or deterministic system dynamics that may not be applicable to AI systems. For example, a typical method used in verification is to stress test the system on all possible input values and check/verify if the key system parameters (and output variables) stay within prescribed bounds (i.e., safety limits). Current assurance processes also rely primarily on safety assurance thresholds, which drive validation and verification requirements for approval. While safety is a major factor in determining requirements, other factors addressing the unique technical aspects (e.g., non-deterministic and autonomous) and functional aspects (e.g., purpose and role) of AI technologies should also be considered. Thus, exploring an initial AI classification scheme is a crucial step in maturing research for the future acceptance and approval of different types of AI-driven services. Existing efforts (e.g., European Union Aviation Safety Agency (EASA)) to address requirements for AI technologies have begun to define roadmaps based primarily on level of autonomy. However, our work goes beyond the ""level"" of human/system interaction by considering a formal structure based on both technical and functional aspects of AI. A multi-facet classification scheme may also facilitate low-risk, high-value, advisory AI use cases by assigning an appropriate level of rigor.The goal of our work is to propose, research, and analyze a classification scheme considering both technical and functional AI characteristics that can be applied to a wide range of AI technologies. Our aim is to document gaps and challenges in defining a robust classification scheme. In our methodology, we employ a three-fold approach. First, based on literature review and subject matter expertise, we identify and define components that form the classification scheme. These classification components include the functionality (i.e., functional criticality) of the application being developed, the role of the AI with respect to a human user (e.g., advisory), and the trustworthiness (e.g., computational complexity) of the AI method utilized. Each of these components will be categorized into factors that can be considered against a variety of AI applications. Second, we develop relationships to describe how the classification scheme may be used to inform validation and verification approaches. We focus on how each of the components may influence the potential level of rigor needed for approval or acceptance of the AI technology. Third, we support our classification scheme by aligning the classifications with realistic aviation use cases.",10.1109/ICNS54818.2022.9771499,K. T. Tejasen; P. Raju; W. Ryan; A. Jordan; R. Yang
Pseudonymization at Scale: OLCF’s Summit Usage Data Case Study,2022,1,Topic_1_data_big_big data,0.7210526485593396,"The analysis of vast amounts of data and the processing of complex computational jobs have traditionally relied upon high performance computing (HPC) systems, which offer reliable and efficient management of large-scale computational and data resources. Understanding these analyses’ needs is paramount for designing solutions that can lead to better science, and similarly, understanding the characteristics of the user behavior on those systems is important for improving user experiences on HPC systems. A common approach to gathering data about user behavior is to extract workload characteristics from system log data available only to system administrators. Recently at Oak Ridge Leadership Computing Facility (OLCF), however, we unveiled user behavior about the Summit supercomputer by collecting data from a user’s point of view with ordinary Unix commands.In this paper, we discuss the process, challenges, and lessons learned while preparing this dataset for publication and submission to an open data challenge. The original dataset contains personal identifiable information (PII) about the users of OLCF which needed be masked prior to publication, and we determined that anonymization, which scrubs PII completely, destroyed too much of the structure of the data to be interesting for the data challenge. We instead chose to pseudonymize the dataset, which reduced the linkability of the dataset to the users’ identities. Pseudonymization is significantly more computationally expensive than anonymization, and the size of our dataset, which is approximately 175 million lines of raw text, necessitated the development of a parallelized workflow that could be reused on different HPC machines. We demonstrate the scaling behavior of the workflow on two leadership class HPC systems at OLCF, and we show that we were able to bring the overall makespan time from an impractical 20+ hours on a single node down to around 2 hours. As a result of this work, we release the entire pseudonymized dataset and make the workflows and source code publicly available.",10.1109/BigData55660.2022.10020380,K. Maheshwari; S. R. Wilkinson; A. May; T. Skluzacek; O. A. Kuchar; R. Ferreira da Silva
A Review on Cloud Data Assured Deletion,2022,2,Topic_2_data_privacy_security,0.7090225187751117,"At present, cloud service providers control the direct management rights of cloud data, and cloud data cannot be effectively and assured deleted, which may easily lead to security problems such as data residue and user privacy leakage. This paper analyzes the related research work of cloud data assured deletion in recent years from three aspects: encryption key deletion, multi-replica association deletion, and verifiable deletion. The advantages and disadvantages of various deletion schemes are analysed in detail, and finally the prospect of future research on assured deletion of cloud data is given.",10.1109/GCRAIT55928.2022.00101,B. Li; Y. Fu; K. Wang
An Approach towards Reusability in Hybrid Avionic Software Development by Using a Unified Graph Representation of the Software System,2022,-1,Outliers,0.12561524800600404,"Reusability of software is seen as a possibility to reduce the cost for new avionic products. More and more avionic software projects make use of a hybrid approach where model-driven and traditional software development methods are used in the same project. From certification point of view the resulting software system is seen as a single system regardless of the development methodology. This paper focuses on the “process artefacts” that describe the various elements created during the software development process and the trace links that provide a logical connection between the process artefacts. The use of a graph database is a new approach for evaluating the process artefacts as nodes and the trace links as edges. The resulting unified graph is used for analysis purposes and for identifying subgraphs that embody possible reusable software components throughout the hybrid software project. The knowledge of the nodes contained in the subgraph, which are equivalent to process artefacts, makes the export of a software component from the graph possible. The software component is documented and can be added to a software component repository with all its process artefacts from traditional and model-driven software development and its bidirectional trace links. This way, projects that can make use of one or more software components that already exist, can integrate those into the software development process with all the available process artefacts from past development and therefore increase reusability in software development.",10.1109/DASC55683.2022.9925888,N. Skotnik; A. Frey
Trustworthy Protection Technology for Industrial Internet,2022,2,Topic_2_data_privacy_security,0.26292908593986536,"Industrial Internet is a new model and new format of industrial control, which realizes intelligent production, networked collaboration, personalized customization and service extension; it makes production control develop from layered, closed, local to flat, open, and global; information security issues May directly lead to functional safety failure. Facing the status quo of information security protection of the Industrial Internet of Things, this paper analyzes the security risks and protection requirements from the three aspects of equipment and control systems, data, and networks in the Industrial Internet, and proposes corresponding security protection technologies to ensure the system security and data of the Industrial Internet, security and cybersecurity.",10.1109/DSA56465.2022.00076,L. Wu; Y. Zhang; Z. Shen; Y. Jian; L. Tang
A Toolchain and Interoperability Framework to enhance privacy and individual control at the Edge,2022,-1,Outliers,0.2943910722367607,"Internet-of-Things (IoT) has the potential to create new services and markets by allowing the exploration of new, often completely different ways of doing things, based on the clustering and aggregation of data from different sources and fields of activity. As technology advances, new ethical, legal, and technological concerns arise. In this paper, we present five key pillars of innovation towards privacy-preserving edge computing, regarding smart sampling of IoT devices, anonymous authentication and consent management, dynamic data-driven pattern management, opportunistic IoT clustering, distributed IoT data governance, and resource integrity validation. The overall concept of this paper, is to create a comprehensive methodological framework and toolset for definition, deployment and operation of privacy-compliant IoT platforms tailored to specific use-cases. During this process we are not creating “yet another IoT platform”., but rather building upon past efforts to the maximum extent possible. This approach takes into consideration existing solutions in the following areas: high-level concepts and standards; integration and interoperability frameworks; IoT platforms and infrastructural elements.",10.1109/ISC255366.2022.9922309,P. Katrakazas; T. Kallinolitou; S. Markopoulou; A. Chronopoulou
Research on Collaborative Governance of Data Security in the Whole Life Cycle of Electric Power Manufacturing Data Space,2022,2,Topic_2_data_privacy_security,0.42344703459856875,"Data space is a technology system that allows data to be connected securely and efficiently. In the full life cycle of data space data, data collection, data storage, processing, transmission, exchange and destruction, and provision of services according to the dynamic changes of the subject's needs, is a brand-new data management model. This article first starts from the data security risk assessment of the data space of the electric power manufacturing industry, has analyzed the data characteristics of the data space of the electric power manufacturing industry, and sorted out the actual needs of data security. Then, based on the data life cycle process, the risk index of data space data security of electric power manufacturing enterprises are screened out. And used the method of combining Likert five-level scale and questionnaire to assess the amount of risk, graded risk index and proposed corresponding management measures, and constructed a dynamic cycle data security risk assessment model. Finally, a security governance system for data collection, data fusion and data transmission and other data full life cycle multi-faceted collaborative governance was established, and the data security risk of a Beijing electric power manufacturing company was evaluated, and put forward suggestions on how to coordinate the management of data security risks of power manufacturing enterprises.",10.1109/CSCWD54268.2022.9776120,Y. Liu; T. Gao; D. Niu; H. Zhang
Health Indicator Construction Method of Gun Anti-recoil Device Based on Dynamic Degradation Distance,2022,0,Topic_0_prediction_degradation_rul,1.0,"The weight difference between different feature parameters is not considered in the traditional process of constructing health indicator. The study proposes a dynamic degradation distance to overcome the above problem and is used in the construction of health indicator of gun anti-recoil device. Firstly, the unique feature parameters Xmax, Vmax, Umax, Uend and common feature parameters of the gun anti-recoil device signal were extracted to form a vector group of feature parameters, and the feature selection is completed by the volatility index and box plot. Then the dynamic degradation distance is used to address the difference between the standard state and other states of the gun anti-recoil device. Finally, based on the dynamic degradation distance and exponential function, the health indicator of gun anti-recoil device is constructed. The simulation results show that the health indicator constructed in this study has the best performance, and the monotonicity is 0.0614 and 0.7253 higher than that of the method proposed in the references, which indicates that the health indicator constructed in this study can effectively evaluate the degradation state of the gun anti-recoil device.",10.1109/SRSE56746.2022.10067807,J. Wei; F. Zhang; W. Shi; J. Lu; X. Yang
Big data Citizen-centred smart city governance. Evidence from Wuhan and the Coronavirus battle,2022,1,Topic_1_data_big_big data,0.561594477806894,"Smart City governance is a topical subject and many scholars have developed diverse top-down and bottom-up models and frameworks. This paper presents insights from a qualitative and exploratory investigation from the city of Wuhan, China. It proposes and develops a novel urban governance, citizen-oriented and leveraging different emerging technologies such as mobile, social media and Big data. Our empirical results describe the actions realized by the Wuhan local government against the coronavirus epidemic disease between January and March 2020 to monitor the temperature of the citizenship and authorize or not their displacement in order to minimize the cross-contamination and avoid a large human mortality. Our conclusions corroborate the existence of this novel ICT urban governance model. In doing so, the paper advances our understanding of modern urban organization model which comprises of a 360° citizen holistic view, a data-oriented decision-making process and a centralized perspective of the citizen during all the data life cycle. The heuristic that manages this new governance model suggests ex-ante actions at individual or group level and largely exploits public and private sector information breaking (temporarily if necessary) the individual privacy rules for the purpose of its action.",10.1109/ICE/ITMC-IAMOT55089.2022.10033160,D. Grimaldi
Optimizing the Electronic Health Records Through Big Data Analytics: A Knowledge-Based View,2019,1,Topic_1_data_big_big data,0.7314237272853947,"Many hospitals are suffering from ineffective use of big data analytics with electronic health records (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been a key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and big data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data analytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a large hospital in China in 2019. Structural equation modelling was used to examine relationships between knowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs utilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve nurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health and big data literature by exploring the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to shape meaningful use of big data analytics with EHRs.",10.1109/ACCESS.2019.2939158,C. Zhang; R. Ma; S. Sun; Y. Li; Y. Wang; Z. Yan
Dynamic Model-Assisted Bearing Remaining Useful Life Prediction Using the Cross-Domain Transformer Network,2023,0,Topic_0_prediction_degradation_rul,1.0,"Remaining useful life (RUL) prediction of rolling bearings is of paramount importance to various industrial applications. Recently, intelligent data-driven RUL prediction methods have achieved fruitful results. However, the existing methods heavily rely on the quality and quantity of the available data. For some critical bearings in industrial scenarios, the real run-to-failure data are insufficient, which impair the applicability of data-based methods for industrial practices. To address these issues, this article proposes a novel dynamic model-assisted RUL prediction approach for rolling bearing, in which sufficient simulation data are applied as the training data to solve the problem caused by insufficient real data. More specifically, a dynamic rolling bearing model is introduced for simulating the degradation process of physical structures. Then, a multilayer cross-domain transformer network is developed to implement RUL prediction and adapt the learned prediction knowledge from simulation to the actual measurements. Furthermore, a mutual information loss is utilized to preserve the generalized prediction knowledge of the measured data. The proposed approach can achieve a high RUL prediction accuracy with only limited measured data, which tackles the drawbacks of the existing data-driven methods. The experimental results of the rolling bearing degradation datasets demonstrate the effectiveness and superiority of the proposed RUL prediction approach.",10.1109/TMECH.2022.3218771,Y. Zhang; K. Feng; J. C. Ji; K. Yu; Z. Ren; Z. Liu
Data Evolution Governance for Ontology-Based Digital Twin Product Lifecycle Management,2023,-1,Outliers,0.15585747417068688,"Product lifecycle management (PLM) is an effective method for enhancing the market competitiveness of modern manufacturing industries. The digital twin is characterized by a profound integration of physics and information systems, which provides a technical means for integrating multisource information and breaking the time and space barrier of communication at each link of the lifecycle. Currently, however, the application of this technology focuses primarily on the product itself and “service-oriented” application results. There is a lack of focus on twin data and its internal evolutionary mechanisms separately. In the management of global data resources, the benefits of digital twin technology cannot be fully realized. This article applies ontology technology in an innovative manner to the field of the digital twin to increase the reusability of twin data. Initially, a four-layered ontology-based twin data management architecture is presented. Then, a three-dimensional and three-granularity unified evolution model of full lifecycle twin data is proposed, as well as its ontology model. Then, the service mode of data components at each stage of the lifecycle is defined, a knowledge-sharing plane is established in the digital twin, and a data governance method based on ontology reasoning using data components on the shared plane is proposed. The ICandyBox simulation platform is then used to demonstrate the concept of the proposed method, and future research directions are proposed.",10.1109/TII.2022.3187715,Z. Ren; J. Shi; M. Imran
A Privacy-Preserving Architecture and Data-Sharing Model for Cloud-IoT Applications,2023,2,Topic_2_data_privacy_security,0.5693143799436773,"Many service providers offer their services in exchange for users’ private data. Despite new regulations created to protect users privacy, users are often given little choice over the way their data is collected and used. To address privacy concerns in cloud-IoT applications, we propose to use an architecture, called Data Bank, which gives users fine-grained control over their data. Data Bank uses a category-based data access (CBDA) model which covers the whole data life-cycle, from data collection from IoT devices to data sharing with services. We show how dynamic policies can be specified using a new attribute-based instance of CBDA, and describe the use of policy graphs to visualise and analyse policies.",10.1109/TDSC.2022.3204720,M. Fernández; J. Jaimunk; B. Thuraisingham
Experimental Research Reproducibility and Experiment Workflow Management,2023,1,Topic_1_data_big_big data,0.6636234145909929,"Research reproducibility is an important factor to support the full research life cycle; this is especially important for experimental research, where it is required to also reproduce the whole experiment environment and equipment setup. This paper presents the methodology and solution proposed in the SLICES Research Infrastructure to enable research reproducibility in modern digital technologies for complex and large scale experimentation. The paper provides a short overview of existing research and approaches for experimental research reproducibility, generally including git based experiments deployment and operation, Jupyter Notebook and Common Workflow Language (CWL) for workflow management. The paper describes the framework and approaches taken in the SLICES-RI that also address research environment provisioning on demand with the Platform Research Infrastructure as a Service (PRIaaS) and data management infrastructure to ensure data quality and support effective data sharing.",10.1109/COMSNETS56262.2023.10041378,Y. Demchenko; S. Gallenmüller; S. Fdida; P. Andreou; C. Crettaz; M. Kirkeng
Modeling and Simulation Techniques for Energy Behavior in Production Systems: A Review and Systematic Taxonomy,2023,3,Topic_3_industry_manufacturing_chain,0.9464119858132214,"So far, existing publications related to energy simulation of production systems predominantly focus on application or simulation aspects, with a particular emphasis on discrete event simulation (DES) of process chains. However, the modeling of energy aspects, even beyond the typical DES area, is crucial for a successful simulation project in terms of analyzing and optimizing energy-saving potentials within the industry. Therefore, this paper focuses on major techniques used to model the energy behavior of production systems across all levels of production systems, from individual components to a network of factories. Through a structured literature review, we identified the primary modeling techniques and typical characteristics for energy modeling and structured them in the form of a taxonomy.",10.1109/ONCON60463.2023.10431385,M. Barth; E. Russwurm; B. Gutwald; D. Kunz; T. Reichenstein; J. Franke
Real-Time LSM-Trees for HTAP Workloads,2023,1,Topic_1_data_big_big data,0.7607203544931141,"Real-time analytics systems employ hybrid data layouts in which data are stored in different formats throughout their lifecycle. Recent data are stored in a row-oriented format to serve OLTP workloads and support high insert rates, while older data are transformed to a column-oriented format for OLAP access patterns. We observe that a Log-Structured Merge (LSM) Tree is a natural fit for a lifecycle-aware storage engine due to its high write throughput and level-oriented structure, in which records propagate from one level to the next over time. To build a lifecycle-aware storage engine using an LSM-Tree, we make a crucial modification to allow different data layouts in different levels, ranging from purely row-oriented to purely column-oriented, leading to a Real-Time LSM-Tree. We give a cost model and an algorithm to design a Real-Time LSM-Tree that is suitable for a given workload, followed by an experimental evaluation of LASER - a prototype implementation of our idea built on top of the RocksDB key-value store.",10.1109/ICDE55515.2023.00097,H. Saxena; L. Golab; S. Idreos; I. F. Ilyas
Detecting Favorite Topics in Computing Scientific Literature via Dynamic Topic Modeling,2023,1,Topic_1_data_big_big data,0.5956823036618787,"Topic modeling comprises a set of machine learning algorithms that allow topics to be extracted from a collection of documents. These algorithms have been widely used in many areas, such as identifying dominant topics in scientific research. However, works addressing such problems focus on identifying static topics, providing snapshots that cannot show how those topics evolve. Aiming to close this gap, in this article, we describe an approach for dynamic article set analysis and classification. This is accomplished by querying open data of notable scientific databases via representational state transfers. After that, we enforce data management practices with a dynamic topic modeling approach on the associated metadata available. As a result, we identify research trends for a given field at specific instants and the referred terminology trends evolution throughout the years. It was possible to detect the associated lexical variation over time in published content, ultimately determining the so-called “hot topics” in arbitrary instants and how they correlate.",10.1109/ACCESS.2023.3269660,R. V. Encinas Quille; J. M. Barros; M. B. Júnior; F. V. De Almeida; P. L. P. Corrêa
Meta Standard Requirements for Harmonizing Dataspace Integration at the Edge,2023,-1,Outliers,0.3502876422378312,"Emerging technologies such as the Industrial Internet of Things (IIoT), hyper-connected societies, and cyber-physical systems in Industry 5.0 (I5.0) have increased data volume substantially. Conventionally, the data process and analysis are carried out within the boundaries of organizations that can be perceived as edges closer to the data sources, resulting in the generated values stuck within the boundaries of organizations. However, this data can enable a cross-organizational data pool, based on which lots of semantics context-driven services can be developed and monetized to harness the collective power of data. This is precisely what a Dataspace (DS) leveraging distributed edge-coordinated ecosystem can offer to build a value chain on shared services and reusable data across organizations, domains, systems and devices. However, various challenges need to be addressed to achieve the same, such as lack of participatory motivation, heterogeneity due to the integration interfaces at different levels of data management operations and diverse standards available to perform the same target operation. Therefore, this study proposes a model for motivating entities to open up for data integration and build a services-driven cross-boundary value-chain system through a coordinated edge-driven DS ecosystem. In addition, it introduces the modified data life cycle, emphasising the integration phase and the integration harmonization ecosystem focusing on standards-driven heterogeneity harmonization, which demands Meta-Standard development and related guidelines.",10.1109/CSCN60443.2023.10453211,P. Singh; Nidhi; A. U. Haq; M. Beliatis
A Security model with efficient AES and Security Performance Trade-off Analysis of Cryptography Systems with Cloud Computing,2023,2,Topic_2_data_privacy_security,0.7450615845174445,"Cloud computing is awesome progress the information technology. It allows users to store a large amount of data on a remote storage location and access files, software, and services through their internet connection on the personal computer based on the pay-per-use model. End-users mostly use the cloud for storing information on the cloud, sharing files, and data backup. In fact, according to the flex era 2021 state of cloud report, roughly 90 % of enterprises anticipate cloud users have been expanded because of COVID-19. Data security and privacy are basic concerns because of the nature of storing unencrypted cloud computing. Though many encryption standards are being used to make crucial information safe, single encryption is not able to cope with fast computing systems. To circumvent the threats, this paper uses a triple encryption approach before porting on a cloud with a hash value to deliver the data integrity, confidentiality, and non-repudiation. This solution is designed to offer complete data security to the data throughout the data life cycle, such as to data in transit, and data in storage. Subsequently, Improved AES achieves better optimization over AES. Moreover, the security and performance analysis of the proposed approach with the existing approaches assist to find the best security performance trade-off.",10.1109/ICECCT56650.2023.10179752,G. Kaur; B. N; M. S; P. K; S. G; V. K. M
Assuring safety in a flexible aerospace certification — Lessons learned on applying OPs at the system level—,2023,-1,Outliers,0.134525959224749,"Fast development and adoption of new technologies has outpaced the development of new aerospace certification standards. Overarching Properties (OPs) have emerged as a promising flexible framework for proposing alternative Means of Compliance. The hope is that the FAA may eventually establish an Advisory Circular that offers the OPs for safety critical approvals by showing the product possesses the three OPs: In-tent (specification of the intended behavior), Correctness (implementation of the intended behavior), and Innocuity (safety of unintended behavior). However, there is a lack of industrial case studies that evaluate its applicability and scalability. This paper provides an experience report of using OPs for jointly seeking software and system certification approval for an industrial Auxiliary Power Unit (APU) Control System. This project results in a certification argument that allows to use of an efficient model-based system engineering (MBSE) approach for developing airworthy system. The proposed MBSE approach is currently not supported by certification standards. We combine textual and graphical notation for specifying the argument to facilitate its assessment by the evaluation team. This paper highlights important aspects for the creation and evaluation for these arguments. These aspects show that the greater the scope of the certification and more novel the technology, the more detailed the argument must be. Keywords— Overarching Properties, Certification, Arguments, Assurance Cases.",10.1109/SysCon53073.2023.10131225,Z. Daw; S. Beecher
"Privacy-preserving Data Federation for Trainable, Queryable and Actionable Data",2023,2,Topic_2_data_privacy_security,1.0,"Privacy preservation over federated data has gained its momentum in the era of securing users’ sensitive information. Combining and analysing sensitive information from multiple data sources offers considerable potential for knowledge discovery. However, there are different constraints which should be fulfilled, such as what are the data to be preserved; what is meant by privacy preservation; what are the constraints on federated computing; and what are the secure mechanisms to train, query and explore data without accuracy loss. We introduce the Protected Federated Query Engine which applies Fully Homomorphic Encryption and querying processing over decentralised data sources of diverse schemas and granularities to efficiently collect, align, aggregate and serve Artificial Intelligence Operations (AIOps) and Data Operations (DataOps) without sacrificing accuracy and efficiency.",10.1109/ICDEW58674.2023.00012,S. Iatropoulou; T. Anastasiou; S. Karagiorgou; P. Petrou; D. Alexandrou; T. Bouras
Utilizing Data Strategy Framework for Retail Business in the Metaverse,2023,1,Topic_1_data_big_big data,0.4962280622789068,"The emergence of the metaverse has brought about a significant shift in the retail industry as businesses strive to couple from the physical and digital worlds. However, this transition presents challenges in managing the massive and rapidly streaming data originating from the metaverse. In this paper, we explore the business opportunities that the metaverse offers for retail businesses and shed light on the ways to utilize data strategy in metaverse retailing. Our research analyzes the potential metaverse scenarios and the characteristics of metaverse data. We identify these scenarios into three key categories: Decoupled, Semi-coupled, and Tightly coupled. In particular, we highlight the importance of robust data strategies for organizations to effectively handle metaverse data and unlock success in this new realm. As a result, we propose a comprehensive data strategy framework that empowers retail businesses to effectively manage metaverse data, leverage valuable data insights, and make informed decisions at the rapid pace of the metaverse, enabling them to stay competitive in this evolving digital realm.",10.1109/iMETA59369.2023.10294793,P. Tancharoen; W. Pongpech
Automating Airborne Software Certification Compliance Using Cert DevOps,2023,-1,Outliers,0.13144199132546872,"The development of avionics software follows strict processes to ensure care is taken commensurate with the safety required of the system. This effort is significant and has many aspects that bring safety, but the complex set of artifacts required to prove the process was followed can result in significant cost and schedule. This can put at risk the ability to create new and transformative software for a future that includes advanced aircraft and single pilot operations. We are applying DevOps principles to avionics software development in a transformative way to address these issues.DevOps principles have been used in software development for many years to increase flow through automation, provide faster feedback, and incorporate continuous improvement. This is typically done in the frame of software implementation but is typically not inclusive of domain requirements like DO-178C. Automating the tasks and evidence required by DO-178 can provide a significant speed advantage. In this research project we worked to understand how much could be automated, and how the automation can become a robust infrastructure that reduces the chance of human error in the development process.Our preliminary results indicate > 75% of common defects, especially those liable to become open problem reports (OPRs), can be addressed through robust infrastructure. We have also measured substantial improvement to cycle time and reduction in the use of difficult-to-justify, regression-oriented testing.Our conclusion is that this sort of step change in safety-driven software development is possible. A process we call ‘Cert DevOps’ enables a more robust and agile development cycle necessary for the significant number of new features required for avionics in support of single pilot operation. Implications of this change include the possibility of auditing the automations of the infrastructure rather than the resulting automated output, which could form the basis of future standards work.",10.1109/DASC58513.2023.10311205,C. Hubbs; J. Myren
Data Security Risk Assessment Method of Intelligent and Connected Vehicles Based on Data Security Classification and Grading,2023,2,Topic_2_data_privacy_security,0.3637181542034188,"The rapid development of intelligent and connected vehicle (ICV) has brought increasing communication data and valuable data assets. However, the diversity and complexity of ICV data make it hard to carry out data security risk assessment. Aiming at the problem, this paper proposes an effective analytical method for the data security risk assessment method of ICVs. Considering the typical scenario for the generated data of ICVs, a data security classification and grading framework is constructed from the aspects of in-vehicle data, personal data, and external environment data. The data security risk assessment model is built for ICVs and includes functional modules such as asset identification, threat identification, impact assessment, vulnerability identification, and risk assessment. This work provides methods and security management and control strategy support for ICV data security risk assessment.",10.1109/ICITE59717.2023.10733882,L. Wang; L. Jin; H. Ji; J. Wang; Y. Chen; J. Fang
Research on data security risk of intelligent and connected vehicles,2023,2,Topic_2_data_privacy_security,0.40263838557124265,"With increased connectivity and the application of intelligent technologies, intelligent and connected vehicles are evolving rapidly, which offers new opportunities for vehicle data security risks. However, there are currently insufficient studies to comprehensively map the security risks throughout the life cycle of intelligent and connected vehicle data. The object of this paper is to identify the main data security risks at different data life cycle phases in the field of intelligent and connected vehicles, and the data security problems those risks may bring. The following are some of the techniques used to protect the security of data against risks. The test verification is implemented by using functional reproduction and data packet capture analysis. The results indicate that there are vehicle data security risks to personal information, including location and biometric information. This paper is useful for intelligent and connected vehicle data processors in their targeted application of technical and managerial measures to mitigate data security risks in the whole data life cycle.",10.1109/ICMIII58949.2023.00034,Y. Li; Y. Wang; J. Wang; H. Wu; X. Xia
Data Privacy and Security Model in Cloud Environments,2023,2,Topic_2_data_privacy_security,0.8256769252002326,"The privacy and security of data stored in cloud environments remain a significant challenge for organizations and individual users. Since data is stored in different physical locations, data security and privacy are particularly critical in cloud environments. Today, data privacy and security are the main concerns of governments, organizations, academics, and IT industry experts. The future development of cloud computing depends on providing reliable data privacy and security solutions. The current study performs an in-depth assessment of data privacy and security issues affecting individual users and organizations. The analysis is followed up by a suggested solution of an adequate data security and privacy model based on a combination of file encryption and service fragmentation. Specifically, the development of the analysis and security model focuses on the Linux operating system. Finally, extensive simulation results prove that the proposed model gives more privacy accuracy and reliability in cloud environments.",10.1109/ICCIT58132.2023.10273911,W. Almuseelem
Towards Reliable Collaborative Data Processing Ecosystems: Survey on Data Quality Criteria,2023,1,Topic_1_data_big_big data,0.43456672379415284,"Data quality plays a crucial role in the data governance of organizations, as it is essential to ensure that data are fit for the purpose for which they are intended, whether for operational activities, decision-making processes, or strategic planning. As data silos begin to be integrated to form data spaces, guaranteeing data quality becomes a necessity to achieve a reliable collaborative ecosystem. Nevertheless, the concept of data quality remains ambiguous, with various definitions and interpretations offered in the literature, despite its importance. This lack of consensus has led to the need for a thorough review of the different data quality criteria used in scientific work. Therefore, this paper serves as a systematic survey aimed at exploring and consolidating diverse perspectives on data quality. By thoroughly analyzing existing literature, this study compiles a comprehensive set of 30 agreed-upon data quality criteria, with their respective names and definitions. These criteria act as a valuable resource for organizations seeking to establish effective data quality monitoring practices. Then, we expose challenges raised by collaborative data processing and highlight possible research directions where data quality plays a major role.",10.1109/TrustCom60117.2023.00345,L. Sahi; R. Laborde; M. -A. Kandi; M. Sibilla; G. Macilotti; B. Abdelmalek; A. Ferreira
Research on the Big Data Security Application Based on Artificial Intelligence Technology in Operators,2023,2,Topic_2_data_privacy_security,0.4140293296900835,"The importance of data security and privacy is increasing under the trend of digital development and has become an important work research direction for government and enterprises, operators carry a large amount of data of users in the industry. This paper describes potential data security threats in the big data environment, especially operators, and proposes a security protection scheme based on AI technology and the whole process data is safe and controllable traceability. This solution provides new ideas and methods for data security protection for operators.",10.1109/ICAIBD57115.2023.10206137,B. Zhao; W. Xu; Z. Zhang
Big Data Reference Architecture Standards: Analysis and Presenting a Development Road Map,2023,1,Topic_1_data_big_big data,1.0,"Due to drastically increasing data and increasing productivity which can be obtained, processing and analyzing big data plays a crucial role nowadays. On the other hand, a growing range of standardization organizations has addressed various challenges in the field of big data, which are important in design and development of big data systems and services. The big data reference architecture is developed by different standardization institutes including joint technical committee of the International Organization for Standardization (ISO/IEC), the American National Institute of Standards and Technology (NIST), and also the International Telecommunication Union (ITU). The reference architecture describes the structure and components of big data from different perspectives, including conceptual, user, and functional. This paper addresses the most important related standards and provides a mapping of them to the big data reference architecture, which could be used for various purposes, mainly as a standardization roadmap. This road map could facilitate various activities, including technology-level assessment, testing and comparing capabilities, type approval, and licensing.",10.1109/ICWR57742.2023.10139266,E. Arianyan; D. Maleki; A. Mansouri
"Understanding the Importance of Data Continuum Enabling the Data Exchange, and Semantic Interoperability with Trust in Future Networks",2023,-1,Outliers,0.3606140601819626,"The Internet as a network for global communications and exchange of information has continuously changed its architecture, mainly as result of the continuous demands for more privacy and security alike processing capacity at the edge of the network (i.e. smart connected devices). In this paper, a vision about the origins of the Data Continuum, its evolution towards becoming the enabler of data exchange and semantic interoperability and how it has promoted the growth of cross-domain integrated data services is presented. This is a paper describing the transition from data collection to data marketplaces in the context of the Future Networks evolution and how the data continuum its considered not only an enhancing network technique but a new technology method, where Data and Information & Communications Technology in the form of new services can evolve rapidly.",10.1109/FNWF58287.2023.10520489,M. Serrano; A. Zappa; W. Ashraf
A review of data governance challenges in smart farming and potential solutions,2021,-1,Outliers,0.3361215094935134,"The expectation on the agricultural system is constantly growing to be more productive with less labor, less water and less arable land. To achieve this goal, the use of digital technologies is being promoted. This has resulted in growth in use of wireless sensors, IoTs, cloud computing and other technologies in farms which have fueled the explorational of data. Data collected at farms varies from business operation data (farm management data), transport and farm storage data, land data (water, soil, GPS), machine data, agronomic data, livestock data, climate, and weather data. This large amount of data needs to be managed to ensure confidentiality and other governance requirements and enhance technical capacity and performance such as data integration and processing. This paper reviews the data governance challenges generated in smart farms and provides recommendations on how those challenges can be addressed.",10.1109/ISTAS52410.2021.9629169,A. Anidu; R. Dara
Remaining Useful Life Prediction Approach Based on Data Model Fusion: An Application in Rolling Bearings,2024,0,Topic_0_prediction_degradation_rul,1.0,"Data-driven methods based on deep neural networks (DNNs) are widely employed for predicting the remaining useful life (RUL) of equipment, yielding remarkable results. However, the performance of DNN relies on the availability and completeness of full life cycle data. Moreover, problems such as lack of interpretability of prediction results and weak model generalizability still exist. An RUL prediction approach based on data model fusion (DMF) is proposed in this article to address these problems. This approach incorporates physics knowledge into the stacked bidirectional long short-term memory (SBiLSTM) network in three ways. First, the full life cycle data based on the physics degradation model is integrated with sensed data to ensure the integrity of degradation data. Second, the degradation trajectory simulated based on the physics degradation model is used as an input feature for the SBiLSTM, enabling the model to better learn the state evolution process of the equipment. Moreover, a multiobjective loss function is constructed by introducing a physics-guided inconsistency loss function alongside the data loss function to ensure the model predictions consistent with the known physics phenomena and enhance the interpretability of the model. Case studies are conducted for the XJTU-SY dataset and the PHM2012 dataset to systematically validate the proposed approach. Comparisons with existing data-driven and hybrid methods are made, and the results consistently demonstrate the accuracy of the predictions and the robustness of the performance.",10.1109/JSEN.2024.3477489,Y. Zhu; J. Cheng; Z. Liu; X. Zou; Z. Wang; Q. Cheng; H. Xu; Y. Wang; F. Tao
A Novel Transfer Learning Approach for State-of-Health Prediction of Lithium-Ion Batteries in the Absence of Run-to-Failure Data,2024,0,Topic_0_prediction_degradation_rul,1.0,"Accurate and reliable state-of-health (SOH) prediction becomes increasingly vital to ensure the safe and reliable operation of lithium-ion batteries (LIBs). The existing data-driven methods for LIBs’ SOH prediction are developed with an ideal database, i.e., a huge run-to-failure data with a consistent distribution of training and testing sets. However, due to individual quality differences and complex operating conditions, data distribution shifts among different batteries may be obvious, and the entire life-cycle samples are difficult to collect in real world. Therefore, there exists a distribution discrepancy between source and target domains. Besides, temporal distribution shift may also exist with incomplete target domain, called time covariate shift (TCS). Thus, the model trained with source and incomplete target domains will cause prediction bias in unseen target data. To address such issues, a novel transfer learning (TL) approach using multiple feature alignment transformer (MFA-Transformer) model is conducted for the SOH prediction of LIBs. First, a multilayer feature alignment is performed via encoder-decoder structure of transformer framework, and multikernel maximum mean discrepancy (MK-MMD) is adopted to tackle data distribution discrepancy. Then, a new loss item based on Weibull distribution is utilized to enhance the data alignment effect. Moreover, a shift compensation strategy using shape-based distance (SBD) estimation is designed to dynamically eliminate the prediction bias resulting from TCS. Finally, experiments on two public LIBs datasets validate the effectiveness of the proposed method, which can offer a promising solution for industrial prognostic without entire life-cycle data.",10.1109/TIM.2024.3450095,L. Song; X. Gui; J. Du; Z. Fan; M. Li; L. Guo
Advancements in Data Quality Management in the Big Data Era: A Comprehensive Review,2024,1,Topic_1_data_big_big data,0.6155729175198602,"This paper reviews the latest advancements in data quality within the Big Data era, focusing on developments over the last five years. We explore emerging challenges in assessing data quality, particularly as organizations manage increasingly complex datasets from domains like geospatial data, healthcare, and the Internet of Things (IoT). Traditional rule-based approaches are becoming insufficient, leading to the rise of AI-powered methods for data cleansing, anomaly detection, and real-time validation. Additionally, the paper examines organizational strategies for effective data quality management, as well as the growing importance of data quality tools and frameworks in both industry and academia. Key technologies such as distributed systems (Hadoop, Spark, Snowflake) and machine learning models are evaluated in their roles for improving data accuracy, consistency, and reliability. Case studies from industries highlight the practical implementation of these technologies, demonstrating their impact on operational efficiency and decision-making. Lastly, we discuss future research directions, focusing on scalable solutions, ethical considerations, and the role of emerging technologies like quantum computing. By synthesizing insights from recent studies and industry practices, this review provides a comprehensive overview of current trends and future outlooks in data management and quality assurance.",10.1109/AICECS63354.2024.10956447,V. Mishra; H. Darade
"Securing Personally Identifiable Information: A Survey of SOTA Techniques, and a Way Forward",2024,2,Topic_2_data_privacy_security,1.0,"The current age is witnessing an unprecedented dependence on data originating from humans through the devices that comprise the Internet of Things. The data collected by these devices are used for many purposes, including predictive maintenance, smart analytics, preventive healthcare, disaster protection, and increased operational efficiency and performance. However, most applications and systems that rely on user data to achieve their business objectives fail to comply with privacy regulations and expose users to numerous privacy threats. Such privacy breaches raise concerns about the legitimacy of the data being processed. Hence, this paper reviews some notable techniques for transparently, securely, and privately separating and sharing personally identifiable and non-personally identifiable information in various domains. One of the key findings of this study is that, despite various advantages, none of the existing techniques or data sharing applications preserve data/user privacy throughout the data life cycle. Another significant issue is the lack of transparency for data subjects during the collection, storage, and processing of private data. In addition, as privacy is unique to every user, there cannot be a single autonomous solution to identify and secure personally identifiable information for users of a particular application, system, or people living in different states/countries. Therefore, this research suggests a way forward to prevent the leakage of personally identifiable information at various stages of the data life cycle in compliance with some of the common privacy regulations around the world. The proposed approach aims to empower data owners to select, share, monitor, and control access to their data. In addition, the data owner is a stakeholder and a party to all data sharing contracts related to his personal data. The proposed solution has broad security and privacy controls that can be tailored to the privacy needs of specific applications.",10.1109/ACCESS.2024.3447017,I. Makhdoom; M. Abolhasan; J. Lipman; N. Shariati; D. Franklin; M. Piccardi
Blockchain Anchored Federated Learning and Tokenized Traceability for Sustainable Food Supply Chains,2024,-1,Outliers,0.30584791421506125,"The management of food supply chains relates to just about the most imperative issues concerning sustainability, quality, and traceability, where existing methodologies usually fail to meet the demands caused by data privacy, lack of transparency, and inefficiencies concerning collaboration among stakeholders. This compromises data security, and thus there arises single points of failure. Conversely, traditional traceability frameworks fail to incite wide-scale participation and lack real-time accuracy. Having these limitations in mind, we introduce a hybrid framework consisting of three advanced methodologies: Decentralized Federated Learning with Blockchain Anchoring (DFLBA), Dynamic Tokenized Traceability Framework (DTTF), and Adaptive Supply Chain Graph Neural Networks (ASCGNN) process. DFLBA would support privacy-preserving machine learning as such that collaborative training of yield prediction and crop monitoring models by stakeholders without revealing raw data will be feasible. Blockchain anchoring ensures logs of training updates are tamper-proof, thus increasing trust and transparency. DTTF introduces tokenized incentives that enhance traceability in real-time: every product has a digital twin represented by blockchain-anchored tokens updated by stakeholders. Smart contracts enforce the compliance that will improve participation and lifecycle data accuracy. ASCGNN models the food supply chain as a dynamic graph, leveraging graph neural networks to capture stakeholder interdependencies, optimize operations, and detect fraud. Blockchain integration ensures reliability and transparency by securing edge weights. The suggested framework has demonstrated good impact, like ~95% model accuracy for yield prediction, 99% traceability coverage, and ~92% prediction accuracy for bottlenecks of the supply chain. It provides a scalable, privacy-preserving, and transparent solution that enhances the efficiency of food supply chains, and positively impacts stakeholders in addressing data privacy concerns and incentivizes collaboration among them-setting up a new benchmark in managing sustainable supply chains.",10.1109/ICUIS64676.2024.10866271,K. R. Chaganti; B. N. Kumar; P. K. Gutta; S. L. Reddy Elicherla; C. Nagesh; K. Raghavendar
Harnessing ICT-Enabled Warfare: A Comprehensive Review on South Korea’s Military Meta Power,2024,3,Topic_3_industry_manufacturing_chain,0.7195866763543735,"Major countries around the world are leveraging information and communications technology (ICT), such as artificial intelligence, big data, cloud computing and cybersecurity, to strengthen their militaries. These technological advancements are driving the changes in weapon systems, strategies, and tactics. In 2021, the South Korean Ministry of National Defense introduced the concept of “military meta power” to describe the power generated in the cognitive military domain utilizing ICT. This concept is significant in that it differentiates between cognitive and physical military power, emphasizing the importance of the cognition-based power in future warfare. Additionally, the concept consolidates various cognitive military capabilities, including AI capability, cyber capability, space capability, and C4ISR (Command, Control, Communications, Computers, Intelligence, Surveillance, and Reconnaissance), into a unified ICT-generated power. This study aims to contribute to the literature on “military meta power” by providing a comprehensive overview of its characteristics, composition, and attributes. To achieve this objective, the research begins with an exploration of the philosophical insights on technology related to ICT, followed by a comprehensive review of the perspectives of existing academic papers. Finally, this study provides an in-depth analysis of various military strategies.",10.1109/ACCESS.2024.3378735,S. J. Oh; S. K. Cho; Y. Seo
Towards a policy tuning method for data ecosystems,2024,1,Topic_1_data_big_big data,0.4437713580892432,"Data ecosystems are often used in many social, industrial, and research areas to boost the learning process or enhance the value of the endless amount of data generated and collected daily. Their design principles are consolidated, but today, data ecosystems must address new needs, as they are faced with multiple interdependent challenges, such as users’ engagement, intellectual property, data confidentiality, and data sharing. The achievement of these business objectives highly depends on the deployed policies that govern many, if not all, aspects of the data ecosystems. Identifying the role of these policies in achieving data ecosystems’ objectives is key to their success. This paper analyzes policies and their relations to define how they impact the business objectives of the systems and, possibly, to improve them. The proposed method highlights the interdependencies between different policies. Such results will be leveraged to tune policies to achieve data ecosystems’ business objectives.",10.1109/ICWS62655.2024.00024,M. Salnitri; E. Ramalli; B. Pernici
Integrating Ontology Design with the CRISP-DM in the Context of Cyber-Physical Systems Maintenance,2024,3,Topic_3_industry_manufacturing_chain,0.27722415739869466,"In the following contribution, a method is introduced that integrates domain expert-centric ontology design with the Cross-Industry Standard Process for Data Mining (CRISP-DM). This approach aims to efficiently build an application-specific ontology tailored to the corrective maintenance of Cyber- Physical Systems (CPS). The proposed method is divided into three phases. In phase one, ontology requirements are systematically specified, defining the relevant knowledge scope. Accordingly, CPS life cycle data is contextualized in phase two using domain-specific ontological artifacts. This formalized domain knowledge is then utilized in the CRISP-DM to efficiently extract new insights from the data. Finally, the newly developed data-driven model is employed to populate and expand the ontology. Thus, information extracted from this model is semantically annotated and aligned with the existing ontology in phase three. The applicability of this method has been evaluated in an anomaly detection case study for a modular process plant.",10.1109/ETFA61755.2024.10710898,M. S. Gill; T. Westermann; G. Steindl; F. Gehlhoff; A. Fay
Rolling Element Bearing Degradation Prediction Using Dynamic Model and an Improved Adversarial Domain Adaptation Approach,2024,0,Topic_0_prediction_degradation_rul,1.0,"Rolling element bearing degradation prediction is an important issue in rotating machinery. With the rapid development of artificial intelligence (AI), AI-based bearing degradation prediction has aroused extensive attention. However, current methods rely on whole life cycle data, which is quite difficult to acquire in real industrial scenarios. To solve this problem, a rotor-bearing dynamic model is built to generate simulation signals for a range of spall sizes, and an improved domain adversarial neural network is proposed to transfer degradation knowledge from simulation data to experimental data. To be specific, complete simulation data is used to pre-train a network for learning comprehensive degradation knowledge, and guides the extracted high-level features in the adversarial domain adaptation stage to align with it as an additional optimization item. The proposed approach is verified on bearing degradation datasets under different working conditions, and results show that the proposed approach can successfully predict bearing degradation progress only with some early stage experimental data.",10.1109/ACCESS.2024.3403476,S. Xu; C. Jiang; C. Yang; H. Cao; Z. Song; R. Zheng; S. Lu; H. Xu; H. Zhang
Computational Archival Processes & Assessable Sustainability: Challenges and Opportunities,2024,-1,Outliers,0.30226780901267264,"This article highlights the environmental impacts associated with information and communication technologies (ICT) used for data storage and processing, emphasizing the significant emissions generated during the lifecycle of electronic devices. It addresses the challenges of assessing these environmental impacts using methodologies like the GHG Protocol and Life Cycle Assessment (LCA). It also explores opportunities for mitigating these impacts through better data governance, techniques for reducing digital waste, and sustainability initiatives such as the Arch’Eco project, who aims to assess environmental impacts for the entire lifecycle of data and identify best practices for data management in an environmentally friendly manner.",10.1109/BigData62323.2024.10826059,A. Nicolet; B. M. Shabou
Enhancing Privacy in Real-Time Stream Processing: Federated Transfer Learning Approaches,2024,2,Topic_2_data_privacy_security,1.0,"In the era of data ubiquity, safeguarding privacy while harnessing the power of real-time streaming presents a critical challenge. This study delves into the intersection of privacy preservation, real-time streaming, and federated transfer learning to devise robust solutions. First, we elucidate the essence of data privacy, real-time streaming, and federated transfer learning. Subsequently, we propose an innovative approach that seamlessly integrates privacy considerations into the fabric of real-time streaming analytics through federated learning models. Leveraging this framework, we ascertain the optimal model selection process tailored to the specific data characteristics and user requirements. Our analysis encompasses a comparative evaluation of five prominent models: Differential Privacy, Secure Multi-party Computation, Homomorphic Encryption, Federated Learning with User-level Differential Privacy, and Decentralized Federated Learning. Through empirical examination, we discern the most suitable model for preserving privacy in real-time streaming contexts, thereby advancing the discourse on privacy-preserving machine learning paradigms.",10.1109/MetroLivEnv60384.2024.10615956,S. Jog; D. Palaniappan; M. A. Jabbar
Research on Vertical Sharing and Penetration Technology of Smart Substation,2024,-1,Outliers,0.20795819055306622,"In this paper, aiming at the problems such as the difficulty of expanding the breadth and depth of substation centralized monitoring, the low degree of business collaboration and data sharing between the main and sub-stations, and the poor relevance of the main and auxiliary business in the substation, the vertical data sharing and business collaboration between the main and sub-stations are supported through the construction of equipment model fusion method, service interaction mechanism and data resource pool architecture. Support holographic panoramic monitoring such as collaborative monitoring, collaborative early warning, collaborative operation and maintenance of remote master station.",10.1109/EEE59956.2024.10709760,X. Liang; S. Yang; Q. Zhao; J. Liu; H. Pan; J. Shen
Revolutionizing Data Management: An Analysis of Automated Front-End Processes in the Data Life Cycle Using Machine Learning,2024,1,Topic_1_data_big_big data,0.3968330626022853,"Companies must focus on managing the data life cycle efficiently and reliably to obtain valuable insights, especially with the increasing amount of data generated. This work emphasizes the importance of automating the initial stages of the data life cycle, such as data collection, ingestion, cleaning, preprocessing, and integration. Organizations must focus on managing data throughout their life cycles to gain valuable insights, especially with the increasing amount of data available. This work features the significance of computerizing the front-end periods of the information life cycle, which incorporate information assortment, ingestion, cleaning, preprocessing, and reconciliation. Thus, associations can guarantee viable and solid administration of their information. The motivation behind this study is to evaluate the achievability and knowledge of the future headings of information life cycle mechanization.",10.1109/ICACCM61117.2024.11059195,C. S. Charan; A. S. Sahitha; G. Thanuja; K. Gagandeep; L. Arya
Navigating Exascale Operational Data Analytics: From Inundation to Insight,2024,1,Topic_1_data_big_big data,0.7683846017496984,"In this paper, we address the challenges in achieving sustainable data-driven efficiency by providing a detailed exploration of the end-to-end operational data analytics (ODA) framework that evolved through two generations of supercomputer systems at the Oak Ridge Leadership Computing Facility (OLCF). This framework addresses large data streams ingested from heavily instrumented HPC environment that accumulates multi-terabytes per day. We outline the multifaceted data life cycle across HPC procurement, operations, and research & development, identifying key obstacles and design decisions that shape effective strategies in building and supporting data pipelines end-to-end. By sharing key insights and lessons learned from our experience, we offer recommendations for the HPC community on enabling sustainable operational data analytics and beyond. Our contributions aim to bridge the gap between potential and real benefits of operational data, guiding future efforts towards integrated and sustainable operational intelligence in high-performance computing environments.",10.1109/SCW63240.2024.00226,W. Shin; T. Osborne; A. M. Karimi; R. Palumbo; A. May; C. Lester; J. Hines; N. S. Sattar; L. Huk; S. Simmerman; W. Brewer; J. Miller; R. Adamson; O. Kuchar; R. Prout; F. Wang; S. Atchley; S. Oral
Research on Railway Data Security Protection Based on Data Grading Rules,2024,2,Topic_2_data_privacy_security,0.4153285521898107,"Nowadays, a large amount of heterogeneous data from multiple sources in the railway industry are frequently exchanged internally and externally. According to business requirements, data should be shared in a safe and orderly manner through classification and grading management of data. However, under complex business scenarios, there are still security weaknesses in the process of data flow, and there is a lack of targeted security protection recommendations for the life-cycle of railway data. In order to solve this problem, we designed a data security knowledge graph based protection technology searching method. The searching method designs: i) a knowledge reasoning scheme for data grading protection recommendations based on railway data grading rules, ii) a converged knowledge graph, and iii) combination of the graph search method and feature similarity algorithm to achieve the directed security protection management of data assets with different life cycle stages at different security levels under railway special business scenarios. Finally, We chose the railway EMU maintenance business scenario for example analysis to verify the reliability of the method.",10.1109/ICAIBD62003.2024.10604532,H. Geng; W. Mu; X. Cui; L. Huang
AnyComputing: A Novel Architecture for Orchestrating Various Computing Paradigms for Adaptive Workload Management,2024,1,Topic_1_data_big_big data,1.0,"The proliferation of physical objects and intelligent systems, along with their inherent differences, such as levels of device mobility, generated data volume, required storage memory, and complexity in data processing, presents a significant challenge in achieving real-time and optimal responsiveness to the Massive volume of sent requests from these systems. Depending on the nature of each request and its requirements, effective processing must be by available and suitable computational platforms. This situation necessitates the urgent development of novel ideas and frameworks in request orchestration and processing by suitable computational platforms to enhance system performance and efficiently utilize available computational resources. In this article, we propose an integrated computational architecture spanning from edge devices of users and intelligent things to cloud servers. This design encompasses the reception, processing, and response to various computational and storage requests, considering specific requirements, through computational units deployed within this span. Our objective in devising this architecture is to establish mechanisms for categorizing diverse computational requests from users and things, effectively allocating resources to them, thereby reducing response time, optimizing bandwidth utilization and other available resources, managing latency for real-time requests, and minimizing energy consumption in user devices and intelligent objects.",10.1109/CCE62852.2024.10770984,S. Taghizadeh; S. Yousefi
DSPOL: A High-Level Language for Defining Data Policies in Data Spaces,2024,-1,Outliers,0.3074834669053995,"In recent years, data spaces have emerged as a framework for secure data collaboration. Data spaces are distributed data collaboration infrastructure systems that enable data collaboration among stakeholders. One of the requirements for data space systems is the technical enforcement of governance policies for the shared data. The objective of this study is to propose a new policy definition language, DSPOL, as a stepping stone toward realizing the enforcement of data governance for the practical use of data spaces. DSPOL can describe the constraints that must be followed in data space systems when accessing and using data, and when distributing deliverables. It has verification and validation functions to ensure that the policy description is as the writer intended and that there are no inconsistencies in the policy. We defined the state transitions of data usage in data user environments, developed a model of the execution infrastructure in data space systems, and formulated the contents of the describable policies. We implemented DSPOL and its verification and validation functions. Examples of policy descriptions based on supposed scenarios were shown, and examples of verification and validation were presented.",10.1109/BigData62323.2024.10825983,S. Taniguchi; S. Nakajima; T. Michikata; H. Seike; N. Koshizuka
Integration of UAV and Satellite Data in Remote Sensing,2024,1,Topic_1_data_big_big data,1.0,"The integration of satellite and Unmanned Aerial Vehicle (UAV) data significantly enhances monitoring capabilities. This paper presents a comprehensive survey on the applications of UAV/Satellite integration throughout the data life cycle, including data collection, pre-processing, processing, and storage. Two primary approaches are examined: data fusion and analysis fusion. For each approach, we explore existing remote sensing big data platforms and various application domains. This work aims to assist researchers in developing UAV/satellite systems by helping them select appropriate big data architectures and technologies tailored to their specific needs.",10.1109/AICCSA63423.2024.10912625,G. Graja; T. Abdellatif
Improvement of VMD and Compensation Distance for Rolling Bearing Fault Detection Methods,2024,0,Topic_0_prediction_degradation_rul,0.8659104797281231,"To address the problem of varying detection performance and the lack of universal standards for early fault detection in rolling bearings, a method for early fault detection in rolling bearings based on an improved variational modal decomposition (IVMD) and compensation distance (CD) is proposed from the perspective of constructing performance degradation curves. Firstly, the number of modes K for variational mode decomposition (VMD) is adaptively determined using the estimated signal-to-noise ratio (eSNR) method. Secondly, the probability density function (PDF) distribution of the intrinsic mode function (IMF) obtained from the decomposition is calculated, and the compensation distance (CD) degradation evaluation index is constructed by integrating the inter-class and intra-class distances. Then, a sliding window is used to repair the false fluctuations in the CD degradation index based on linear regression equations and the $3 \sigma$ principle, resulting in a sensitive degradation index that accurately characterizes the rolling bearing degradation process Improved Compensation Distance (ICD). Finally, the Chebyshev inequality is used to establish the relationship between the healthy threshold and the ICD index, enabling early fault detection in rolling bearings. Comparative analysis of one set of experiments shows that the proposed method can accurately characterize the degradation process of rolling bearings and effectively detect early faults, providing a new solution for early fault diagnosis of bearings.",10.23919/CCC63176.2024.10661375,K. Guo; X. Xiong; J. Ma
Research on Archival Management of Electricity Trading Entity Data Based on the Whole Life Cycle,2024,3,Topic_3_industry_manufacturing_chain,0.21098051040092874,"With the development of electricity market in China from construction to operation, it is of great significance to strengthen the business data management of power trading entities in order to support the supervision of market entities, the analysis of market operations and the improvement of trading mechanism. In order to record the whole process information of trading entities from market entry to delisting, this paper proposes the idea of creating the whole life cycle archives of market trading entities, discusses the governance framework of archival data of power trading entities, and puts forward a time-series multidimensional spatial data model serving the archives of market entities. Further, based on the current electricity trading platform, this paper studies and proposes a unified construction method for the archival data warehouse of market entities, and verifies the feasibility of archival management of power trading platform data through the example of metering archives of power market entities.",10.1109/ICPST61417.2024.10601777,H. Zhang; M. Shan; P. Xu; Y. Wang
IoTO++: An Enhanced Interoperability Based on Semantic for IoT Environments,2024,-1,Outliers,0.41187463814354863,"The rapid adoption of Internet of Things (IoT) technology has allowed the development of applications where devices such as sensors generate data periodically. These devices are integrated into IoT systems where large volumes of data are managed and whose processing and handling for the proper operation of the IoT system is necessary. One of the main challenges in IoT is the effective data management and real-time communication between heterogeneous devices. Ontologies, which are semantic representations of knowledge, are built to provide semantic interoperability. However, the diversity of applications and the scope of new technologies makes current ontology's proposals incomplete in modeling privacy and security, energy awareness, and ethics at the same time. In this context, we propose an enhanced Ontology, called IoTO++, for improving semantic interoperability in IoT environments. We validated the proposal considering lexical, structural and domain knowledge levels. Results show that even though lite ontologies are more maintainable, compatible and transferable, ontologies with more annotations are better in terms of functional adequacy. This includes but is not limited to characteristics like, knowledge reuse, acquisition and representation. Furthermore, IoTO++ demonstrates superior performance in the domain knowledge level, proving to be a more effective solution for modeling the variety of IoT applications.",10.1109/CLEI64178.2024.10700398,A. Aguilera; D. Garrido; I. Dongo; M. Cornejo-Lupa
Methods for Correlation Analysis of Alarm Information in Multi-Microservice Application Environments,2024,1,Topic_1_data_big_big data,0.6493115471492849,"This research develops a comprehensive analytical framework utilizing data analysis, machine learning, and clustering technologies to address the correlation analysis of alarm information in multi-microservice application environments. By aggregating and preprocessing alarm logs from various security devices and analyzing their underlying logic, this framework generates correlation rules that assist network security personnel in understanding current network threats. Furthermore, the study employs advanced algorithms for anomaly detection within the metrics data of microservice logs, facilitating the direct identification of anomalies in business or IT systems, significantly reducing the reliance on manual threshold settings, and enhancing the accuracy of alerts. The findings demonstrate that this method not only bolsters the stability and reliability of microservice environments but also offers an efficient and systematic approach to analyzing alarm data.",10.1109/ISCIPT61983.2024.10672861,X. Zhu; R. Lu; X. Li; G. Zhang; L. Pan
An Information Model for Modernizing Brownfield Plants in the Process Industry,2023,3,Topic_3_industry_manufacturing_chain,0.6210135209051143,"The concept of data-centered companies is of rapidly increasing interest within the process industry due to applications like data analysis, plant optimization and visualization. However, chemical companies face high barriers regarding the wide variety and heterogeneity of data along the brownfield plant lifecycle caused by the long history of tool-focused enterprises. This paper proposes an Information Model architecture for the process industry and an approach to overcome the hurdles associated with brownfield plants and information mapping.",10.1109/INDIN51400.2023.10217983,D. Pantfoerder; B. Vogel-Heuser; J. Alterbaum; L. Rudolph; F. Ocker
DSChain: A Blockchain System for Complete Lifecycle Security of Data in Internet of Things,2024,-1,Outliers,0.27754714695589777,"There is a growing concern about the complete lifecycle security of data in Internet of Things (IoT). This may cause privacy and trust problems for users regarding data sources, data storage, and access control for data sharing. Blockchain is a valuable solution to the above problems through distributed ledger technology, and it has been widely applied in various fields such as public services, finance, and IoT. However, the data in IoT are characterized by a large quantity, large capacity, and timely response, and existing blockchain systems only partially resolve them for data security and performance. We propose DSChain for IoT data security to address the challenges mentioned above. Our system uses a certificateless signature to ensure a trusted data source and public auditing to ensure the integrity of stored data while using ciphertext-policy attribute-based encryption to control access to shared data. Moreover, we propose a packaging mechanism based on the Merkle Hash Tree that effectively improves system performance. We implement the DSChain and provide a detailed analysis of performance and security. The experimental results indicate that DSChain can achieve approximately 1035 transactions per second on a single peer and is scalable.",10.1109/TDSC.2023.3337093,J. Cui; Y. Li; Q. Zhang; H. Zhong; C. Gu; D. He
Exploring the interplay between DataSpaces and Large Language Models,2024,1,Topic_1_data_big_big data,0.5724922799596474,"The confluence of Large Language Models (LLMs) and Dataspaces presents a captivating prospect for the future of data science and AI, fostering new avenues for data exploration, analysis, and knowledge extraction. This paper explores both patterns arising from the Dataspaces-LLMs convergence: Dataspaces applied to LLMs (DS4LLM) and LLMs applied to Dataspaces (LLM4DS), further investigating the latter. Dataspaces, a conceptual framework for data-centric systems, are poised to revolutionize data management and sharing, indeed. They provide a structured and secure environment for managing and integrating data from multiple sources. Traditional approaches to data management, nevertheless, are bottom-up, often hindering the timely use of data. This paper advocates for a top-down perspective, prioritizing data collection and provisioning to other data management tasks that have to be thus delivered on purpose and in a timely manner a-posteriori, after data (re)source discovery. To bridge the gap between these two approaches, LLMs emerge as a powerful tool able to unlock the value hidden within vast data repositories and Dataspaces. By LLMs, data management tasks such as filtering, cleaning, aggregation, integration, and augmentation can be automated into LLM4DS workflows. This research specifies some LLM4DS prompt templates that can be tailored to specific use cases for on demand and on purpose (a-posteriori) data management requiring post-scheme. A case study on a medical Dataspace demonstrates the practical application and suitability of the proposed LLM4DS approach to the problem at hand.",10.1109/BigData62323.2024.10825298,S. Distefano; Y. N. Yifru
Design of personalized recommendation algorithm for mobile intelligent book management system based on cloud computing technology,2022,1,Topic_1_data_big_big data,1.0,"With the continuous maturity of network technology, digital technology has gradually penetrated into many aspects of real life. The concept of digital technology is consistent with the concept of library management. The use of digital technology can realize the connection of library management information value chain. Library management is cumbersome and complex, the number of borrowing continues to increase [1], including a large number of information data, there is an urgent need to develop an effective library information management system. At present, in today's society with the continuous development of economy and the continuous innovation of science and technology, the management procedure of library borrowing information cannot meet people's growing needs. There will also be problems such as improper information processing and less functions, which will eventually affect people's use emotion and borrowing service. Therefore, in order to give readers a more comfortable and convenient borrowing experience and more effectively improve the library workflow, it is particularly important to design and run a library borrowing information management system based on cloud computing. Therefore, this paper studies the design of book query system based on Java, in order to reduce the work intensity of library managers and provide convenience for readers to query books. Using the relevant algorithms of big data technology to classify and sort out book information, provide users with personalized recommendations and improve their use efficiency. It can also make it easier for administrators to manage books and user information and improve the utilization rate of book resources.",10.1109/ICDSCA56264.2022.9988017,X. Zhong
IoT Edge and Fog Computing Architecture for Educational Systems in Universities,2022,1,Topic_1_data_big_big data,1.0,"Smart lights, ovens, and industrial data-gathering equipment are all instances of internet-connected gadgets. With 41.6 billion IoT devices being used, 79.4 ZB of data will be generated by 2025, according to research firm IDC. The earliest IoT gadgets sent information to the cloud for analysis. When sending many billions of gigabytes to the cloud, the data pipeline clogs up. With computing at the edge, IoT gadgets can analyze data without transferring it to a remote server. Data is handled locally, at the “edge” of your network, rather than being transmitted elsewhere. Technology's malleability makes it a valuable tool in the field of education. The specifications for edge computing in IoT networks vary from those of other networks. At the edge, well-suited setting to handle the massive volumes of devices and data generated by the Internet of Things. Several tasks can be moved to the device's edge, which can help keep costs down. The importance of IoT edge computing architecture in education has grown as a result of its deployment in real-time applications. The research simplifies the process of analyzing the structural layout of educational systems by proposing a framework for Internet of Things (IoT) Edge computing in the field of education. There will be a focus on concerns and issues with the IoT that will be brought out by the framework. In this research, potential and pitfalls of using IoT edge computing in education institutions is explored. In addition to this, the research article analyzes the effectiveness of edge computing for Internet of Things applications in an educational environment.",10.1109/CCET56606.2022.10079946,S. T. Siddiqui; M. O. Ahmad; A. Siddiqui; H. Khan; M. R. Khan; A. H. Alsabhan
JITA4DS: Disaggregated Execution of Data Science Pipelines Between the Edge and the Data Centre,2022,1,Topic_1_data_big_big data,1.0,"This paper targets the execution of data science (DS) pipelines supported by data processing, transmission and sharing across several resources executing greedy processes. Current data science pipelines environments provide various infrastructure services with computing resources such as general-purpose processors (GPP), Graphics Processing Units (GPUs), Field Programmable Gate Arrays (FPGAs) and Tensor Processing Unit (TPU) coupled with platform and software services to design, run and maintain DS pipelines. These one-fits-all solutions impose the complete externalization of data pipeline tasks. However, some tasks can be executed in the edge, and the backend can provide just in time resources to ensure ad-hoc and elastic execution environments. This paper introduces an innovative composable “Just in Time Architecture” for configuring DCs for Data Science Pipelines (JITA-4DS) and associated resource management techniques. JITA-4DS is a cross-layer man-agement system that is aware of both the application characteristics and the underlying infrastructures to break the barriers between applications, middleware/operating system, and hardware layers. Vertical integration of these layers is needed for building a customizable Virtual Data Center (VDC) to meet the dynamically changing data science pipelines' requirements such as performance, availability, and energy consumption. Accordingly, the paper shows an experimental simulation devoted to run data science workloads and determine the best strategies for scheduling the allocation of resources implemented by JITA-4DS.",10.13052/jwe1540-9589.2111,G. Vargas-Solar; M. S. Hassan; A. Akoglu
Significance of Internet-of-Things Edge and Fog Computing in Education Sector,2023,1,Topic_1_data_big_big data,1.0,"IoT gadgets include smart lights, ovens, and industrial data-collecting equipment. IDC expects 41.6 billion connected IoT devices to generate 79.4 ZB of data by 2025. Early IoT devices sent data to the cloud for analysis. When sending trillions of gigabytes to the cloud, the data pipeline clogs. Edge computing enables IoT devices to handle data locally instead of sending it to the cloud. Data is processed at the “edge” of your local network, rather than transferred away. Technology's versatility makes it valuable in education. IoT edge computing has different requirements than non-IoT edge computing. IoT devices have limited data processing and storage capabilities, thus processing must occur off the device. The edge offers an environment to manage huge numbers of IoT devices and data. Many functions can be off-loaded to the edge, reducing device cost. In education, IoT edge computing architecture has gained importance due to real-time applications. In order to facilitate the analysis of educational system architecture, this paper seeks to provide an IoT Edge computing framework in the field of education. The framework will identify IoT-related issues and concerns. This study discusses IoT Edge computing in education, identifies limits, and makes recommendations. As well as the paper evaluates IoT Edge computing's performance in educational systems.",10.1109/ICSCA57840.2023.10087582,S. T. Siddiqui; M. R. Khan; Z. Khan; N. Rana; H. Khan; M. I. Alam
A Topic-Based Unsupervised Learning Approach for Online Underground Market Exploration,2019,1,Topic_1_data_big_big data,0.8939750426288562,"Cyber fraud has become a lucrative form of illicit business by leveraging the Internet as a communication channel and as a result, causes significant losses to the economy. Criminals in the cyber fraud underground economy use online underground markets and other forms of social media to exchange and trade illegitimate information. Due to the high variability in the marketplaces and actors therein, analyzing these underground markets is challenging. To understand more about the underground economy of cyber fraud and its actors, we propose a topic-based hierarchical self-organizing map, which can well represent and visualize actors' similarity and thus, uncover their roles in the underground markets. We compare the proposed method with a topic-based social network analysis method for identifying the key users and their roles in the cyber fraud value chain. Experiments conducted on data from several online underground markets suggest that the proposed method can aid in identifying key actors in terms of roles, influence levels, and their social relationships.",10.1109/TrustCom/BigDataSE.2019.00036,S. -Y. Huang; T. Ban
Abnormal Image Classification Using Vision Transformer for Smart Agriculture,2023,-1,Outliers,0.26850951043782856,"Smart agriculture is an emerging innovative sector that harnesses technological advancements like sensor monitoring, image processing, soil quality evaluation, and automated water systems, all of which contribute towards enhanced crop yields and superior quality of produce. With the progression towards automated systems in smart agriculture, the need for a robust data infrastructure that assures data integrity is critical. This demands an integrated approach with anomaly detection algorithms and a robust classification technique to ensure an error-free data pipeline. This study employs two distinct datasets to achieve abnormal image classification using a vision transformer (ViT) and a convolutional neural network (CNN). The primary dataset, obtained through direct collection from cotton fields in Karachi and Balochistan, Pakistan, comprises high-resolution agricultural images captured using specialised cameras and Unmanned Aerial Vehicles (UAVs). The secondary dataset was sourced from a publicly available database. The two classification models, ViT and CNN, were applied to both datasets. These supervised models are primarily used to classify images as normal or abnormal. The results of the two classification models were also compared, with the ViT model surpassing the CNN model in performance and achieving the highest accuracy on both datasets. The Vision Transformer model proved to be robust for supervised data, showing remarkable accuracy of 96.00% on the public dataset, 99.18% on both camera based images and 97.56% on UAV images of collected dataset as compared to the CNN model's accuracy that are 82.00% on the public dataset and 98.36% on camera and 95.12% on UAV images of collected dataset.",10.1109/ICETAS59148.2023.10346441,T. Hafeez; M. F. Shahid; T. J. S. Khanzada
Open data based value networks: Finnish examples of public events and agriculture,2017,1,Topic_1_data_big_big data,0.4716091194466993,"In recent years, several countries have placed strong emphasis on openness, especially open data, which can be shared and further processed into various applications. Based on studies, the majority of open data providers are government organizations. This study presents two cases in which the data providers are companies. The cases are analyzed using a framework for open data based business models derived from the literature and several case studies. The analysis focuses on the beginning of the data value chain. As a result, the study highlights the role of data producers in the ecosystem, which has not been the focus in current frameworks.",10.23919/MIPRO.2017.7973649,P. Linna; T. Mäkinen; K. Yrjönkoski
Hybrid Supervised Machine Learning Driven Novel IP Reputation Validating Techniques for Cloud Firewalls,2024,1,Topic_1_data_big_big data,0.6212763295829901,"Cloud firewall systems have exponentially improved with artificial intelligence (AI) to achieve more accurate IP Reputation (IPR) validation and robust threat detection. In this paper, we present a novel hybrid Machine Learning (ML) approach to infer IP address reputation to protect hosted applications in Amazon Web Services (AWS) cloud infrastructure. A multi-source data pipeline, including Security Operation Centres (SOC) logs, WAF and Guard Duty Logs, is leveraged to have higher precision and better utility in the IP reputation assessment. This solution takes pre-processed IP threat metadata as input and passes it through an ensemble ML model of Random Forest (RF), Linear Regression (LR), and Support Vector Machine (SVM) classifiers. Different aspects of IP threat detection are addressed by each algorithm. This hybrid model tracks suspicious IP patterns, aggregates findings for supervisory analysis, and dynamically creates AWS Firewall rules to block identified threats in real time. The system learns to synchronously adapt to changing ensemble models’ output through the IP-List and Instant Blacklists, significantly improving their overall defence capability. It achieves substantial reduction of false positives and fasten the response to combat against malicious IP behaviour in cloud hosted applications and is quite secure and robust.",10.1109/ICATC64549.2024.11025257,N. C. Lasantha; M. Maduranga; R. Abeysekara
A Multimodal Learning Analytics Approach to Support Evidence-based Teaching and Learning Practices,2020,-1,Outliers,0.15888441104475987,"Multimodal Learning Analytics (MMLA) aims to support evidence-based educational practices by collecting, processing, analyzing and sense-making of multimodal evidence of learning. MMLA is not widespread yet and there are few tailored MMLA solutions to meet the requirements of a specific learning scenario. This PhD project investigates the main challenges behind the limited development of MMLA solutions and proposes an MMLA infrastructure as the main contribution. The proposed infrastructure includes three components - a data value chain, a data model and a software architecture. The overall project follows the design-based research methodology where multiple iterations are involved to refine the contributions.",10.1109/ICALT49669.2020.00120,S. K. Shankar; A. Ruiz-Calleja; L. P. Prieto; M. J. Rodríguez-Triana
Agricultural 4.0 Leveraging on Technological Solutions: Study for Smart Farming Sector,2024,-1,Outliers,0.3998192876504775,"By 2050, it is predicted that there will be 9 billion people on the planet, which will call for more production, lower costs, and the preservation of natural resources. It is anticipated that atypical occurrences and climate change will pose severe risks to agricultural output. It follows that a 70% or more significant rise in food output is anticipated. Smart farming, often known as agriculture 4.0, is a tech-driven revolution in agriculture with the goal of raising industry production and efficiency. Four primary trends are responsible for it: food waste, climate change, population shifts, and resource scarcity. The agriculture industry is changing as a result of the adoption of emerging technologies. Using cutting-edge technology like IoT, AI, and other sensors, smart farming transforms traditional production methods and international agricultural policies. The objective is to establish a value chain that is optimized to facilitate enhanced monitoring and decreased labor expenses. The agricultural sector has seen tremendous transformation as a result of the fourth industrial revolution, which has combined traditional farming methods with cutting-edge technology to increase productivity, sustainability, and efficiency. To effectively utilize the potential of technology gadgets in the agriculture sector, collaboration between governments, private sector entities, and other stakeholders is necessary. This paper covers Agriculture 4.0, looks at its possible benefits and drawbacks of the implementation methodologies, compatibility, reliability, and investigates the several digital tools that are being utilized to change the agriculture industry and how to mitigate the challenges.",10.1109/ICMI60790.2024.10585910,E. K. Gyamfi; Z. ElSayed; J. Kropczynski; M. A. Yakubu; N. Elsayed
Keynote: Building Smart Cities with Knowledge Graphs,2019,1,Topic_1_data_big_big data,0.43170255228759763,"Smart city systems are increasingly built with data analytics and machine learning techniques, basing on massive data sets. They have a heavy impact on human behavior and quality of life, and thus need to deliver a controllable and sufficiently transparent experience for the users.The aim of my work is to make smart city systems more interoperable and explainable, involving data visualization and communication techniques, sensor data processing, as well as the associated intelligent data value chain production and consumption. Here, the data and the information are shared employing Knowledge Graphs, that are becoming a key enabler for large-scale processing of massive collections of interrelated facts. Examples include the Google Knowledge Graph with dozens of billion facts, dataCommons, DBPedia, YAGO, and Knowledge Vault, a very large scale probabilistic knowledge graph created with information extraction methods for unstructured or semi-structured information. Specifically, Knowledge Graphs provide the means of development of the newest methods for data management, data fusion, data merging, and graph and network optimization and modeling, serving as a source of high quality data and a base for information integration.In particular, Knowledge Graphs help to infer new relationships out of existing facts, giving context and meaning to the content, and can be used in applications. For example, the data generated by a computer vision system could be semantically represented and shared across numerous systems, taking into account the needs and requirements of these systems, as well as the context, provenance, licensing and consent aspects of the generated data. I demonstrate Knowledge Graphs-based methods in advanced smart city applications from the domains such as automation and construction of buildings, energy efficiency, tourism, transport.",10.1109/IC3INA48034.2019.8949613,A. Fensel
Platform Economy Enables Electricity Retail Trading Market: ——Take “Lai Tao Dian” retail trading platform in Yunnan Province as an example,2024,-1,Outliers,0.40907718159186696,"With the full liberalization of general industrial and commercial users to participate in market-oriented transactions, the scale of the retail market will further grow rapidly. In 2021, Kunming Power Exchange Center established the “Lai Tao Dian” retail power trading platform to provide better services for retail users and power selling companies, demonstrating that the platform economy enables Yunnan's electricity retail market. In order to further promote the reform of national electricity sales side, this paper studies the characteristics of platform economy and the operation mode of power retail platform, and believes that the current platform economic practice can meet the needs of various power market players. The platform economy is conducive to promoting the reform of the power selling side, guiding the digital and intelligent transformation of the power selling industry, and laying a good foundation for the integration of the ecological value chain of the power selling industry. This paper puts forward suggestions on the continuous improvement of design, operation and governance capabilities of “Lai Tao Dian” platform.",10.1109/CIEEC60922.2024.10583175,X. Yang; Y. Xing; W. Ding; X. Liu; M. Jin; Z. Yang; T. Sun
Creating Value from Philippine Space Activities: Mobilizing Space Data,2022,1,Topic_1_data_big_big data,0.4921436356809623,"The Philippine Space Agency (PhilSA) has been implementing various initiatives to provide timely and reliable space data and information to government agencies and other stakeholders in the country. By mobilizing space data in support of evidence-based programs and policies in national security and development, environmental protection, food security, and disaster risk reduction and management, PhilSA aims to strengthen the local components of the space data value chain and promote value creation activities from space. In this paper, various space data applications are presented. The activities under the Mobilizing Space Data Project, one of PhilSA's flagship programs, are also described. The emerging role of PhilSA in disaster risk reduction and management is highlighted considering Typhoon Rai (Odette) which caused significant losses and damages in the Philippines.",10.1109/IGARSS46834.2022.9884630,A. C. Blanco; J. S. Marciano; G. J. Perez; S. Meneses; R. K. Aranas; S. Muta; R. dela Cruz; A. J. Sabuito
Epidemic Algorithms on Distributed Systems Towards Industry 4.0,2017,1,Topic_1_data_big_big data,1.0,"In today changing world, with the advent of technology and a challenging competition, reliable information methodologies and tools are required to collaborate big data and various components in both vertical and horizontal integrations over the value chain. Cyber-Physical System or CPS is one of available technologies serving industries' needs and desires. However, its important properties are required to be investigated to fully understand its characteristics and performance. This study present an epidemic-based communication and the results show that the base station could obtain the higher accuracy of the information while the method was adopted.",10.1109/ICSEC.2017.8443937,P. Poonpakdee; J. Koiwanit; C. Yuangyai
An introduction and initial assessment of Uncrewed Systems Standards as a catalyst for data interoperability,2022,1,Topic_1_data_big_big data,0.4365682635460033,"Uncrewed Underwater, Surface and Aerial Systems (UxS) are increasing in use and capability in the marine sectors. With much of the new emphasis on UxS sensor development and higher resolution data, data challenges have arisen to include findability, accessibility, interoperability, and reusability - the key to which is metadata. NOAA and Navy are developing a logical metadata model to serve as a base for unifying UxS data interoperability in the marine domain. The Logical Metadata Model has been presented to a mix of industry, government, and academic partners. It is constructed around modular components from different stages and aspects of the data lifecycle. These components include Sensor Metadata that feed into a Vehicle (platform) Metadata component that documents platform and sensor configuration, calibration, and data collection parameters. These components are assembled into the model’s Core Metadata, which is comparable to a Mission Report; to create one, complete ISO 191** series metadata record paired with the data in a repository. Following the path of the data lifecycle, Curated Metadata is created once the raw data is processed and products are created. The Data Acquisition Metadata feeds into this Curated Data Metadata along with information such as the file names, file types, keywords, and usage constraints. By linking Data Acquisition and Processed metadata to each other, as well as to the scientific data, data become more discoverable and users can quickly leverage new analytical tools and algorithms as they become operational.",10.1109/OCEANS47191.2022.9977372,A. Evans; K. Weathers
Commercial Growth Analysis due to AI Integration in Developing the Level of Fertility at National Level,2024,3,Topic_3_industry_manufacturing_chain,0.5132802944097636,"In fact, the application of Artificial Intelligence (AI) in all fields has always been considered one of the driving forces for the economy and increasing productivity through technological advancement in a fast-changing modern age. The study tries to delve into the relatively subtle relationship that exists between AI adoption and national productivity and possibly clarify the — channels through which AI fosters economic growth in the AI era. This research will, therefore, seek to comprehensively analyze, through the use of existing literature and empirical analysis, the multi-faceted impact of AI on national productivity. It starts by exposing the key concepts and the theoretical framework supporting the position of AI in economic growth. These then unpack both the direct and indirect channels through which AI influences the productivity at national level. Some of the findings seem to indicate that the use of AI does provide a source of productivity gain through several different mechanisms. First, AI can augment labor productivity through automating routine tasks. This frees up human capital to engage in activities of higher value that leverage creativity and problem-solving. Secondly, big data analytics insights, run by AI, help firms to take intelligent decisions, optimize processes all along the value chain, and gain efficiency. Thirdly, AI speeds up the innovation process by boosting the pace of research and development, enhancing the creation, and diffusion of new technologies, products, and services.",10.1109/ICACITE60783.2024.10616962,B. Sharma; N. Giri; R. Chaudhary
ICC World Cup Prediction Based Data Analytics and Business Intelligent (BI) Techniques,2018,1,Topic_1_data_big_big data,0.7261370698262998,"The ICC 2019 Cricket World Cup is scheduled to be hosted by England and Wales. This research work aims to predict the winner of the 12th version of ICC world cup using Business Intelligent (BI) and K Nearest Neighbors KNN bigdata approach. The research works will start off with business intelligent (BI) case and the Big Data lifecycle. Machine Learning, KNN and R Language will be defined in depth. Then, it will give a detailed to extract all patterns that suitable for machine learning tool to predict the winner. Thereafter, data reduction algorithm will be presented. Additionally, explain in details all steps are taken to achieve the KNN classifications. The source and selected datasets required are given. Finally, the root of this paper, the prediction of the winner of the 2019 ICC Cricket World Cup is declared.",10.1109/CyberC.2018.00056,A. A. Aburas; M. Mehtab; Y. Mehtab
Cyber Security Situational Awareness,2016,2,Topic_2_data_privacy_security,0.40723962316048584,"Situational awareness in the context of cyber security has been well recognized. In a time cyber-attacks getting increasingly sophisticated and making potentially disruptive impacts, it becomes apparent that a holistic approach is fundamentally needed to handling security data effectively. Cyber Security Situational Awareness (CSSA) emerges timely. In this paper, after revisiting the concept of CSSA, we have aligned the process of CSSA with security data lifecycle and analyzed the requirements of CSSA. Then, we have put forward a multi-level analysis framework for CSSA.",10.1109/iThings-GreenCom-CPSCom-SmartData.2016.165,H. Tianfield
A model of the process of Big Data with generalized net,2016,1,Topic_1_data_big_big data,0.5137425312913793,"Theoretical generalized net (GN) model is presented. The aim is to follow the process of data lifecycle during a data analysis using Data Science. The presented model should be used during the process of huge amount of data, with various inner structure and near real time input speed.",10.1109/IS.2016.7737487,D. Orozova; M. Georgieva
Leveraging Intel SGX Technology to Protect Security-Sensitive Applications,2018,2,Topic_2_data_privacy_security,0.2791132717964803,"This paper explains the process by which Intel Software Guard Extensions (SGX) can be leveraged into an existing codebase to protect a security-sensitive application. Intel SGX provides user-level applications with hardware-enforced confidentiality and integrity protections and incurs manageable impact on performance. These protections apply to all three phases of the operational data lifecycle: at rest, in use, and in transit. SGX shrinks the trusted computing base (and therefore the attack surface) of the application to only the hardware on the CPU chip and the portion of the application's software that is executed within the protected enclave. The SDK enables SGX integration into existing C/C++ codebases while still ensuring program support for legacy and non-Intel platforms. This paper is the first published work to walk through the step-by-step process of Intel SGX integration with examples and performance results from an actual cryptographic application produced in a standard Linux development environment.",10.1109/NCA.2018.8548184,J. Sobchuk; S. O'Melia; D. Utin; R. Khazan
A Data-centric Network Architecture for Service Optimization and Data Sharing,2018,1,Topic_1_data_big_big data,1.0,"In a hyper-connected intelligent society, data is at the core of businesses. However, currently there is no platform that optimizes the data lifecycle which involves collection, processing, and utilization of data. The existing practice is application specific and based on individual software. By having monolithic characteristics for processing and sharing data, we encounter a limit in the development of applications and observe network bandwidth inefficiency. On the other hand, in current data sharing and utilization procedures, data is being collected vertically, as in an application-domain silo; it drastically increases the overhead in processing data in terms of utilization and sharing. In this paper, we propose Data-centric Network Architecture, a new network design for the future applications that require optimized executions and active data-sharing amongst application domains. We believe Data-centric Network Architecture will serve as a useful common platform for creating new businesses and values in a hyper-connected society.",10.1109/ICTC.2018.8539470,S. Kim; S. Kim; S. H. Park; S. -H. Kim
Design and Interface Testing of Connected Data Architecture of DataLake,2018,1,Topic_1_data_big_big data,0.5601709888133641,"The data plays a pivotal role in many aspects of enterprise operations, and is becoming more and more important to many companies in aspect of Industry Revolution 4.0 and Big Data. Gradually, the value of the enterprise is changing to data-centric. In this paper, we analyze the requirements of the CDA for integrating and managing the data lifecycle of various application areas based on the advantages of the DataLake framework, design the interfaces for CDA, and performs validation of interface # 3 between micro-Storage and DataLake.",10.1109/ICTC.2018.8539708,B. Cha; S. Park; J. Kim
Large scale MTConnect data collection,2019,1,Topic_1_data_big_big data,0.8665597064180226,"Data collection is the first stage of data lifecycle to drive manufacturing activities such as monitoring, prediction and optimization. Data collection at shop floor is more complex since it is affected by both the physical and cyber worlds. Two common issues are: difficult to collect data of various machines which support different interfaces and communication protocols, and hard to analyze inconsistent data. MTConnect provides a standard solution to collect consistent data from various machines and devices. The current MTConnect applications are based on a single computer which is not suitable to collect large-scale MTConnect data. Cloud computing technology can handle this issue easily such as Apache NiFi. In this paper, a NiFi based solution is proposed to address issues of large-scale MTConnect data collection, processing, and long-term storage. The proposed approach also provides a fault tolerant infrastructure for data provenance to ensure historical data integrity for the requirement of enterprise compliance.",10.1109/CIS-RAM47153.2019.9095828,Y. Cui; S. Kara; K. C. Chan
Comprehensive Data Management System of the Electric Power Enterprise based on the Interdisciplinary Data Mining,2021,3,Topic_3_industry_manufacturing_chain,0.20850866940344656,"Aiming at the high residual rate of the traditional power enterprise integrated data management system, a comprehensive data management system for power enterprises based on cross-professional data mining is designed. In terms of hardware, metadata collector and monitoring Board are designed. At the same time, integrated power enterprise comprehensive data based on cross professional data mining, set collection time cycle, frequency, priority, increment, full amount of collection strategy, record and form report on each exploration result, establish power enterprise comprehensive data lifecycle management system, realize power enterprise comprehensive data governance function, complete system design. The experimental results show that the governance legacy rate of the designed system is significantly lower than that of the traditional system, which can solve the problem of high governance legacy rate of the traditional system.",10.1109/ICWCSG53609.2021.00121,K. Liu; L. Wang; L. Gong; Y. Song
ESSA 2022 Invited Speaker: The Curious Incident of the Data in the Scientific Workflow,2022,1,Topic_1_data_big_big data,0.7683285353667314,"The volume, veracity, and velocity of data generated by the accelerators, colliders, supercomputers, light sources and neutron sources have grown exponentially in the last decade. Data has fundamentally changed the scientific workflow running on high performance computing (HPC) systems. It is necessary that we develop appropriate capabilities and tools to understand, analyze, preserve, share, and make optimal use of data. Intertwined with data are complex human processes, policies and decisions that need to be accounted for when building software tools. In this talk, I will outline our work addressing data lifecycle challenges on HPC systems including effective use of storage hierarchy, managing complex scientific data processing, and enabling search on large-scale scientific data.",10.1109/IPDPSW55747.2022.00181,L. Ramakrishnan
Cybersecurity Issues in Space Optical Communication Networks and Future of Secure Space Health Systems,2024,1,Topic_1_data_big_big data,1.0,"Space is an extreme environment and as a result, unmanned and manned missions rely on the operations of the spacecraft/space habitat, and/or the health of the astronaut(s) in that extreme setting for the success of the mission and the safe return to Earth of the astronauts. Advances in Internet of Things (IoT) enabled sensor devices, Big Data pipelines and artificial intelligence (AI) present new opportunities for real-time monitoring of various spacecraft/space habitat equipment (i.e., hardware), systems (i.e., software) and the environment along with human health. While the Big Data pipeline and AI could exist solely on the spacecraft/space habitat, the provision of a communication network with suitable bandwidth, enables the transmission of raw and derived data streams for additional processing, review and research on Earth. It also provides a different layer of redundancy for onboard AI systems. Free space optical communication links (e.g., laser communication) can support greater bandwidth for exchanging space health data than classic RF communication links. Therefore, the extra provided bandwidth makes these emerging technologies excellent candidates for supporting shorter response times to urgent adverse health situations in space. The lower latency experienced in the exchange of space health data using laser transceivers can further improve the security and well-being of astronauts as well as space occupants, which are considered essential components for successful and scalable manned space missions. That transmission to Earth is at risk of cyber at-tack. In addition, to transmit data through Space, one must consider the jurisdiction and relevant bodies to oversee data traffic management in Space. In this work, we discuss the role of Space Traffic Management (STM) in improving the reliability and security of optical space communication links. Specifically, we discuss different cyber threats that adversely affect the real-time delivery of space health data that are communicated over such links. Furthermore, we propose a series of security controls that any practical Space Traffic Management scheme must take into consideration to ensure the reliability and safety of optical communicating parties/links beyond Low Earth Orbit (LEO).",10.1109/AERO58975.2024.10521087,P. Madani; C. McGregor
Facial Image Pre-Processing and Emotion Classification: A Deep Learning Approach,2019,1,Topic_1_data_big_big data,0.4656411324945021,"Facial emotion detection and expressions are vital for applications that require credibility assessment, evaluating truthfulness, and detection of deception. However, most of the research reveal low accuracy in emotion detection mainly due to the low quality of images under consideration. Conducting intensive pre-processing activities and using artificial intelligence especially deep learning techniques are increasing accuracy in computational predictions. Our research focuses on emotion detection using deep learning techniques and combined preprocessing activities. We propose a solution that applies and compares four deep learning models for image pre-processing with the main objective to improve emotion recognition accuracy. Our methodology includes three major stages in the data value chain, pre-processing, deep learning and post-processing. We evaluate the proposed scheme on a real facial data set, namely Facial Image Data of Indian Film Stars for our study. The experimentation compares the performance of various deep learning techniques on the facial image data and confirms that our approach enhanced significantly the image quality using intensive pre-processing and deep-learning, improves accuracy in emotion prediction.",10.1109/AICCSA47632.2019.9035268,A. N. Navaz; S. M. Adel; S. S. Mathew
Waste Minimization and Performance Improvement in Vegetable Supply Chains: A Case Study from a Developing Economy,2023,3,Topic_3_industry_manufacturing_chain,0.40195710992239575,"Unproductive upstream and downstream supply chains' operational performance in the developing economies have resulted large scale waste within vegetable supply chains. This resulted loss of return on investments. This has been further exacerbated by the lack of empowerment and bargaining ability as well as lack of a standard measurement and reporting system in the vegetable supply chains. A case study has been carried out in a developing economy to explore the currently existing vegetable supply chain eco-system and performance. A major vegetable distribution hub of Sri Lanka has been selected to collect data. Value chain analysis (VCA) combined with the cause-and-effect analysis was performed to visualize the currently existing supply chain network and to identify prevailing issues. The findings reveal that a marginally lower share has been passed to the micro and small scale producers after selling their harvest to intermediaries. The small and medium scale producers currently have no influence on product pricing and are heavily influenced by the market syndicates. The overall findings of this study provide a foundation for developing policies to uplift the current existing vegetable supply chain eco-system, and to minimize the food security risk, in developing economies by minimizing systemic waste in the vegetable supply chains.",10.1109/ICBIR57571.2023.10147413,M. M. Jayalath; R. M. C. Ratnayake; H. N. Perera; A. Thibbotuwawa
Stimulating (Open) Data Literacy at the Basis of Society: Approaches for Active Learning and Teaching to Young Children,2023,-1,Outliers,0.15053348148508175,"Children in the primary educational level learn about fundamental reading, writing and mathematics skills as a basis for future learning. In some European Union countries, they are also taught basic informatics and computer usage. However, only a few efforts are made to start teaching them about (open) data literacy. When teaching about data, multiple stages of the data value chain can be tackled, adapted to children’s capabilities; from data production or collection to data usage or impact. Such teaching can be more theoretical, with already prepared examples, or a complete hands-on experience, where the data is collected from the environment, further analyzed, visualized and the conclusions about data are made. In this paper, we will present the currently available research on the approaches to teaching children about data. We will also discuss the potential benefits and the main issues impeding such an effort. Finally, we will propose the way forward for informal active learning about data for lower-level primary school children, based on the hands-on workshops using environmental data.",10.23919/MIPRO57284.2023.10159715,I. Bosnić; A. K. Divjak; B. van Loenen
"Smart Cities: A Survey on Data Management, Security, and Enabling Technologies",2017,1,Topic_1_data_big_big data,0.7600562564054372,"Integrating the various embedded devices and systems in our environment enables an Internet of Things (IoT) for a smart city. The IoT will generate tremendous amount of data that can be leveraged for safety, efficiency, and infotainment applications and services for city residents. The management of this voluminous data through its lifecycle is fundamental to the realization of smart cities. Therefore, in contrast to existing surveys on smart cities we provide a data-centric perspective, describing the fundamental data management techniques employed to ensure consistency, interoperability, granularity, and reusability of the data generated by the underlying IoT for smart cities. Essentially, the data lifecycle in a smart city is dependent on tightly coupled data management with cross-cutting layers of data security and privacy, and supporting infrastructure. Therefore, we further identify techniques employed for data security and privacy, and discuss the networking and computing technologies that enable smart cities. We highlight the achievements in realizing various aspects of smart cities, present the lessons learned, and identify limitations and research challenges.",10.1109/COMST.2017.2736886,A. Gharaibeh; M. A. Salahuddin; S. J. Hussini; A. Khreishah; I. Khalil; M. Guizani; A. Al-Fuqaha
Privacy-preserving data analytics in cloud-based smart home with community hierarchy,2017,-1,Outliers,0.32063096167148925,"The emergence of the Internet of Things (IoT) has led to increasing data volumes, which are expected to grow exponentially. The IoT has great potential to positively change society but also presents challenges regarding privacy. In practice, Smart community public housing projects involving tens of thousands of households have recently been implemented. This study proposed a privacy-preserving smart home system, which connects a single home controller with data-hiding capabilities through community networking and integrates the data to a hierarchical architecture on a cloud platform for a data analytical access control mechanism. In addition, this paper outlines a variety of smart home data applications through data collected from a smart community environment with the developed privacy protection mechanism. The monitoring and protecting mechanism combines privacy-enhancing technologies with a privacy-preserving strategy from the initial system-designing stage through to full data lifecycle management. The types of smart home data that can be obtained from a community hierarchy, such as identification values, sensitive data, and non-sensitive data, were collected and classified. Through a combination of empirical application and sophisticated exploration of theoretical knowledge, this paper substantially contributes to the home automation field. The proposed system architecture is expected to enable both easy understanding for users and compliance for analytical service providers regarding the operation, procedure, limitation, and benefits of smart home data analysis, thereby providing a solution that ensures both privacy and data availability.",10.1109/TCE.2017.014777,Y. -T. Lee; W. -H. Hsiao; Y. -S. Lin; S. -C. T. Chou
SMART-ITEM: IoT-enabled smart living,2016,-1,Outliers,0.5373522809392248,"The main goal of this proposed project is to harness the emerging IoT technology to empower elderly population to self-manage their own health, stay active, healthy, and independent as long as possible within a smart and secured living environment. An integrated open-sourced IoT ecosystem will be developed. It will encompass the entire data lifecycle which involves the following processes: data acquisition, data transportation; data integration, processing, manipulation and computation; visualisation; data intelligence and exploitation; data sharing; data storage. This innovative cloud-based IoT ecosystem will provide a one-stop shop for integrated smart IoT-enabled services to support older people (greater or equal to 65 years old) who live alone at home (or care homes). Another innovation of this system is the design and implementation of an integrated IoT gateway for wellbeing wearable and home automation system sensors with varying communication protocols. The SMART-ITEM system and services will appropriately address the following (i) smart health and care; (ii) smart quality of life; (iii) SMART-ITEM social community. The development of the system will be based on the User Centred Design methodology so as to ensure active user engagement throughout the entire project lifecycle and necessary standards as well as compliances will be adhered to (e.g. security, trust and privacy) in order to enhance user acceptance.",10.1109/FTC.2016.7821687,A. -L. Kor; M. Yanovsky; C. Pattinson; V. Kharchenko
Workload Shifting: Contention-Insular Disk Arrays for Big Data Systems,2016,1,Topic_1_data_big_big data,0.7852106864159581,"It is well known that in-place update index, unordered log structured index and ordered log structured index are three typical data organizations which are designed to meet different workload requirements respectively and wildly used in big data storage systems. Differentiated workload requirements in different phase of the data lifecycle, e.g. various types of data are injected into the big data storage systems in the write optimized manner, then they are needed to be read in the read optimized manner for analysis, lead to data organization transformation(data transformation for short). However, the simple mixture of foreground data injection and background data transformation causes serious disk contention. Frequent disk head seeks result in low disk throughput, and not only prolong the data transformation process, but also increase foreground data injection latency. In this paper, we propose \emph{Workload Shifting}, a novel log- structured design that shifts background data transformation away from the foreground data injection. Compared with conventional RAID0 disk array, \emph{Workload Shifting} effectively isolates background data transformation and foreground data injections, avoids the disk contention between them to boost their performance. We have implemented \emph{Workload Shifting} prototype on one multiple disks based disk array. Extensive experimental evaluation results show that compared with conventional RAID0 disk arrays, \emph{Workload Shifting} can avoid disk contention and speed up both data injection and data transformation significantly.",10.1109/NAS.2016.7549425,F. Pan; Y. Yue; J. Xiong
Customisable Data Science Educational Environment: From Competences Management and Curriculum Design to Virtual Labs On-Demand,2017,-1,Outliers,0.14849464929039968,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and is based on the Big Data and data intensive technologies that both provide a basis for effective use of the data driven research and economy models. Modern data driven research and industry require new types of specialists that are capable to support all stages of the data lifecycle from data production and input to data processing and actionable results delivery, visualisation and reporting, which can be jointly defined as the Data Science professions family. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects all multi-disciplinary knowledge and competences that are required from the Data Science practitioners in modern, data driven research and the digital economy. The educational model and approach should also solve different aspects of the future professionals that includes both theoretical knowledge and practical skills that must be supported by corresponding education infrastructure and educational labs environment. In modern conditions with the fast technology change and strong skills demand, the Data Science education and training should be customizable and delivered in multiple form, also providing sufficient data labs facilities for practical training. This paper discussed both aspects: building customizable Data Science curriculum for different types of learners and proposing a hybrid model for virtual labs that can combine local university facility and use cloud based Big Data and Data analytics facilities and services on demand. The proposed approach is based on using the EDISON Data Science Framework (EDSF) developed in the EU funded Project EDISON and CYCLONE cloud automation systems being developed in another EU funded project CYCLONE.",10.1109/CloudCom.2017.59,Y. Demchenko; A. Belloum; C. de Laat; C. Loomis; T. Wiktorski; E. Spekschoor
On the Continuous Processing of Health Data in Edge-Fog-Cloud Computing by Using Micro/Nanoservice Composition,2020,1,Topic_1_data_big_big data,1.0,"The edge, the fog, the cloud, and even the end-user's devices play a key role in the management of the health sensitive content/data lifecycle. However, the creation and management of solutions including multiple applications executed by multiple users in multiple environments (edge, the fog, and the cloud) to process multiple health repositories that, at the same time, fulfilling non-functional requirements (NFRs) represents a complex challenge for health care organizations. This paper presents the design, development, and implementation of an architectural model to create, on-demand, edge-fog-cloud processing structures to continuously handle big health data and, at the same time, to execute services for fulfilling NFRs. In this model, constructive and modular $blocks$ , implemented as microservices and nanoservices, are recursively interconnected to create edge-fog-cloud processing structures as infrastructure-agnostic services. Continuity schemes create dataflows through the blocks of edge-fog-cloud structures and enforce, in an implicit manner, the fulfillment of NFRs for data arriving and departing to/from each block of each edge-fog-cloud structure. To show the feasibility of this model, a prototype was built using this model, which was evaluated in a case study based on the processing of health data for supporting critical decision-making procedures in remote patient monitoring. This study considered scenarios where end-users and medical staff received insights discovered when processing electrocardiograms (ECGs) produced by sensors in wireless IoT devices as well as where physicians received patient records (spirometry studies, ECGs and tomography images) and warnings raised when online analyzing and identifying anomalies in the analyzed ECG data. A scenario where organizations manage multiple simultaneous each edge-fog-cloud structure for processing of health data and contents delivered to internal and external staff was also studied. The evaluation of these scenarios showed the feasibility of applying this model to the building of solutions interconnecting multiple services/applications managing big health data through different environments.",10.1109/ACCESS.2020.3006037,D. D. Sánchez-Gallegos; A. Galaviz-Mosqueda; J. L. Gonzalez-Compean; S. Villarreal-Reyes; A. E. Perez-Ramos; D. Carrizales-Espinoza; J. Carretero
A Zero Emission Neighbourhoods Data Management Architecture for Smart City Scenarios: Discussions toward 6Vs challenges,2018,1,Topic_1_data_big_big data,1.0,"A huge volume of data are being generated from multiple sources, including smart cities, the IoT devices, scientific modeling, or different big data simulations; but also from users' daily activities. These daily new data are added to historical repositories, providing the huge and complex universe of the digital data. Recently, the Fog-to-Cloud (F2C) data management architecture is envisioned to handle all big data complexities, from IoT devices (the closest layer to the users) to cloud technologies (the farthest layer to the IoT devices), as well as different data phases from creation to usage from fog to cloud scenario. Moreover, the F2C data management architecture can have several benefits from the combined advantages of fog (distributed) and cloud (centralized) technologies including reducing network traffic, reducing latencies drastically while improving security. In this paper, we have several novel contributions. First, we described the previous studies of the Zero Emission Buildings (ZEB) in the context of the data flow and movement architecture. Second, we have proposed Zero Emission Neighbourhoods (ZEN) data management architecture for smart city scenarios based on a distributed hierarchical F2C data management. Indeed, we used the 6Vs big data challenges (Volume, Variety, Velocity, Variability, Veracity, and Value) for evaluating the data management architectures (including ZEB and ZEN). The result of the evaluation shows that our proposed ZEN data management architecture can address 6Vs challenges and is able to manage the data lifecycle from its production up to its usage.",10.1109/ICTC.2018.8539669,A. Sinaeepourfard; J. Krogstie; S. A. Petersen; A. Gustavsen
Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms,2025,2,Topic_2_data_privacy_security,1.0,"The widespread adoption of conversational AI platforms has introduced new security and privacy risks. While these risks and their mitigation strategies have been extensively researched from a technical perspective, users' perceptions of these platforms' security and privacy remain largely unexplored. In this paper, we conduct a large-scale analysis of over 2.5M user posts from the r/ChatGPT Reddit community to understand users' security and privacy concerns and attitudes toward conversational AI platforms. Our qualitative analysis reveals that users are concerned about each stage of the data lifecycle (i.e., collection, usage, and retention). They seek mitigations for security vulnerabilities, compliance with privacy regulations, and greater transparency and control in data handling. We also find that users exhibit varied behaviors and preferences when interacting with these platforms. Some users proactively safeguard their data and adjust privacy settings, while others prioritize convenience over privacy risks, dismissing privacy concerns in favor of benefits, or feel resigned to inevitable data sharing. Through qualitative content and regression analysis, we discover that users' concerns evolve over time with the evolving AI landscape and are influenced by technological developments and major events. Based on our findings, we provide recommendations for users, platforms, enterprises, and policymakers to enhance transparency, improve data controls, and increase user trust and adoption.",10.1109/SP61157.2025.00241,M. Ali; A. Arunasalam; H. Farrukh
Are You Sure That is Secure? Assessing Organizations Security Readiness Via a Devsecops Maturity Model,2025,-1,Outliers,0.23603201077778407,"With the introduction of the DevSecOps manifesto, securing a software supply chain demands more than technical measures. DevSecOps requires a security culture and shared responsibility among all the employees of a company. Keeping track of all the best security practices and ensuring the security readiness of companies with different core businesses is hence a challenging task. To solve this issue, organizations and researchers proposed maturity models, i.e., structured evaluation forms that help identify gaps in companies' security practices with respect to the standard required ones. However, currently existing maturity models cannot cover all the aspects of a DevSecOps pipeline, with limitations including secure data lifecycle management, education and training, and cloud security. In this paper, we propose Maturity Model for DevSecOps (MM4DSO), a novel maturity model that fills our previously identified gaps. Thanks to a set of 320 multiplechoice questions divided into six security domains, MM4DSO evaluates the maturity level of a company and highlights gap areas that require improvement. To ease the assessment, we formulate questions with a unified set of answers to identify a maturity level between one and four. To assess the viability and usefulness of MM4DSO, we tested it on five real-life companies, achieving an average subministration time of one and a half hours and highlighting that critical areas are those related to less classical security practices. Lastly, we propose a theoretical framework to guide security patches based on the results of MM4DSO and the available budget.",10.1109/EuroSPW67616.2025.00068,A. Brighente; E. Burato; M. Conti
Research on Data Integrity Protection and Encryption Technologies for Large Language Models,2025,2,Topic_2_data_privacy_security,0.9392269673385172,"Addressing critical challenges in data integrity and privacy protection for large language models (LLMs) during training and inference, this paper introduces a ChatGLM-3-based security framework designed to overcome limitations in current semantic integrity verification and homomorphic encrypted inference methods. Our approach begins by constructing a data lifecycle risk model using STRIDE threat analysis and Attack Tree modeling, precisely identifying key tampering vulnerabilities. We then develop a semantic hash embedding mechanism leveraging Sentence-BERT combined with Merkle Tree structures, enabling robust semantic-level integrity verification. For secure inference, we implement an end-to-end encrypted pipeline based on the CKKS homomorphic encryption scheme. Furthermore, we integrate differential privacy gradient perturbation with comprehensive training audit logs, establishing a closed-loop defense against data leakage. Experimental validation on the LLaMA 2-13B platform demonstrates the effectiveness of our solution: the semantic hashing showed high robustness with minimal latency, homomorphic inference maintained accuracy at acceptable latency while effectively thwarting reconstruction attacks, and the combined DP-audit mechanism significantly reduced leakage probability enabling effective traceability. These results confirm that our integrated framework strengthens data integrity and privacy safeguards for LLMs without severely compromising utility, offering a practical path for deployment in security-sensitive domains.",10.1109/CAIBDA65784.2025.11183022,B. Yan; Q. Ban; X. Lu
Digital Interconnected Intelligent Decision-Making Platform Based on AI Technology,2025,3,Topic_3_industry_manufacturing_chain,1.0,"In response to the ongoing digital transformation in the oil and gas industry, CNPCNP has developed the “Digital Interconnected Intelligent Decision-Making Platform” to enhance production efficiency, optimize operations, reduce costs, strengthen safety controls, and drive the high-quality development of the company. This platform integrates various AI technologies, including mining technology, business intelligence (BI), robotic process automation (RPA), optical character recognition (OCR), machine learning, low-code development, and AI-generated content (AIGC). By leveraging industrial interconnection, the platform creates a unified database that spans the entire data lifecycle, providing a comprehensive governance and data intelligence system. It effectively connects isolated data silos and enables the creation of advanced intelligent data models with minimal coding through “drag-and-drop” methods. This new approach to digitalization promotes a value framework of “connected by digitalization, driven by digitalization, and reshaped through digitalization.” Ultimately, the platform achieves key objectives, including paperless operations across all company processes, online processing of all workflows, automated information transmission, real-time monitoring of all data, interconnected project management and early warning systems, and overall intelligent company management.",10.1109/AIITA65135.2025.11047625,X. Li; S. Peng; X. Ma; Z. Tian; X. He; Y. Wang
Elevating Quantum-Enhanced Security: Pioneering Advances for Cloud Computing Environments,2025,2,Topic_2_data_privacy_security,0.6433305772824086,"Robust security is vital in cloud computing due to the proliferation of sensitive data. This paper presents an innovative quantum-based security framework to address emerging threats. Measurement-Device-Independent Quantum Key Distribution (MDI-QKD) enhances data transmission security by establishing unhackable channels through quantum entanglement, ensuring data integrity. Quantum Homomorphic Encryption (QHE) enables secure computations on encrypted data, ensuring confidentiality during storage. The Quantum Secure Storage and Management Communication (QSSMC) protocol ensures end-to-end privacy and prevents unauthorized access throughout the cloud data lifecycle. Additionally, Quantum Random Number Generation (QRNG) leverages quantum uncertainty to generate truly random numbers, mitigating the risks of pseudo-randomness and strengthening cryptographic applications. By integrating MDI-QKD, QHE, QSSMC, and QRNG, this framework provides robust security, confidentiality, and data integrity. Implemented in Python with a scalability rate of 95%, it offers a comprehensive solution for modern cloud computing challenges.",10.1109/ICISCN64258.2025.10934366,D. Swetha; S. K. Mohiddin
Understanding IoT Platforms : Towards a comprehensive definition and main characteristic description,2019,1,Topic_1_data_big_big data,1.0,"Internet of things (IoT) offers some advanced vertical services through data capture and processing. To provide the services to the end user, some other services such as data analytics, device management, and connection management should be delivered in the IoT ecosystem. IoT platform is the element, which delivers the central process and management, and the vertical services to the end users, by providing some tools, and computation for device management and data lifecycle management (from sensors networks to the end users). Although there are lots of IoT platform products in the market, there is not any unique, precise, or standard definition with the detailed description of IoT platform, which includes various definitions and functionalities of IoT platform from scientific and market perspective, on both cloud and fog computing resources. In this paper, a novel, comprehensive definition for IoT platform and its attributes in Cloud and Fog layer is proposed, which is extracted from scientific definitions in academic papers, the definition, and features for commercial products provided by IoT leader companies, as well as the description of IoT platform in some open source projects.",10.1109/ICWR.2019.8765259,M. Asemani; F. Abdollahei; F. Jabbari
EDISON Data Science Framework (EDSF) Extension to Address Transversal Skills Required by Emerging Industry 4.0 Transformation,2019,-1,Outliers,0.14726847056368791,"The emerging data-driven economy (also defined as Industry 4.0 or simply 4IR), encompassing industry, research and business, requires new types of specialists that are able to support all stages of the data lifecycle from data production and input, to data processing and actionable results delivery, visualisation and reporting, which can be collectively defined as the Data Science family of professions. Data Science as a research and academic discipline provides a basis for Data Analytics and ML/AI applications. The education and training of the data related professions must reflect all multi-disciplinary knowledge and competences that are required from the Data Science and handling practitioners in modern, data-driven research and the digital economy. In the modern era, with ever faster technology changes, matched by strong skills demand, the Data Science education and training programme should be customizable and deliverable in multiple forms, tailored for different categories of professional roles and profiles. Referring to other publications by the authors on building customizable and interoperable Data Science curricula for different types of learners and target application domains, this paper is focused on defining a set of transversal competences and skills that are required from modern and future Data Science professions. These include workplace and professional skills that cover critical thinking, problem solving, and creativity required to work in highly automated and dynamic environment. The proposed approach is based on the EDISON Data Science Framework (EDSF) initially developed within the EU funded Project EDISON and currently being further developed in the EU funded MATES project and also the FAIRsFAIR projects.",10.1109/eScience.2019.00076,Y. Demchenko; T. Wiktorski; J. Cuadrado Gallego; S. Brewer
"SAMPRA: Scalable Analysis, Management, Protection of Research Artifacts",2021,2,Topic_2_data_privacy_security,0.7919719201608576,"This paper describes SAMPRA, a framework for supporting effective research on sensitive data being deployed at the University of New Mexico. SAMPRA and its associated implementation are designed to support the needs of a diverse set of use-cases from researchers across different disciplines at UNM, including from clinical neurosciences, forensic anthropology, and community health. From these use-cases, we identified a set of common unaddressed demands when handling data with privacy/protection requirements, particularly collaborative research projects, interfacing with scientific instruments, and full-lifecycle management of sensitive data. To properly address and accelerate research projects with these needs, SAMPRA a) integrates privacy-preserving storage and data transfer systems with data-centric virtual environments, and b) supports effective researcher use of the system through active collaboration between local IT personnel, campus enterprise IT service providers, and campus data librarians by defining clear roles with associated personnel. By doing so, SAMPRA seeks to meet the needs of research on sensitive data across the entire data lifecycle and avoid the pitfalls of generic “one-size-fits-all” services.",10.1109/eScience51609.2021.00028,P. G. Bridges; Z. Akhavan; J. Wheeler; H. Al-Azzawi; O. Albillar; G. Faustino
A Time Series Analysis and Persistence Framework for Global Multicloud,2019,1,Topic_1_data_big_big data,0.7314907796878091,"In the global multicloud environment, time series database is an essential service in large-scale monitoring or IoT data persistence and analysis. However, due to various factors such as huge amounts of data, large numbers of concurrent reading and writing devices and complex network environments, conventional time series databases are often difficult to achieve unified management through global multicloud. This paper tried to put forward a persistence and analysis framework for global multicloud distributed time series databases, which could achieve unified analysis and distributed data persistence in multicloud environments and support scale-out, concurrent writing as well as high performance query and analysis. In addition, the framework presented is able to optimize data lifecycle management, query routing and cost, along with forming a good integration with monitoring ecosystem.",10.1109/ICCC47050.2019.9064422,L. Ming; W. Youyan; W. Lijuan; F. Yatong
Data Lake Architecture for Storing and Transforming Web Server Access Log Files,2023,1,Topic_1_data_big_big data,1.0,"Web server access log files are text files containing important data about server activities, client requests addressed to a server, server responses, etc. Large-scale analysis of these data can contribute to various improvements in different areas of interest. The main problem lies in storing these files in their raw form, over long time, to allow analysis processes to be run at any time enabling information to be extracted as foundation for high quality decisions. Our research focuses on offering an economical, secure, and high-performance solution for the storage of large amount of raw log files. Proposed system implements a Data Lake (DL) architecture in cloud using Azure Data Lake Storage Gen2 (ADLS Gen2) for extract–load–transform (ELT) pipelines. This architecture allows large volumes of data to be stored in their raw form. Afterwards they can be subjected to transformation and advanced analysis processes without the need of a structured writing scheme. The main contribution of this paper is to provide a solution that is affordable and more accessible to perform web server access log data ingestion, storage and transformation over the newest technology, Data Lake. As derivative contribution, we proposed the use of Azure Blob Trigger Function to implement the algorithm of transforming log files into parquet files leading to 90% reduction in storage space compared to their original size. That means much lower storage costs than if they had been stored as log files. A hierarchical data storage model has also been proposed for shared access to data over different layers in the DL architecture, on top of which Data Lifecycle Management (DLM) rules have been proposed for storage cost efficiency. We proposed ingesting log files into a Data Lake deployed in cloud due to ease of deployment and low storage costs. The aim is to maintain this data in the long term, to be used in future advanced analytics processes by cross-referencing with other organizational or external data. That could bring important benefits. While the proposed solution is explicitly based on ADLS Gen2, it represents an important benchmark in approaching a cloud DL solution offered by any other vendor.",10.1109/ACCESS.2023.3270368,E. Zagan; M. Danubianu
Accelerating Time to Science using CRADLE: A Framework for Materials Data Science,2023,1,Topic_1_data_big_big data,0.739776799517498,"Modern materials science research problems present a challenge to data science and analytics as experiments generate Petabyte-scale spatiotemporal datasets that span a number of modalities and formats. Creating computing infrastructure and frameworks that support the scale and diversity of materials science data while remaining accessible for materials scientists to use is a non-trivial task. We have developed the Common Research Analytics and Data Lifecycle Environment (CRADLE) to solve the challenges of materials data science through a scalable research computing framework and cyber infrastructure that can (1) handle large-scale, heterogeneous datasets (2) provide a flexible toolbox for building machine learning pipelines that span from ingestion to model deployment (3) be accessible to research scientists with limited to extensive computational backgrounds and (4) utilize a myriad of low performance to high performance computer systems. CRADLE is a framework that integrates distributed systems like Hadoop and High-Performance Computing (HPC) infras-tructure to handle materials data at scale. This all enables the general materials data scientist to query Petabytes of data and train thousands of models in a parallel, distributed environment. We demonstrate three use cases for CRADLE to benchmark its capability to ingest and analyze spatiotemporal materials data at scale. These tasks span three data modalities: transforming 2.6 billion Photovoltaic time-series power measurements, training hundreds of deep learning models on Atomic Force Microscopy images, and ingesting 27 billion geospatial data points. CRADLE exemplifies an overarching framework that accelerates time to science, extends to other domains with similar challenges, and expands the horizon of data science and research.",10.1109/HiPC58850.2023.00041,A. Nihar; T. G. Ciardi; R. Chawla; O. Akanbi; V. Chaudhary; Y. Wu; R. H. French
"Towards Cultural Heritage Digital Twin: Concept, Characteristics, Framework and Applications",2023,-1,Outliers,0.15532686506099072,"The introduction of the digital twin has promoted the development of the lifecycle management of cultural heritage and the innovation of digital protection and utilization, interpretation and dissemination of cultural heritage. By reviewing the relevant literature at home and abroad, this paper summarizes the characteristics of digital twin, compares the data lifecycle management, enabling technologies and application framework in architectural cultural heritage and cultural heritage metaverse, analyzes the connotation of cultural heritage digital twin and digital twin process, and puts forward the framework of cultural heritage digital twin, taking two representative heritage areas of Huai’an section of the Grand Canal as examples for application design, the necessary conditions and challenges are discussed.",10.1109/ICVR57957.2023.10169702,L. Xin; G. Hongyu; S. E. Kyeong; W. Qitao; Y. Guojun; D. Bangkun
Enhancement Strategies For Copy-Paste Generation & Localization in RGB Satellite Imagery,2023,1,Topic_1_data_big_big data,0.34202031706399466,"Satellite imagery is widely used for various applications, such as land-cover classification, field delineation, and environmental monitoring. However, satellite images can also be subject to malicious manipulation, such as copy-paste attacks, where a region from one image is pasted onto another to create a fake scene. Due to the different processing chains that characterize their lifecycle, the multimedia forensics community developed specific tools for analyzing RGB satellite images. Among the characteristics that differentiate satellite images from standard digital pictures, their dynamic characteristics have still not been investigated. In this paper, we study the effect of different data normalization techniques for the generation and analysis of copy-paste attacks on RGB satellite imagery. We show that these techniques boost deep learning tools developed for copy-paste localization, as they promote the extraction of satellite attribution traces. However, they could also help hide these clues, making the detection task more challenging. Our results suggest that careful attention and precise consideration of the data lifecycle should be given when analyzing data modalities different from standard digital pictures.",10.1109/WIFS58808.2023.10374637,E. D. Cannas; S. Baireddy; P. Bestagini; S. Tubaro; E. J. Delp
Privacy Computing with Right to Be Forgotten in Trusted Execution Environment,2023,2,Topic_2_data_privacy_security,1.0,"Sharing private data is at risk of potential data breaches, including the violation of the “right to be forgot-ten” principle, undermining people's willingness to share their data. A common solution is to involve the Trusted Execution Environment (TEE), which allows the data provider to verify the computation process without trusting others. However, previous works have either encountered incomplete computations or lacked scalability. In this paper, we propose TEERASE, a secure data-sharing framework that addresses these issues. TEERASE protects every phase of the data lifecycle and enables individuals to share personal data with a predefined privacy budget. In particular, TEERASE applies comprehensive privacy budgeting mechanisms to efficiently manage privacy budgets and employs an asynchronized execution approach that decouples budget consumption from data computation. TEERASE records the predefined privacy budgets, verifies privacy consumption requests, updates the remaining budgets, and deletes data that have exhausted their budgets by preventing any attempts to access them. We implement a prototype of TEERASE and evaluate its effectiveness with a realistic case study on Genome-Wide Association Study.",10.1109/GLOBECOM54140.2023.10437471,H. Liu; H. Luo; S. Li; T. Dong; G. Chen; Y. Meng; H. Zhu
The Application of data Mining Technology in Student Management,2023,-1,Outliers,0.31508630924769,"In order to understand the application of data mining technology in student management, the author proposes an application research based on data mining technology in student management. The author first introduces big data technology into university management work, opens up information channels between multiple business systems, and establishes a university data lifecycle management model. Secondly, the KNN classification algorithm in data mining is used to mine a large amount of data from students’ online activities. Finally, the relationship between traffic volume, duration, and student grades is found, and abnormal data student behavior is intervened, which is applied to student behavior management. The credit passing status of students was compared based on traffic trends. The experimental prediction results show that the accuracy, precision, and recall rates of abnormal prediction are 74.25%, 77.65%, 81.23%, and the F-Score value is 77.56%, respectively. The use of data mining technology has become an important way to solve this problem.",10.1109/ICIICS59993.2023.10421046,M. Wang
Personal Data Privacy in Software Development Processes: A Practitioner’s Point of View,2023,2,Topic_2_data_privacy_security,0.9374248309548405,"Ensuring customer preferences and needs are met while maintaining legal compliance has become a significant concern for software development organizations. Laws and regulations such as the LGPD in Brazil and the GDPR in the European Union emphasize incorporating personal data privacy rights from the initial stages of system development, extending throughout the entire data lifecycle. However, recent studies indicate that many companies still neglect data privacy in their development processes. Therefore, this research investigates how organizations integrate personal data privacy into their software development processes. A multiple case study was conducted involving five organizations of varying sizes and domains, and qualitative analysis techniques were applied to summarize the findings. The study identified that organizations primarily address data privacy issues in the later stages of the development process. This can be attributed to the need for personal data privacy specialists within the composition of the development teams and the absence of encouragement from organizations for employees to undergo relevant training and courses focused on data privacy. These findings help organizations understand how to improve their software processes to mitigate issues related to user privacy.",10.1109/TrustCom60117.2023.00381,V. C. Andrade; S. Reinehr; C. O. A. Freitas; A. Malucelli
An Integrated AI-Based Information System for Energy Forecasting,2023,3,Topic_3_industry_manufacturing_chain,0.23907683059002124,"The aim of reducing carbon emissions through the digitization of the energy sector involves several challenges. One potential solution is the integration of renewable energy sources; however, the volatile and intermittent nature of this type of energy requires precise forecasting. This paper presents a novel, intelligent information system that covers the entire data lifecycle in the energy sector, including data acquisition, processing and storage, model development and training, and a user-friendly web application. The system was tested in three real-life use cases and demonstrated effectiveness and versatility in addressing the intricate data challenges faced by the energy industry. The paper concludes with suggestions for further research in this area.",10.1109/IISA59645.2023.10345891,P. Skaloumpakas; Z. Mylona; S. Strompolas; E. Sarmas; H. Doukas
HIPAA Technical Compliance Evaluation of Laravel-based mHealth Apps,2024,2,Topic_2_data_privacy_security,0.7497962627095078,"The advent of mobile health applications (mHealth apps) has significantly altered the landscape of personal health management. Particularly, mHealth apps developed with the Laravel framework have gained popularity due to their robustness and scalability. However, this convenience comes with the increased responsibility of safeguarding Protected Health Identifier (PHI). The Health Insurance Portability and Accountability Act (HIPAA) sets forth standards for entities handling PHI to ensure privacy and security. Despite these critical regulations, many Laravel developers lack in-depth knowledge of HIPAA compliance, leading to potential vulnerabilities. This work introduces a comprehensive evaluation framework for assessing Laravel-based mHealth apps’ HIPAA Technical compliance. Our framework provides developers with analytical tools to review their source code for compliance issues and guide them in implementing robust security features. Through static code analysis, the framework examines access control, audit controls, integrity, authentication, encryption, transmission security, user inactivity monitoring, and data lifecycle management. To validate our framework’s effectiveness, we developed a specialized web-based tool and conducted an empirical analysis of 200 Medical and Health Fitness category Laravel applications from Github. The analysis revealed significant compliance gaps, particularly in user authorization, data protection, audit controls, and transmission security. In response, we offer detailed recommendations for developers to address common pitfalls and adhere to HIPAA standards. Additionally, the framework has been used to develop a secured web-based portal to assist consumers in evaluating the privacy and security of mHealth apps.",10.1109/ICDH62654.2024.00020,M. S. Akter; M. A. Barek; M. M. Rahman; A. K. I. Riad; M. A. Rahman; M. R. Mia; H. Shahriar; W. Chu; S. I. Ahamed
"Decentralized, Distributed, and Hybrid ICT Architectures: Hierarchical Multitier Big Data Driven Management for Smart, Sustainable, Scalable and Reliable Cities",2024,1,Topic_1_data_big_big data,0.8050730524989567,"A smart city hopes to intelligently manage a city’s resources to address urban challenges and complexities through urban development and digital transformation strategies. This involves the participation of all stakeholders, including private and public sectors and city residents, in addition to the municipality itself. Data is an essential resource in smart cities, often called green oil. To effectively organize data in smart cities, managing all data lifecycle stages, from creation to consumption, is essential, emphasizing the critical role of information and communications technology (ICT) and information technology (IT) in resource and data management. Sustainable technology focuses on designing, organizing, and managing Internet of Things (IoT) technologies within ICT and IT architectural solutions in smart cities, addressing management of data/databases, resources, network communication, cybersecurity issues, and software services development. This paper reviews various smart city ICT architectures, such as centralized, decentralized-to-centralized (DC2C-ICT), distributed-to-centralized (D2C-ICT), and hybrid-ICT architectures. This discussion comprehensively covers key concepts of big data-driven architecture, including various design perspectives, management, control, and monitoring systems, as well as a range of ICT/IoT technologies and techniques. It involves organizing city data within IoT networks, utilizing multiple computing nodes and platforms in edge-to-cloud orchestration. Emphasis is on the convergence and integration of ICT, IoT, and Big Data with artificial intelligence (AI) and machine learning for efficient organization and management. The paper introduces an innovative big data-driven architecture for hierarchical multitier ICT management in smart city IoT networks, rooted in edge-to-cloud orchestration. This architecture can incorporate diverse ICT and IoT technologies across different multilayer ICT architectures over time, making it adaptable and suitable for various domains. Furthermore, it is well-equipped to manage ICT resources and address big data challenges, including the Vs challenges, for the Smart, Sustainable, Scalable, and Reliable development of cities. The paper summarizes key lessons from the author’s experiences, studies, and publications.",10.1109/SusTech60925.2024.10553566,A. Sinaeepourfard; S. Shaik; N. Mesgaribarzi
AI-driven Optimization of Operational NOTAM Management,2024,1,Topic_1_data_big_big data,0.41973480108397393,"One of the biggest problems affecting the Notice to Air Missions (NOTAM) systems at Air Navigation Service Providers (ANSPs) around the world is the achievement of good data quality or often the lack of data quality whatsoever. Fundamentally, NOTAMs are free text data objects, without a prescribed data structure but rather conventions for syntax and semantics. This leads to the significant risk that data inconsistencies are introduced during the data lifecycle starting directly with the NOTAM origination. NOTAM solutions introduce business rule-based data quality enforcement by checking and, if necessary, correcting errors in NOTAMs. This is both time-consuming and resource-intensive, and, as such, a major source of effort and costs for Aeronautical Information Management (AIM) departments of ANSPs and other stakeholders. In addition, these corrections are always reactive by nature. This challenge is continuously growing, as the number of NOTAMs being published is increasing every year. The solution is the introduction of Digital NOTAMs. However, experience has shown that this transition is not happening overnight and takes many years. Alternate methods to reduce these efforts and still ensure the consistency and correctness of the NOTAM system are needed. This paper describes an Artificial Intelligence (AI) driven, Machine Learning (ML) based algorithm for supporting AIM specialists in identifying and correcting erroneous NOTAMs. Extensive analysis of historic (ICAO) NOTAMS has shown that most of the errors are in the encoding of the Q-code section of a NOTAM. An initial validation is performed based on static business rules, enhanced with an AI algorithm. The AI algorithm uses the concept of confidence scores to correct the Q-section of a NOTAM. It also checks for logical consistency using a trained neural net comparing it to the free-text E section of the NOTAM. The AIM specialist workload is reduced to only having to accept, modify, or decline the corrections proposed by the system. To ensure that the AI algorithm increases efficiency across the entire value chain of the NOTAM it is embedded into an optimized NOTAM processing flow. The paper will outline the NOTAM AI-enhanced correction functionality and describe the building blocks. Finally, the lessons learned from a real-world implementation will be discussed and an outlook on how this functionality can be expanded to address additional NOTAM-related pain points in AIM will be provided.",10.1109/ICNS60906.2024.10550725,M. M. Morărașu; C. Horațiu Roman
Bifrost : No-Code ETL Tool,2024,1,Topic_1_data_big_big data,0.8885321654407103,"In today’s world where mobile and web application usage is ubiquitous, a notable increase in data generation demands skilled management in order for organizations to expand. In this data-driven environment, Extract, Transform, Load (ETL) procedures play a crucial role in coordinating the smooth flow of data from various sources to analytics systems. Businesses use enterprise data lifecycle management (ETL) pipelines to extract data from a variety of sources, such as cloud computing and databases (SQL/NoSQL), which contain structured, semi-structured, and unstructured data in a variety of formats. After that, the data is transformed to match destination schemas, like cloud storage and data warehouses, where it is kept for analysis. But creating these ETL pipelines from scratch for every application takes a significant amount of time and money. This research study presents a brand-new no-code ETL tool with an easy-to-use drag-and-drop pipeline creation interface in order to solve this problem. To create a data flow that is efficient from source to destination, users can design pipelines with ease by choosing sources, defining necessary transformations, defining targets, and connecting these elements. In addition to streamlining pipeline development, the suggested system makes effective planning and storing for later use easier. This makes it an affordable and time-saving solution for businesses navigating the complexities of data management in the ever-evolving world of technology.",10.1109/INOCON60754.2024.10511886,S. S. Shah; V. V. Koranne; K. N. A. Ahmed; D. Ambawade
Light-PoEDDP: A Fast and Lightweight RSA-Based Proof of Possession of Outsourced Data & Correct Computation,2024,1,Topic_1_data_big_big data,0.7220604174604051,"Smart cities rely on seamless integration and communication of diverse sectors through high-end connectivity. Each urban sector is connected to a mesh of domain-specific devices that react to sector-centric data through different events such as capturing, transmitting, storing, training models, and performing inferences. Consequently, the data lifecycle and these events impact urban management and the lives of the citizens positively when sound events trigger correct actions at the right time using available resources. However, relying on these resource-restricted devices to handle critical computations (e.g., cryptographic operations) raises concerns regarding the integrity of the data that directly affects the soundness of these events and the correctness of the taken decisions. To this end, we introduce Light-PoEDDP, a lightweight integrity verification protocol. It leverages the cloud service provider to operate as a compute service of the outsourced computation and as a data store service. This protocol is an enhanced version of our previous protocol, Proof of Exponentiation of Dynamic Data Possession PoEDDP, which is based on RSA-Accumulators.",10.1109/MSCC62288.2024.10697057,A. A. Harchaoui; A. Younes; A. El Hibaoui; A. Bendahmane; A. Machti
"Guest Editorial Special Section on Cloud Computing, Edge Computing, Internet of Things, and Big Data Analytics Applications for Healthcare Industry 4.0",2019,-1,Outliers,0.36460617101630305,"The papers in this special section focus on cloud computing, fog computing, the Internet of Things, and Big Data analytics for the future healthcare industry, or Healthcare 4.0. Healthcare Industry 4.0 allows increasing flexibility in production, speeding up both manufacturing and market processes, increasing both the product quality and productivity, and changing business models modifying the interaction with value chain, competitors, and clients. Healthcare Industry 4.0 requires investments and mind-set change for cross-industry collaboration, agreements on data ownership, security, legal issue solving, product registration standards, new machine-to-machine communication protocols, and employment/skills development. Furthermore,Healthcare Industry 4.0 is revolutionizing the market of health service provisioning to patients and clinical operators.",10.1109/TII.2018.2883315,
Guest Editorial: Cognitive Analytics of Social Media for Industrial Manufacturing,2021,3,Topic_3_industry_manufacturing_chain,1.0,"The papers in this special section focus on cognitive analytics of social media for industrial manufacturing. Business innovation and industrial intelligence pave the way to a future in which smart factories, intelligent machines, networked processes, and big data are brought together to foster industrial growth and shift the modalities. Industry 4.0 or the Industrial Internet of Things (IIoT) is the latest catchphrase of technological innovation in manufacturing with the goal of increasing productivity in a flexible and efficient manner. Concurrently, the new collaborative Web (called Web 2.0) resiliently defines the notion of the techno-social system of computer-mediated, web/internet-based technologies and channels that have the primary objective of creating and enabling a collaborative and interactive virtual community of participants who can share or communicate information. These social technologies are essentially transforming the way we communicate, collaborate, consume, and create data and characterize one of the insurgent impacts of information technology on any industry, both within and outside industrial boundaries. Social media augments as a nontrivial element to this industrial value chain with the intent of making it more efficient. Collaborative sensing or crowd sensing can be used to help producers, suppliers, and customers understand and use insights learned from large amounts of sensing data in order to obtain competitive advantages",10.1109/TII.2020.3028762,
"Introduction to the Special Section: Convergence of Automation Technology, Biomedical Engineering, and Health Informatics Toward the Healthcare 4.0",2018,3,Topic_3_industry_manufacturing_chain,0.39205811172566374,"Industry 4.0 is spilling out from manufacturing to healthcare. In this article, we provide a brief history and key enabling technologies of Industry 4.0, and its revolution in healthcare-Healthcare 4.0-and its reshaping of the landscape of the entire healthcare value chain. We discuss the shift in the system design paradigm from open, small, and single loop to closed, large, and multiple loops. We provide the example of a Caregiving Home, and discuss emerging research topics and challenges, including healthcare big data, automated medical production, healthcare robotics, and human-robot symbiosis. Relevant papers published in this special section are also presented.",10.1109/RBME.2018.2848518,Z. Pang; G. Yang; R. Khedri; Y. -T. Zhang
Data analytics and consumer profiling: Finding appropriate privacy principles for discovered data,2016,2,Topic_2_data_privacy_security,1.0,"In Big Data, the application of sophisticated data analytics to very large datasets makes it possible to infer or derive (“to discover”) additional personal information about consumers that would otherwise not be known from examining the underlying data. The discovery and use of this type of personal information for consumer profiling raises significant information privacy concerns, challenging privacy regulators around the globe. This article finds appropriate privacy principles to protect consumers' privacy in this context. It draws insights from a comparative law study of information privacy laws in the United States and Australia. It examines draft consumer privacy legislation from the United States to reveal its strengths and weaknesses in terms of addressing the significant privacy concerns that relate to Big Data's discovery of personal data and subsequent profiling by businesses.",10.1016/j.clsr.2016.05.002,"King, Nancy J.; Forder, Jay"
Big data framework for analytics in smart grids,2017,-1,Outliers,0.13558960513739055,"Smart meters are being deployed replacing conventional meters worldwide and to enable automated collection of energy consumption data. However, the massive amounts of data evolving from smart grid meters used for monitoring and control purposes need to be sufficiently managed to increase the efficiency, reliability and sustainability of the smart grid. Interestingly, the nature of smart grids can be considered as a big data challenge that requires advanced informatics techniques and cyber-infrastructure to deal with huge amounts of data and their analytics. For that, this unprecedented smart grid data require an effective platform that takes the smart grid a step forward in the big data era. This paper presents a framework that can be a start for innovative research and take smart grids a step forward. An implementation of the framework on a secure cloud-based platform is presented. Furthermore, the framework has been applied on two scenarios to visualize the energy, for a single-house and a smart grid that contains over 6000 smart meters. The application of the two scenarios to visualize the grid status and enable dynamic demand response, suggests that the framework is feasible in performing further smart grid data analytics.",10.1016/j.epsr.2017.06.006,"Munshi, Amr A.; Mohamed, Yasser A.-R. I."
Big Data technologies: A survey,2018,1,Topic_1_data_big_big data,1.0,"Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.",10.1016/j.jksuci.2017.06.001,"Oussous, Ahmed; Benjelloun, Fatima-Zahra; Ait Lahcen, Ayoub; Belfkih, Samir"
Unveiling Dark Data in Organisations:,2025,-1,Outliers,0.2891840059322955,"ABSTRACT The rapid growth of dark data in organisations presents both opportunities and challenges. While dark data contains hidden insights that could improve decision-making, it also leads to compliance, security, and storage risks. This study explores the sources of dark data, its challenges to organisations, and strategies for mitigation of its risks. The findings reveal that legacy systems, unstructured data, and governance gaps are major contributors to dark data accumulation. The study highlights artificial intelligence-driven solutions, role-based access controls, and improved data literacy as effective strategies for addressing dark data challenges. Organisations can enhance data visibility, reduce redundant storage, and improve overall data management by implementing structured governance frameworks and leveraging automation. The study offers propositions that align with organisational implications and outline a roadmap for better utilisation of dark data.",10.4018/IJSSMET.386167,"Marumolwa, Letlhogonolo; Marnewick, Carl"
A conceptual framework for the government big data ecosystem (‘datagov.eco’),2024,1,Topic_1_data_big_big data,0.47621572269006707,"The public sector, private firms, and civil society constantly create data of high volume, velocity, and veracity from diverse sources. This kind of data is known as big data. As in other industries, public administrations consider big data as the “new oil"" and employ data-centric policies to transform data into knowledge, stimulate good governance, innovative digital services, transparency, and citizens' engagement in public policy. More and more public organizations understand the value created by exploiting internal and external data sources, delivering new capabilities, and fostering collaboration inside and outside of public administrations. Despite the broad interest in this ecosystem, we still lack a detailed and systematic view of it. In this paper, we attempt to describe the emerging Government Big Data Ecosystem as a socio-technical network of people, organizations, processes, technology, infrastructure, standards & policies, procedures, and resources. This ecosystem supports data functions such as data collection, integration, analysis, storage, sharing, use, protection, and archiving. Through these functions, value is created by promoting evidence-based policymaking, modern public services delivery, data-driven administration and open government, and boosting the data economy. Through a Design Science Research methodology, we propose a conceptual framework, which we call ‘datagov.eco’. We believe our ‘datagov.eco’ framework will provide insights and support to different stakeholders’ profiles, including administrators, consultants, data engineers, and data scientists.",10.1016/j.datak.2024.102348,"Shah, Syed Iftikhar Hussain; Peristeras, Vassilios; Magnisalis, Ioannis"
Chapter Seven - The growing role of integrated and insightful big and real-time data analytics platforms,2020,1,Topic_1_data_big_big data,0.6022766037455348,"Digitization era is altering several industries which include the way in which the data is analyzed and it is inferred that about 2.7 Zettabytes of data exist in the digital world today. By 2020 the data generated per second for every human being will approximate amount to 1.7 megabytes and the volume of data would double every 2 years thus reach the 40 ZB point by 2020. Interactive Data Corporation (IDC) estimated that by the end of year 2020, the e-commerce transactions B2B and B2C will hit 450 billion per day on the internet. The advent of Big and real time Data has triggered disruptive changes in many fields and the exploding volume of different sources of data like heterogeneous data, data integration, spatio-temporal correlation of data, batch analytics and real-time analytics, data sharing, semantic interoperability requires the development of a scalable platform that can fuse multiple data layers to handles the data intelligently. In Big Data approaches, the challenge is not anymore to collect the data, but to draw valuable conclusions by properly analyzing them. The growth in Unstructured Data generated by business is irrefutable and they are under more pressure to preserve it for longer periods of time. To be clear, exploiting the collected data has been always considered by practitioners and researchers, but the huge velocity, heterogeneity and enormity of massive stream of real-time data shove the limits of the current storage, management and processing capabilities. Admittedly, the traditional method of Extract, Transform and Load (ETL) are challenged and cannot be applied on the emerging opportunistically and crowed sensed data streams. Some of these data streams are structured in a way that serve only one predefined purpose and cannot be directly used for other means. Yet, there are emerging unstructured data such as context-based data from the internet and social media as well as credit card transactions that is not clear if they can be used to better understand the mobility patterns. The analytical company Gartner states that by 2020 there will be over 26 billion interconnected devices. It is obvious, that they will produce massive amounts of meaningful data. Those data can be used for many applications such as real-time industrial equipment monitoring, traffic planning, automated maintenance, etc. Therefore, it is essential to develop modern system abstractions that allow us to resourcefully process huge and new data streams. This enormous amount of data urges the growth of integrated and insightful big and real-time data analytics Platforms. The upcoming contemporary technology like digital twin, integrates historical data from past machine usage to the current data. It uses sensors to collect the real-time data, working status and other operational data attached to the physical model. These components send the relevant data via a cloud-based system to the other side of the bridge with the help of data analytics platform which produces the required insights. The big and real-time data analytics Platforms assist to perform useful operations on data analytics as a complete package. For this purpose, data analytics platform are used to acquire constructive insight from the huge volume of data. Data analytics platform is an ecosystem of technologies and services that can help the businesses in increasing revenues, enhance operational efficiency, stabilize marketing campaigns and customer service efforts, respond more quickly to emerging market trends and gain a competitive edge over rivals. The data analytics platform finds the pattern and relationships in data by applying statistical techniques and communicates the results generated by analytical models to executives and end users to make decisions with the help of data visualization tools that display data on a single screen and can be updated in real time as new information becomes available. Big data and real-time data analytics platform supports the full spectrum of data types, protocols and integration to speed up and simplify the data wrangling process. The big data and real time platform provides accurate data, increase efficiency in the workspace, gives answers to complex questions along with security and hence it plays the key role in business analytics.",,"Indrakumari, Ranganathan; Poongodi, Thangamuthu; Suresh, Palanimuthu; Balamurugan, Balusamy"
Towards a sustainable interoperability in networked enterprise information systems: Trends of knowledge and model-driven technology,2016,3,Topic_3_industry_manufacturing_chain,1.0,"In a turbulent world, global competition and the uncertainty of markets have led organizations and technology to evolve exponentially, surpassing the most imaginary scenarios predicted at the beginning of the digital manufacturing era, in the 1980s. Business paradigms have changed from a standalone vision into complex and collaborative ecosystems where enterprises break down organizational barriers to improve synergies with others and become more competitive. In this context, paired with networking and enterprise integration, enterprise information systems (EIS) interoperability gained utmost importance, ensuring an increasing productivity and efficiency thanks to a promise of more automated information exchange in networked enterprises scenarios. However, EIS are also becoming more dynamic. Interfaces that are valid today are outdated tomorrow, thus static interoperability enablers and communication software services are no longer the solution for the future. This paper is focused on the challenge of sustaining networked EIS interoperability, and takes up input from solid research initiatives in the areas of knowledge management and model driven development, to propose and discuss several research strategies and technological trends towards next EIS generation.",10.1016/j.compind.2015.07.001,"Agostinho, Carlos; Ducq, Yves; Zacharewicz, Gregory; Sarraipa, João; Lampathaki, Fenareti; Poler, Raul; Jardim-Goncalves, Ricardo"
Developing a government enterprise architecture framework to support the requirements of big and open linked data with the use of cloud computing,2019,1,Topic_1_data_big_big data,0.476785383066454,"Governmental and local authorities are facing many new information and communication technologies challenges. The amount of data is rapidly increasing. The data sets are published in different formats. New services are based on linking and processing differently structured data from various sources. Users expect openness of public data, fast processing, and intuitive visualisation. The article addresses the challenges and proposes a new government enterprise architecture framework. The following partial architectures are included: big and open linked data storage, processing, and publishing using cloud computing. At first, the key concepts are defined. Next, the basic architectural roles and components are specified. The components result from the decomposition of related frameworks. The main part of the article deals with the detailed proposal of the architecture framework and partial views on architecture (sub-architectures). A methodology, including a proposal of appropriate steps, solutions and responsibilities for them, is described in the next step - after the verification and validation of the new framework with respect to the attributes of quality. The new framework responds to emerging ICT trends in order to evolve government enterprise architecture continually and represent current architectural components and their relationships.",10.1016/j.ijinfomgt.2018.12.003,"Lnenicka, Martin; Komarkova, Jitka"
Understanding big data and data protection measures in smart city strategies: An analysis of 28 cities,2024,1,Topic_1_data_big_big data,0.661633819089628,"The Smart City concept aims to improve urban governance and optimize public services, ultimately enhancing the quality of life for citizens. As data generation and processing grow rapidly in volume, velocity, and variety, Smart Cities must integrate secure big data considerations into their strategic frameworks and project implementations. This paper explores how big data and data protection measures are represented in the strategies of 28 cities worldwide. To achieve this, we employed a three-phase research methodology: 1) identifying resources, 2) conducting content analysis, and 3) using the Delphi method. Our findings indicate that only half of the cities explicitly address big data in their strategies, and most lack adequate data protection measures. Additionally, the paper presents a list of recommendations for big data management and data protection, derived from measures found in Smart City strategies and validated by domain experts through the Delphi method. These recommendations aim to enhance understanding of how to effectively incorporate big data and its protection into urban planning and Smart City projects. However, it is important to note that these insights primarily apply to larger urban areas with abundant resources.",10.1016/j.ugj.2024.12.008,"Lnenicka, Martin; Hervert, Petr; Horak, Oldrich"
Digital Twins and their Implications for Business Models: Overview and Potentials,2024,-1,Outliers,0.15626772047612544,"Digital Twins are key elements to develop the complex digital systems required for an effective Circular Economy transition. This study explores the intersection of Digital Twins and Business Models within Industry 4.0 and offers a comprehensive theoretical background of the advancements in Digital Twins and their potentials to innovate Business Models. Through a literature review, we identify and discuss eleven Digital Twin-enabled Business Models. The study suggests the necessity of further research efforts to validate theoretical perspectives and identifies future research venues, such as the exploration of Digital Twin-enabled Circular Business Models.",10.1016/j.ifacol.2024.09.246,"Adelsberger, Rodrigo Torres; Antons, Oliver; Arlinghaus, Julia"
An ontology-based framework for the management of machining information in a data mining perspective,2018,3,Topic_3_industry_manufacturing_chain,0.2570443455030674,"The advent and fast development of data mining techniques induced, in every field, interest for the data produced and stored. Machining process planning is no exception to this rule, and, with the important amount of related data that is stored in the enterprise information system, the application of data mining seems promising. However, the strong heterogeneity in data, and the distribution of information throughout several different documents, may hinder the application of data-mining methods and machine learning tools. This paper will introduce a knowledge-based engineering framework which can be used as an information system able to query consistent, correct and complete data from the heterogeneous corpus of documents used in process planning of machined parts.",10.1016/j.ifacol.2018.08.300,"Ostermeyer, Emeric; Danjou, Christophe; Durupt, Alexandre; Duigou, Julien Le"
Big data analytics and application for logistics and supply chain management,2018,3,Topic_3_industry_manufacturing_chain,0.5444384519431934,"This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.",10.1016/j.tre.2018.03.011,"Govindan, Kannan; Cheng, T.C.E.; Mishra, Nishikant; Shukla, Nagesh"
Self-adapting cloud services orchestration for fulfilling intensive sensory data-driven IoT workflows,2020,1,Topic_1_data_big_big data,1.0,"Cloud computing has been adopted to support among others the storage and processing of complex Internet of Things (IoT) workflows handling sensory streamed time-series data. IoT workflow is often composed following a set of procedures which makes it hard to self-adapt, self-configure to react to runtime environment changes. Therefore, declarative data-driven workflow composition will provision self-learning and self-configurable workflows such as those of IoT. This paper proposes a comprehensive architecture to support end-to-end workflow management processes including declarative specification and composition, configuration deployment, orchestration, execution, adaptation, and quality enforcement. The later provision runtime intelligence for IoT workflow orchestration; this is achieved through the automated monitoring and analysis of runtime cloud resource orchestration, the monitoring of workflows tasks execution, as well as through cloud resource utilization prediction and workflow adaptation. In addition, it supports other intelligent features that include: (1) integration of edge computing (sensor edge) for local data processing which is very crucial for life-critical IoT workflows, (2) data compression for fast data transmission, and data storage adaptation, and (3) customization of data reporting and visualization. All these features have been evaluated through a set of experiments that proved a significant gain in terms of workflow execution time, cost and optimum usage of cloud resources compared to baseline adaptation strategy.",10.1016/j.future.2020.02.066,"Adel Serhani, M.; El-Kassabi, Hadeel T.; Shuaib, Khaled; Navaz, Alramzana N.; Benatallah, Boualem; Beheshti, Amine"
Adoption of big data analytics for energy pipeline condition assessment - A systematic review,2023,-1,Outliers,0.24277857262001584,"Due to complexity, the oil and gas industry use various sensors to collect data for analysis to maintain the safety and integrity of pipelines and associated infrastructures. There is an enormous amount of data available to conceal crucial information, including precursor data on failure modes and knowledge that may be analyzed. The availability of large amounts of data has enabled the development of analytical tools that integrate methods like predictive analytics using different decision-making models, artificial intelligence (AI), and machine learning. These tools are crucial for managing pipeline conditions, preventing unwarranted failures, enhancing asset performance, availability, and decision-making. Big data analytics enables energy companies to implement a proactive approach to pipeline condition assessment. By integrating real-time data from sensors embedded in pipelines, weather conditions, and maintenance records, it becomes possible to detect potential issues and predict anomalies. The application of big data analytics in the energy pipeline industry is still at its early stage, although literature review discusses big data in oil and gas, however, the application's specific relevance to energy pipeline integrity and condition assessment has been largely unexplored. Therefore, this study addresses the applications of big data analytics in energy pipeline condition assessment by investigating the challenges and benefits. Collaboration among pipeline operators, data scientists, and technology providers were emphasized for successful adoption. The study envisions a future where big data analytics will be crucial in enhancing pipeline safety, efficiency, availability, integrity, and reliability. Recommendations for further research were also provided, culminating in a proposed conceptual framework for adopting big data analytics in the oil and gas pipeline industry.",10.1016/j.ijpvp.2023.105061,"Hussain, Muhammad; Zhang, Tieling; Seema, Minnat"
A Multi-Layer Big Data Value Chain Approach for Security Issues,2020,2,Topic_2_data_privacy_security,0.3902180351443631,"Big Data systems generate a lot of data from different sources, sometimes are less reliable. Also, business ecosystems are highly interconnected, through Big Data Value Chains (BDVC) either internally or with partners, making their data assets and processes more vulnerable to multiple cyber-attacks. However, this kind of sensitive exposition and data workflows requires specific protection and security management. In this contribution, we highlight the importance of coupling BDVC and Big Data security as well as existing contributions addressing these topics. Also, we propose a multi-dimensional model aiming to show cybersecurity milestones and reduce the gap between cyber-risks and how organizations manage their data. For this goal, we suggest a multi-layered security framework to deal with security issues along BDVC. This framework, which is a generic view adaptable to different domains, allows protecting organizations’ sensitive data assets as well as privacy concerns. Furthermore, this multi-layer projection ensures a sustainable cyber-ecosystem.",10.1016/j.procs.2020.07.109,"Faroukhi, Abou Zakaria; El Alaoui, Imane; Gahi, Youssef; Amine, Aouatif"
A novel big data analytics framework for smart cities,2019,1,Topic_1_data_big_big data,0.6584098335520444,"The emergence of smart cities aims at mitigating the challenges raised due to the continuous urbanization development and increasing population density in cities. To face these challenges, governments and decision makers undertake smart city projects targeting sustainable economic growth and better quality of life for both inhabitants and visitors. Information and Communication Technology (ICT) is a key enabling technology for city smartening. However, ICT artifacts and applications yield massive volumes of data known as big data. Extracting insights and hidden correlations from big data is a growing trend in information systems to provide better services to citizens and support the decision making processes. However, to extract valuable insights for developing city level smart information services, the generated datasets from various city domains need to be integrated and analyzed. This process usually referred to as big data analytics or big data value chain. Surveying the literature reveals an increasing interest in harnessing big data analytics applications in general and in the area of smart cities in particular. Yet, comprehensive discussions on the essential characteristics of big data analytics frameworks fitting smart cities requirements are still needed. This paper presents a novel big data analytics framework for smart cities called “Smart City Data Analytics Panel — SCDAP”. The design of SCDAP is based on answering the following research questions: what are the characteristics of big data analytics frameworks applied in smart cities in literature and what are the essential design principles that should guide the design of big data analytics frameworks have to serve smart cities purposes? In answering these questions, we adopted a systematic literature review on big data analytics frameworks in smart cities. The proposed framework introduces new functionalities to big data analytics frameworks represented in data model management and aggregation. The value of the proposed framework is discussed in comparison to traditional knowledge discovery approaches.",10.1016/j.future.2018.06.046,"Osman, Ahmed M. Shahat"
Chapter 6 - Big Data quality assessment in the IoT era,2025,-1,Outliers,0.41613809945174396,"The Internet of Things (IoT) has shown unprecedented data generation and connectivity through disruptive applications across various domains. This data, referred to as Big Data, contains valuable information about various aspects of our lives and the physical world. As the IoT ecosystem continues to expand, ensuring the quality of the vast volumes of data it produces has become crucial. Therefore Big Data quality assessment is important in the IoT era to ensure that the data collected and analyzed is accurate, trustworthy, and usable. By performing Big Data quality assessments, organizations can make better decisions, improve their operations, and gain a competitive advantage. This chapter aims to enhance data quality assessment in IoT by providing an overview of its state-of-the-art. Big Data and IoT data properties and their new lifecycles are discussed. Moreover, selected works from the literature on IoT data quality are exhaustively reviewed. Additionally, a holistic architecture of IoT quality management model is proposed capturing key characteristics of IoT applications. Finally, open challenges and possible future research directions are discussed.",,"Taleb, I.; Dahmani, N.; Mathew, S.S.; Hayawi, K."
Towards Holistic Cyber-Physical Production Systems in Existing Production Environment: Challenges from a Case Study,2025,3,Topic_3_industry_manufacturing_chain,0.8118448758362249,"Cyber-physical production systems (CPPS) are the backbone of Smart Production. Digital transformation often starts from a hierarchical systems landscape based on the automation pyramid with legacy systems and low data availability. CPPS’s are characterized by a decentralized structure with the ability to share data and services across the value chain, to gain benefits in line with Industry 5.0 demands. Literature proposes different models, which in theory should enable companies with individual starting points and legacy systems to implement a holistic CPPS architecture. Nevertheless, companies struggle with its implementation. Therefore, the purpose of this article is to analyze challenges when implementing a holistic CPPS architecture. A case study assesses the transformation of a global manufacturing company towards a holistic CPPS architecture with input from stakeholders at management level to operators. This paper provides examples and insights into current challenges and gives recommendations for next steps towards a holistic CPPS architecture.",10.1016/j.procir.2025.03.020,"Englund, Tobias; Bruch, Jessica; Chirumalla, Koteshwar; Ashjaei, Mohammad"
Data Value Chains in Manufacturing: Data-based Process Transparency through Traceability and Process Mining,2022,-1,Outliers,0.29606841433330816,"Traceability systems are widely used in manufacturing processes, mainly for legal reasons. Based on their ability to generate and gather data along processes, they are an excellent base for creating performance indicators. Changing market demands lead to a rising amount of product variants and decreasing batch sizes causing higher complexity in production processes. In this context the growing availability of data along the value chain offers new opportunities. Based on a manufacturing data set, this paper presents a concept for building a data value chain consisting of a traceability system for data generation and acquisition, as well as a process mining application for the analysis of the generated process data. Firstly, to determine the traceability systems ability to generate relevant process data for manufacturing, secondly to demonstrate how this data contributes to data-based transparency through process mining analysis. Transparency is essential to enable data-based decisions as well as improvement measures in production. The results of the process mining analysis are then connected to the specific configurations of the traceability system in order to show the correlations and dependencies along the entire data value chain. The understanding of the data value chain from traceability system to process mining can empower companies to further benefit from their traceability system, by configuring it to deliver the needed data-based transparency and improvements in their production management.",10.1016/j.procir.2022.05.037,"Schreiber, Markus; Metternich, Joachim"
Advancing Human-Centric Blockchain Applications for Circular Supply Chains: A Pharmaceutical Case Study,2025,-1,Outliers,0.29900596930128553,"This study explores integrating blockchain technology into circular supply chain systems through a human-centered lens within the Industry 5.0 paradigm. Embedding transparency, trust, and collaboration into blockchain-enabled processes ensures alignment with sustainability and societal well-being. Key findings from a pharmaceutical case study highlight the role of human engagement and technological innovation in promoting circular supply chains. The proposed architecture leverages blockchain to enhance traceability, trust, and integration, while emphasizing human engagement and regulatory frameworks.",10.1016/j.ifacol.2025.09.427,"Hajizadeh, Maryam; Alaeddini, Morteza; Reaidy, Paul"
